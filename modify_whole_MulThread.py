import os
import sys
import json

import pandas as pd
import tensorflow_datasets as tfds

from collections import defaultdict
import threading
import queue
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
import tensorflow as tf
from tqdm import tqdm




def _generate_single_episode_video_threadsafe(episode_data):
    """
    çº¿ç¨‹å®‰å…¨çš„å•ä¸ªepisodeè§†é¢‘ç”Ÿæˆå‡½æ•°
    """
    try:
        episode, episode_id, instruction, save_root, fps, frame_stride, image_field, other_image_fields, dataset_nickname = episode_data
        
        import os
        import logging
        import imageio
        import numpy as np
        
        # åˆ›å»ºä»»åŠ¡ä¿å­˜ç›®å½•
        task_save_dir = os.path.join(save_root, 'samples', instruction[:30].replace(' ','_').replace('/','_'))
        os.makedirs(task_save_dir, exist_ok=True)
        
        # ç”Ÿæˆä¸»è§†è§’è§†é¢‘
        if image_field:
            try:
                from scripts.xintong.utils.video_utils import extract_frames_from_episode
                
                frames = extract_frames_from_episode(episode, image_field, frame_stride=frame_stride)
                frames_uint8 = [(f if f.dtype == np.uint8 else (f * 255).astype(np.uint8)) for f in frames]
                video_path = os.path.join(task_save_dir, f"{instruction[:30].replace(' ','_').replace('/','_')}_ep{episode_id}.mp4")
                imageio.mimsave(video_path, frames_uint8, fps=fps)
                logging.info(f"âœ… çº¿ç¨‹ {threading.current_thread().name}: å·²ä¿å­˜ä¸»è§†è§’ ep{episode_id}")
            except Exception as e:
                logging.error(f"âŒ ç”Ÿæˆä¸»è§†è§’è§†é¢‘å¤±è´¥ episode {episode_id}: {e}")
        
        # ç”Ÿæˆå…¶ä»–è§†è§’è§†é¢‘
        if other_image_fields and dataset_nickname:
            # åˆ›å»ºå…¶ä»–è§†è§’ä¿å­˜ç›®å½•
            others_save_root = os.path.join(save_root, 'others_image', instruction[:30].replace(' ','_').replace('/','_'))
            os.makedirs(others_save_root, exist_ok=True)
            
            for other_field in other_image_fields:
                try:
                    from scripts.xintong.utils.video_utils import extract_frames_from_episode
                    
                    frames = extract_frames_from_episode(episode, other_field, frame_stride=frame_stride)
                    frames_uint8 = [(f if f.dtype == np.uint8 else (f * 255).astype(np.uint8)) for f in frames]
                    video_path = os.path.join(others_save_root, f"{instruction[:30].replace(' ','_').replace('/','_')}_ep{episode_id}_{other_field}.mp4")
                    imageio.mimsave(video_path, frames_uint8, fps=fps)
                    logging.info(f"âœ… çº¿ç¨‹ {threading.current_thread().name}: å·²ä¿å­˜å…¶ä»–è§†è§’ {other_field} ep{episode_id}")
                except Exception as e:
                    logging.error(f"âŒ ç”Ÿæˆå…¶ä»–è§†è§’ {other_field} è§†é¢‘å¤±è´¥ episode {episode_id}: {e}")
        
        return (episode_id, instruction, True)  # è¿”å›æˆåŠŸä¿¡æ¯
        
    except Exception as e:
        logging.error(f"âŒ çº¿ç¨‹å¤„ç†episodeå¤±è´¥: {e}")
        return (episode_id, instruction, False)  # è¿”å›å¤±è´¥ä¿¡æ¯


def _process_dataset_parallel(ds, save_root, fps, skip_frame, image_field, other_image_fields, dataset_nickname, manager, max_workers=4, max_queue_size=None):
    """
    å¹¶è¡Œå¤„ç†æ•°æ®é›†ï¼šä¸»çº¿ç¨‹æŒ‰é¡ºåºè¯»å–episodeï¼Œå¤šä¸ªå·¥ä½œçº¿ç¨‹å¹¶è¡Œç”Ÿæˆè§†é¢‘
    
    Args:
        max_queue_size: æœ€å¤§é˜Ÿåˆ—å¤§å°ï¼Œæ§åˆ¶å†…å­˜å ç”¨ã€‚é»˜è®¤ä¸º max_workers * 3
    """
    if max_queue_size is None:
        max_queue_size = max_workers * 3  # é»˜è®¤é˜Ÿåˆ—å¤§å°ä¸ºå·¥ä½œçº¿ç¨‹æ•°çš„3å€
    
    task2count = defaultdict(int)
    unique_tasks = set()
    episode_id = 0
    completed_count = 0
    total_submitted = 0
    
    # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶è¡Œå¤„ç†
    with ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix="VideoGen") as executor:
        # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
        future_to_info = {}
        
        pbar = tqdm(desc="å¤„ç†episodeså¹¶ç”Ÿæˆè§†é¢‘")
        
        # åˆ›å»ºæ•°æ®é›†è¿­ä»£å™¨
        dataset_iter = iter(ds)
        
        try:
            while True:
                # 1. é¦–å…ˆå¤„ç†å·²å®Œæˆçš„ä»»åŠ¡ï¼Œé‡Šæ”¾å†…å­˜
                completed_futures = []
                for future in list(future_to_info.keys()):
                    if future.done():
                        try:
                            result_episode_id, result_instruction, success = future.result()
                            if success:
                                completed_count += 1
                            completed_futures.append(future)
                        except Exception as e:
                            logging.error(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
                            completed_futures.append(future)
                
                # ç§»é™¤å·²å®Œæˆçš„ä»»åŠ¡
                for future in completed_futures:
                    del future_to_info[future]
                
                # 2. æ£€æŸ¥é˜Ÿåˆ—å¤§å°ï¼Œå¦‚æœæœªæ»¡åˆ™æ·»åŠ æ–°ä»»åŠ¡
                if len(future_to_info) < max_queue_size:
                    try:
                        episode = next(dataset_iter)
                        
                        # ä¸»çº¿ç¨‹ï¼šé¡ºåºè¯»å–episodeå¹¶æå–instruction
                        step_0 = next(iter(episode['steps']))
                        from scripts.xintong.utils.dataset_utils import extract_instruction_from_step0 as utils_extract_instruction_from_step0
                        instruction = utils_extract_instruction_from_step0(step_0).replace('\n', '')
                        if instruction == "":
                            instruction = "Dummy instruction"
                        
                        # ç»Ÿè®¡ä»»åŠ¡ï¼ˆä¸»çº¿ç¨‹ï¼Œä¿è¯çº¿ç¨‹å®‰å…¨ï¼‰
                        task2count[instruction] += 1
                        unique_tasks.add(instruction)
                        
                        # æ·»åŠ åˆ°managerï¼ˆä¸»çº¿ç¨‹ï¼Œä¿è¯çº¿ç¨‹å®‰å…¨ï¼‰
                        manager.add_entry(instruction, f"{episode_id}", "TODO caption")
                        
                        # å‡†å¤‡çº¿ç¨‹ä»»åŠ¡æ•°æ®
                        episode_data = (
                            episode, episode_id, instruction, save_root, fps, skip_frame,
                            image_field, other_image_fields, dataset_nickname
                        )
                        
                        # æäº¤åˆ°çº¿ç¨‹æ± 
                        future = executor.submit(_generate_single_episode_video_threadsafe, episode_data)
                        future_to_info[future] = (episode_id, instruction)
                        
                        total_submitted += 1
                        episode_id += 1
                        
                        # æ›´æ–°è¿›åº¦æ¡
                        pbar.set_description(f"å·²æäº¤: {total_submitted}, é˜Ÿåˆ—: {len(future_to_info)}, å®Œæˆ: {completed_count}")
                        pbar.update(1)
                        
                    except StopIteration:
                        # æ•°æ®é›†éå†å®Œæˆ
                        logging.info(f"âœ… æ•°æ®é›†è¯»å–å®Œæˆï¼Œå…±è¯»å– {episode_id} ä¸ªepisodes")
                        break
                    except tf.errors.OutOfRangeError:
                        logging.info(f"âœ… æ•°æ®é›†è¯»å–å®Œæˆï¼Œå…±è¯»å– {episode_id} ä¸ªepisodes")
                        break
                    except Exception as e:
                        logging.error(f"âŒ ä¸»çº¿ç¨‹å¤„ç†episode {episode_id} å‡ºé”™: {e}")
                        episode_id += 1
                else:
                    # é˜Ÿåˆ—å·²æ»¡ï¼Œç­‰å¾…ä¸€å°æ®µæ—¶é—´å†æ£€æŸ¥
                    import time
                    time.sleep(0.1)
        
        except tf.errors.OutOfRangeError:
            logging.info(f"âœ… æ•°æ®é›†éå†å®Œæˆ")
        
        # ç­‰å¾…æ‰€æœ‰å‰©ä½™ä»»åŠ¡å®Œæˆ
        logging.info(f"â³ ç­‰å¾…å‰©ä½™ {len(future_to_info)} ä¸ªè§†é¢‘ç”Ÿæˆä»»åŠ¡å®Œæˆ...")
        for future in as_completed(future_to_info):
            try:
                result_episode_id, result_instruction, success = future.result()
                if success:
                    completed_count += 1
                pbar.set_description(f"å®Œæˆ {completed_count}/{total_submitted} ä¸ªè§†é¢‘")
            except Exception as e:
                logging.error(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
        
        pbar.close()
    
    logging.info(f"ğŸ‰ å¹¶è¡Œå¤„ç†å®Œæˆ: {completed_count}/{total_submitted} ä¸ªepisodeæˆåŠŸï¼Œæ¶‰åŠ {len(unique_tasks)} ä¸ªä¸åŒtask")
    logging.info(f"ğŸ“Š å„taskæ•°é‡: {dict(task2count)}")
    
    return task2count, unique_tasks, total_submitted



# å¯¼å…¥ utils
try:
    from scripts.xintong.utils.log_utils import setup_logging as utils_setup_logging
    from scripts.xintong.utils.dataset_utils import (
        dataset2path as utils_dataset2path,
        debug_local_dataset_structure as utils_debug_local_dataset_structure,
    )
    from scripts.xintong.utils.annotations import TaskVidCaptionManager as UtilsTaskVidCaptionManager
    from scripts.xintong.utils.video_utils import visualize_task_episodes as utils_visualize_task_episodes
    from scripts.xintong.utils.config import DATASET_CONFIGS, Local_dataset, Google_dataset
    from scripts.xintong.utils.dataset_utils import extract_instruction_from_step0 as utils_extract_instruction_from_step0
except ModuleNotFoundError:
    # å°†é¡¹ç›®æ ¹è·¯å¾„åŠ å…¥sys.pathï¼Œæ”¯æŒç›´æ¥ç”¨ `python scripts/xintong/modify_whole.py` è¿è¡Œ
    PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    if PROJECT_ROOT not in sys.path:
        sys.path.append(PROJECT_ROOT)
    from scripts.xintong.utils.log_utils import setup_logging as utils_setup_logging
    from scripts.xintong.utils.dataset_utils import (
        dataset2path as utils_dataset2path,
        debug_local_dataset_structure as utils_debug_local_dataset_structure,
    )
    from scripts.xintong.utils.annotations import TaskVidCaptionManager as UtilsTaskVidCaptionManager
    from scripts.xintong.utils.video_utils import visualize_task_episodes as utils_visualize_task_episodes
    from scripts.xintong.utils.config import DATASET_CONFIGS, Local_dataset, Google_dataset
    from scripts.xintong.utils.dataset_utils import extract_instruction_from_step0 as utils_extract_instruction_from_step0







Whether_local_dataset = True # æ˜¯å¦æ˜¯æœ¬åœ°æ•°æ®é›† false æ˜¯googleæ•°æ®é›†

df = pd.read_csv('/home2/qrchen/embodied-datasets/scripts/xintong/Datasets/metadata.csv') # å¯ä»¥ä¿®æ”¹ä¸ºå…¶ä»–è·¯å¾„

# åˆ›å»ºlogæ–‡ä»¶å¤¹
log_dir = './log'
os.makedirs(log_dir, exist_ok=True)

#æ£€æŸ¥æ•°æ®ç»“æ„
if Whether_local_dataset:
    datasets_name = Local_dataset
    # è°ƒè¯•æœ¬åœ°æ•°æ®é›†ç»“æ„
    print("ğŸ” æ­£åœ¨æ£€æŸ¥æœ¬åœ°æ•°æ®é›†ç»“æ„...")
    utils_debug_local_dataset_structure()
    print("=" * 60)
else:
    datasets_name = Google_dataset


# å¼€å§‹é€ä¸ªå¤„ç†æ•°æ®é›†
for dataset_name in datasets_name:
    dataset = dataset_name
    logger = utils_setup_logging(log_dir, dataset_name)
    logger.info(f"Processing dataset: {dataset}\n")

    # ä»df ä¸­æ‰¾åˆ° DATASET_CONFIG ä¸­ dataset_name_in_csv å¯¹åº”çš„è¡Œï¼Œå¾—åˆ°å…¶ä¸­ nickname çš„å€¼
    dataset_name_in_csv = DATASET_CONFIGS[dataset_name]['dataset_name']
    dataset = df[df['Datasets'] == dataset_name_in_csv]
    dataset = dataset['NickName'].item()
    # è®¾ç½®ä¿å­˜è·¯å¾„
    save_root = os.path.join('/home2/qrchen/embodied-datasets/Modifications', dataset)
    os.makedirs(save_root, exist_ok=True)

    # é…ç½®å‚æ•°
    TRAIN_SPLIT = 'train' #åŸºæœ¬ä¸Šéƒ½æ˜¯train
    MAX_EPISODES = DATASET_CONFIGS[dataset_name]['max_episodes'] # æœ€å¤šæ”¶é›†å¤šå°‘ä¸ªepisode
    STRICT_MODE = True  # æ˜¯å¦ä¸¥æ ¼æ¨¡å¼
    # STRICT_MODE = True  -> ç›´åˆ°æ”¶é½ 10 ä¸ª task ä¸”æ¯ä¸ª task éƒ½ >= 10 ä¸ª episode æ‰åœ
    # STRICT_MODE = False -> ä¸€æ—¦é‡åˆ° 10 ä¸ªä¸åŒçš„ task å°±åœï¼ˆä¸ä¿è¯æ¯ä¸ª task éƒ½æœ‰ 10 ä¸ª episodeï¼‰
    # è§†é¢‘ç›¸å…³é…ç½®
    IMAGE = DATASET_CONFIGS[dataset_name]['image_field']
    OTHER_IMAGE_FIELDS = DATASET_CONFIGS[dataset_name].get('other_image_fields', None)
    START_IDX = 0  # ä»æ‰€é€‰ task ç¬¬å‡ ä¸ª episode å¼€å§‹
    fps = DATASET_CONFIGS[dataset_name]['fps']
    skip_frame = DATASET_CONFIGS[dataset_name]['skip_frame'] # è·³è¿‡å¤šå°‘å¸§ï¼Œé»˜è®¤æ˜¯3å¸§


    # åŠ è½½æ•°æ®é›†
    dataset_path = utils_dataset2path(dataset, is_local=Whether_local_dataset)
    if dataset_path is None:
        logger.error(f"æ— æ³•è·å–æ•°æ®é›†è·¯å¾„ï¼Œè·³è¿‡æ•°æ®é›†: {dataset}")
        continue
    
    if Whether_local_dataset:
        # æœ¬åœ°æ•°æ®é›†åŠ è½½
        logger.info(f"ä»æœ¬åœ°è·¯å¾„åŠ è½½æ•°æ®é›†: {dataset_path}")
        b = tfds.builder_from_directory(builder_dir=dataset_path)
    else:
        # Google Cloud æ•°æ®é›†åŠ è½½
        logger.info(f"ä»Google CloudåŠ è½½æ•°æ®é›†: {dataset_path}")
        b = tfds.builder_from_directory(builder_dir=dataset_path)
    
    # å¦‚æœMAX_EPISODESå¤§äºæ•°æ®é›†çš„episodeæ•°ï¼Œåˆ™ä½¿ç”¨æ•°æ®é›†çš„episodeæ•°
    ds = b.as_dataset(split=TRAIN_SPLIT)
    if MAX_EPISODES is None:
        max_episodes = len(ds)
    else:
        max_episodes = min(MAX_EPISODES, len(ds))
    logger.info(f"total episodes: {len(ds)}")
    ds = ds.take(max_episodes)

    # åˆå§‹åŒ–annotationç®¡ç†å™¨
    annotation_file = os.path.join(save_root, 'annotations.json')
    if os.path.exists(annotation_file):
        os.remove(annotation_file)
    with open(annotation_file, "w", encoding="utf-8") as f:
        json.dump({}, f, indent=2, ensure_ascii=False)  
    manager = UtilsTaskVidCaptionManager(annotation_file)

    # å¹¶è¡Œå¤„ç†æ•°æ®é›†ï¼ˆé»˜è®¤4ä¸ªå·¥ä½œçº¿ç¨‹ï¼‰
    max_workers = 4  # å¯ä»¥æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´
    max_queue_size = max_workers * 2  # æ§åˆ¶é˜Ÿåˆ—å¤§å°ï¼Œé¿å…å†…å­˜å ç”¨è¿‡å¤§
    logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç†ï¼Œä½¿ç”¨ {max_workers} ä¸ªå·¥ä½œçº¿ç¨‹ï¼Œé˜Ÿåˆ—å¤§å°: {max_queue_size}")
    
    task2count, unique_tasks, total_episodes = _process_dataset_parallel(
        ds, save_root, fps, skip_frame, IMAGE,
        DATASET_CONFIGS[dataset_name].get('other_image_fields'),
        dataset, manager, max_workers, max_queue_size
    )
    
    logger.info(f"âœ… {dataset_name} å¤„ç†å®Œæˆ: {total_episodes} ä¸ªepisodeï¼Œ{len(unique_tasks)} ä¸ªä¸åŒtask")
    manager.save()