Datasets,NickName,Score,In OpenVLA?,FInish Modify,need modify,other view,fully download,whether FPS is proper?,AI/human,finish,#Trajectories,fully upload to AWS,verison,download link,Language instructions,In OXE?,How to modify it to align with our overall objectives?,Tasks / Categories,Comments,#Scenes,Avg. frames/trajectory,Control frequency,# Total Cams,# Depth Cams,# First-person Cams,# Third-person Cams,Robot type,Robot Morphology,Tactile sensor?,Objects,Full data structure,Action space,whether annotation,Venue,How to change the raw data to RDLS format data,How is the dataset collected?,Robot ,,Comments,Real or Not?,Granularity,Pixel Quality,Size of Dataset
FMB,fmb,3,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,no need modify,TRUE,"8,611",1.2T 下载太慢,0.0.1,https://functional-manipulation-benchmark.github.io/,"template
{verb} the {color/shape} object
eg.
Insert the hexagon object.
Pick up the round object.",✅,no need modify ,"1. Place 
2. Grasp
3. Insert
4. Move to board
5. Rotate 
6. Regrasp",,1,227,10,4,4,2,2,Franka Panda,Single Arm,,54  assembly objects + 3 assembly boards,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'episode_language_embedding': Tensor(shape=(512,), dtype=float32),
        'episode_language_instruction': string,
        'episode_task': string,
        'file_path': string,
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32),
        'discount': Scalar(shape=(), dtype=float32),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32),
        'language_instruction': string,
        'observation': FeaturesDict({
            'color_id': Scalar(shape=(), dtype=uint8),
            'eef_force': Tensor(shape=(3,), dtype=float32),
            'eef_pose': Tensor(shape=(7,), dtype=float32),
            'eef_torque': Tensor(shape=(3,), dtype=float32),
            'eef_vel': Tensor(shape=(6,), dtype=float32),
            'image_side_1': Image(shape=(256, 256, 3), dtype=uint8),
            'image_side_1_depth': Tensor(shape=(256, 256), dtype=float32),
            'image_side_2': Image(shape=(256, 256, 3), dtype=uint8),
            'image_side_2_depth': Tensor(shape=(256, 256), dtype=float32),
            'image_wrist_1': Image(shape=(256, 256, 3), dtype=uint8),
            'image_wrist_1_depth': Tensor(shape=(256, 256), dtype=float32),
            'image_wrist_2': Image(shape=(256, 256, 3), dtype=uint8),
            'image_wrist_2_depth': Tensor(shape=(256, 256), dtype=float32),
            'joint_pos': Tensor(shape=(7,), dtype=float32),
            'joint_vel': Tensor(shape=(7,), dtype=float32),
            'length': string,
            'object_id': Scalar(shape=(), dtype=uint8),
            'primitive': string,
            'shape_id': Scalar(shape=(), dtype=uint8),
            'size': string,
            'state_gripper_pose': Scalar(shape=(), dtype=float32),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",EEF velocity,,IJRR 2024,,,Franka Panda,,"not general.This task requires an auxiliary bracket ,the black one 
",,,,
IO-AI,io_ai_tech,3,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,no need modify,TRUE,3847,TRUE,0.0.1,https://github.com/ioai-tech/,"template {pick/place} {object name}
1. pick
2. place",✅,✅,"1. pick
2. place",,2,84,30,4,1,2,2,human,Single Arm,,NA(anything in the table-top),"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': string,
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32),
        'discount': Scalar(shape=(), dtype=float32),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32),
        'language_instruction': string,
        'observation': FeaturesDict({
            'depth': Image(shape=(720, 1280, 1), dtype=uint8),
            'fisheye_camera_extrinsic': Tensor(shape=(4, 4), dtype=float32),
            'fisheye_camera_intrinsic': Tensor(shape=(3, 3), dtype=float32),
            'image': Image(shape=(360, 640, 3), dtype=uint8),
            'image_fisheye': Image(shape=(640, 800, 3), dtype=uint8),
            'image_left_side': Image(shape=(360, 640, 3), dtype=uint8),
            'image_right_side': Image(shape=(360, 640, 3), dtype=uint8),
            'left_camera_extrinsic': Tensor(shape=(4, 4), dtype=float32),
            'left_camera_intrinsic': Tensor(shape=(3, 3), dtype=float32),
            'main_camera_intrinsic': Tensor(shape=(3, 3), dtype=float32),
            'right_camera_extrinsic': Tensor(shape=(4, 4), dtype=float32),
            'right_camera_intrinsic': Tensor(shape=(3, 3), dtype=float32),
            'state': Tensor(shape=(8,), dtype=float32),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})

action : EE x y z rx zy rz. gripper
state : 7 joint + gripper ",EEF Position,,,,,,,"✅
although collected by human",,,,
RoboSet,robo_set,2,FALSE,TRUE,TRUE,TRUE,TRUE,TRUE,"human
down sample",FALSE,18250,TRUE,0.0.1,https://robopen.github.io/roboset/,"fine-grad natural language 
1. pick
2. place 
3. flap-close 
4. flap-open  
5. cap
6. uncap
7. slide-close
8. slide-open 
9. press 
10. slide-in
11. slide-put
12. wipe",✅,✅,"1. pick
2. place 
3. flap-close 
4. flap-open  
5. cap
6. uncap
7. slide-close
8. slide-open 
9. press 
10. slide-in
11. slide-put
12. wipe",,1,<100,5,4,0,1,3,Franka panda,Single Arm,,"NA(anything in the kichen  table-top , like the oven , toy , mug , cup and etc.)","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': string,
        'trial_id': string,
    }),
    'steps': Dataset({
        'action': Tensor(shape=(8,), dtype=float32),
        'discount': Scalar(shape=(), dtype=float32),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_instruction': string,
        'observation': FeaturesDict({
            'image_left': Image(shape=(240, 424, 3), dtype=uint8),
            'image_right': Image(shape=(240, 424, 3), dtype=uint8),
            'image_top': Image(shape=(240, 424, 3), dtype=uint8),
            'image_wrist': Image(shape=(240, 424, 3), dtype=uint8),
            'state': Tensor(shape=(8,), dtype=float32),
            'state_velocity': Tensor(shape=(8,), dtype=float32),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})
",joint position ：7 joint position + 1 gripper,,,human VR,,,,,,,,
CMU Food Manipulation,cmu_food_maanipulation,1,FALSE,TRUE,,,,,,,4200,FALSE,,https://sites.google.com/view/playing-with-food/,"{grasp , press , release } the {21 kind food}
for example: 
1.Grasp the tomato slice.
2.Release the bread slice.
3.Release the mozzarella slice.
4.Grasp the boiled_carrot slice.
5.Press down on the boiled_bell_pepper slice.
6.Grasp the boiled_pear slice.
7.Grasp the cheddar slice.
8.Press down on the cucumber slice.
9.Grasp the apple slice.
10.Grasp the celery slice.
11.Press down on the onion slice.
12.Grasp the raw_steak slice.
13.Grasp the kiwi slice.
14.Press down on the boiled_pear slice.
15.Press down on the jalapeno slice.
16.Release the cheddar slice.
...
59.Release the onion slice.
60.Grasp the boiled_apple slice.
61.Press down on the boiled_potato slice",✅,"1. the action here is 8D, 3x end-effector position, 4x end-effector quaternion, 1x gripper open/close].   quaternion need to change to 3D delta rotation","{grasp , press , release } the {21 kind food}
for example: 
1.Grasp the tomato slice.
2.Release the bread slice.
3.Release the mozzarella slice.
4.Grasp the boiled_carrot slice.
5.Press down on the boiled_bell_pepper slice.
6.Grasp the boiled_pear slice.
7.Grasp the cheddar slice.
8.Press down on the cucumber slice.
9.Grasp the apple slice.
10.Grasp the celery slice.
11.Press down on the onion slice.
12.Grasp the raw_steak slice.
13.Grasp the kiwi slice.
14.Press down on the boiled_pear slice.
15.Press down on the jalapeno slice.
16.Release the cheddar slice.
...
59.Release the onion slice.
60.Grasp the boiled_apple slice.
61.Press down on the boiled_potato slice",,1,,10,3,0,2,1,Franka,Single Arm,,21 unique food items with varying slices and properties,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(8,), dtype=float32, description=""Robot action, consists of [3x end-effector position, 4x end-effector quaternion, 1x gripper open/close].""),
        'discount': Scalar(shape=(), dtype=float32, description=""Discount if provided, default to 1.""),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=""Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5""),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(480, 640, 3), dtype=uint8, description=""Main camera RGB observation.""),
            'state': Tensor(shape=(6,), dtype=float32, description=""Robot state, consists of [6x end-effector force].""),
            'finger_vision_1': Image(shape=(480, 640, 3), dtype=uint8, description=""Finger Vision 1 RGB observation.""),
            'finger_vision_2': Image(shape=(480, 640, 3), dtype=uint8, description=""Finger Vision 2 RGB observation.""),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=""Reward if provided, 1 on final step for demos.""),
    }),
})",EEF Position,,,,,,,,,,,
CMU Play Fusion,cmu_play_fusion,2,FALSE,TRUE,TRUE,FALSE,TRUE,TRUE,"human
down sample",FALSE,576,TRUE,0.1.0,https://play-fusion.github.io/,"some example:
1.pick pot and place in sink
2.pick spoon and place in red cup
3.open grill, then pick pineapple and place insid grill
4.pick pot and place on wooden hanger
5.open pot, then pick pineapple and place insid pot
6.open oven, then pick chicken and place insid oven
7.pick yellow cup and place in dish rack
8.pick banana and place in blue plate
9.pick knife and place in blue plate
10.pick knife and place in red cup
11.pick knife and place in pink bowl
12.pick radish and place in pink bowl
13.pick radish and place in blue plate
14.pick banana and place in pink bowl
15.open pot, then pick bread and place insid pot
16.pick yellow bowl and place in sink
17.open grill, then pick carrot and place insid grill
18.open oven, then pick carrot and place insid oven
19.pick radish and place in red cup
20.pick yellow bowl and place on wooden hanger
21.pick yellow bowl and place in dish rack
22.open oven, then pick steak and place insid oven
23.pick orange cup and place on wooden hanger
24.open oven, then pick pineapple and place insid oven
25.pick yellow cup and place in sink
26.pick orange cup and place in sink
27.pick spoon and place in pink bowl
28.open grill, then pick chicken and place insid grill
29.pick yellow cup and place on wooden hanger
30.open pot, then pick carrot and place insid pot
31.pick carrot and place in blue plate
32.open pot, then pick chicken and place insid pot
33.pick pot and place in dish rack
34.pick orange cup and place in dish rack
35.open oven, then pick bread and place insid oven
",✅,"1. action space need to modify 
2. only third-person vedio , which need to consider","some example:
1.pick pot and place in sink
2.pick spoon and place in red cup
3.open grill, then pick pineapple and place insid grill
4.pick pot and place on wooden hanger
5.open pot, then pick pineapple and place insid pot
6.open oven, then pick chicken and place insid oven
7.pick yellow cup and place in dish rack
8.pick banana and place in blue plate
9.pick knife and place in blue plate
10.pick knife and place in red cup
11.pick knife and place in pink bowl
12.pick radish and place in pink bowl
13.pick radish and place in blue plate
14.pick banana and place in pink bowl
15.open pot, then pick bread and place insid pot
16.pick yellow bowl and place in sink
17.open grill, then pick carrot and place insid grill
18.open oven, then pick carrot and place insid oven
19.pick radish and place in red cup
20.pick yellow bowl and place on wooden hanger
21.pick yellow bowl and place in dish rack
22.open oven, then pick steak and place insid oven
23.pick orange cup and place on wooden hanger
24.open oven, then pick pineapple and place insid oven
25.pick yellow cup and place in sink
26.pick orange cup and place in sink
27.pick spoon and place in pink bowl
28.open grill, then pick chicken and place insid grill
29.pick yellow cup and place on wooden hanger
30.open pot, then pick carrot and place insid pot
31.pick carrot and place in blue plate
32.open pot, then pick chicken and place insid pot
33.pick pot and place in dish rack
34.pick orange cup and place in dish rack
35.open oven, then pick bread and place insid oven
",,7,394,5,1,0,0,1,Franka,Single Arm,,table top objects,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(9,), dtype=float32, description=Robot action, consists of [7x delta eef (pos + quat), 1x gripper open/close (binary), 1x terminate episode].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(128, 128, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(8,), dtype=float32, description=Robot state, consists of [7x robot joint angles, 1x gripper position.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,,CoRL2023,,,,,,,,,
CMU Stretch,cmu_stretch,1,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,,,135,TRUE,0.1.0,https://robo-affordances.github.io/,"only 5 instruction
1.lift open green garbage can lid
2.open door
3.open drawer
4.pull open a dishwasher
5.lift up a lid from the pot",✅,"1. action space : only aciton[:8] , first 7 demention is used
2. fine annotation  wihtin the same task
3. many trajectory are failed , need to filter","only 5 instruction
1.lift open green garbage can lid
2.open door
3.open drawer
4.pull open a dishwasher
5.lift up a lid from the pot",,1,185,10,1,0,0,1,Hello Stretch,Mobile Manipulator,,"only 5 object：
1.green garbage can lid
2.door
3.drawer
4. dishwasher
5.lid and pot","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(8,), dtype=float32, description=Robot action, consists of [3x ee pos, 3x ee rot 1x gripper binary action, 1x terminate episode].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(128, 128, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(4,), dtype=float32, description=Robot state, consists of [3x robot joint angles/ee pos, 1x gripper position].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,,,,,,,,,,,
DROID,droid,3,TRUE,TRUE,TRUE,TRUE,FALSE,FALSE,ai,FALSE,92233,FALSE,,https://droid-dataset.github.io/,Natural,✅,"1. contain the detail velocity imforamtion,but it is redundant    ---- >. delete 
2. need to change the  image (180,320, 3 ) to. ( 224,224,3)
3. only one  third-person image is enough , chose the left or the right one is enough 
4. there are 3 instruction. for example : language_instruction :""Put the marker in the pot"" ,  ""language_instruction_2 "":""Get the marker from the table and put it inside the silver pot"" ;  "" language_instruction_3"" : ""Put the marker inside the silver pot"";  
5.  some task have not the second and third language instruction , so we only get the first instrucition ,
6. wash the data without instruction  ","1.put

2. move 

3. pick

4. take 

5. open 

6. close 

7. place 

8. turn 

9. remove

10. other","76k demonstration trajectories  
350 hours of interaction data
564 scenes
84 tasks
Visualization",564,300,15,3,0,2,1,Franka Panda,Single Arm,,"1. Food

2. personal care

3. kichen tools

4. sports

5. accessories

6. hardware

7. clothes

8. appliances

9. utensils

10. furniture

11. textile

12. containers

13. stationary ","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'episode_id': Scalar(shape=(), dtype=int32),
        'file_path': string,
        'recording_folderpath': string,
        'has_exterior_image_1_left': Scalar(shape=(), dtype=bool),
        'has_exterior_image_2_left': Scalar(shape=(), dtype=bool),
        'has_wrist_image_left': Scalar(shape=(), dtype=bool),
        'has_language': Scalar(shape=(), dtype=bool),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32),
        'action_dict': FeaturesDict({
            'cartesian_position': Tensor(shape=(6,), dtype=float32),
            'cartesian_velocity': Tensor(shape=(6,), dtype=float32),
            'gripper_position': Tensor(shape=(1,), dtype=float32),
            'gripper_velocity': Tensor(shape=(1,), dtype=float32),
            'joint_position': Tensor(shape=(7,), dtype=float32),
            'joint_velocity': Tensor(shape=(7,), dtype=float32),
        }),
        'discount': Scalar(shape=(), dtype=float32),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32),
        'language_instruction': string,
        'language_instruction_2': string,
        'language_instruction_3': string,
        'observation': FeaturesDict({
            'exterior_image_1_left': Image(shape=(180, 320, 3), dtype=uint8),
            'exterior_image_2_left': Image(shape=(180, 320, 3), dtype=uint8),
            'wrist_image_left': Image(shape=(180, 320, 3), dtype=uint8),
            'cartesian_position': Tensor(shape=(6,), dtype=float32),
            'joint_position': Tensor(shape=(7,), dtype=float32),
            'gripper_position': Tensor(shape=(1,), dtype=float32),
            'state': Tensor(shape=(14,), dtype=float32),  
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",EEF position,,RSS 2024,"Github link about how to process rawdata

https://github.com/kpertsch/droid_dataset_builder","1. hardware setup ($20,000)
2. softwate setup
3. how to load and visualize samples


https://droid-dataset.github.io/droid/the-droid-dataset.html",,"Robot Platform: 
1. Franka Panda 7DoF robot arm
2. two adjustable Zed 2 stereo cameras
3. one  wristmounted Zed Mini stereo camera,
3. one  Oculus Quest 2 headset 

        
",,,,,
BridgeDataV2,bridge,3,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,ai,FALSE,"60,096 (50,365 human / 9,731 rollouts)",FALSE,,https://rail-berkeley.github.io/bridgedata/,"Natural
1. Pick-and-place (includes reorienting objects in place)
2. Pushing objects
3. Wiping (e.g., wiping the table with a cloth)
4. Sweeping (e.g., sweeping beans into a pile)
5. Stacking
6. Folding cloths
7. Opening and closing drawers
8. Only grasp different objects
9. Twisting knobs
10. Flipping switches
11. Zipping and unzipping
12. Opening and closing doors
13. Opening and closing cardboard box flaps",✅,"Seems to be good
- very diverse, with diverse instructions & objects","1. Pick-and-place (includes reorienting objects in place)
2. Pushing objects
3. Wiping (e.g., wiping the table with a cloth)
4. Sweeping (e.g., sweeping beans into a pile)
5. Stacking
6. Folding cloths
7. Opening and closing drawers
8. Only grasp different objects
9. Twisting knobs
10. Flipping switches
11. Zipping and unzipping
12. Opening and closing doors
13. Opening and closing cardboard box flaps","Important: For the BridgeData V2 component, the version in OXE is out of date (as of 12/20/2023). Instead, you should download the dataset from the official website and place it under the subdirectory bridge_orig/. Replace any reference to bridge in the OXE code with bridge_orig.",24 (4 types),41,5,4,1,1,3,WidowX,Single Arm,❌,"can, pot, towel, table, cloth, spoon, knife, board, kadai, banana, napkin, eggplant, wedge, rag, pepper, sink, strawberry, vessel, colander, potato, sushi, plate, fork, cucumber, srewdriver, bowl, mushroom, peeler, lid, cardboardfence, fence, pear, egg, microwave, spatula, cheese, broccoli","FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'open_gripper': bool,
            'rotation_delta': Tensor(shape=(3,), dtype=float32),
            'terminate_episode': float32,
            'world_vector': Tensor(shape=(3,), dtype=float32),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'image': Image(shape=(480, 640, 3), dtype=uint8),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
            'state': Tensor(shape=(7,), dtype=float32),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",EEF Position,,CoRL 2023,"1. Raw to Numpy to TFrecords processing code

2. Downloading RLDS from OXE & OpenVLA preprocessing code (resize_and_jpeg_encode)","1. Robot setup costs ~$4, 000
2. VR teleoperation
3. To support the evaluation of multi-task learning methods, collect data for many possible tasks simultaneously in each environment.
",,"Cameras: (640x480, 5Hz) 
1 RGBD camera fixed in an over-the-shoulder view
2 RGB cameras with poses that are randomized during data collection 
1 RGB camera attached to the robot’s wrist.",,,,,
Austin Mutex,utaustin_mutex,2,FALSE,TRUE,TRUE,TRUE,TRUE,FALSE,"human
down sample",FALSE,1500,TRUE,0.1.0,https://ut-austin-rpl.github.io/MUTEX/,"1.
Carefully navigate your gripper to the air fryer's handle.
Confidently seize the handle using your gripper.
Gradually bring the handle toward yourself to access the air fryer.
2.
With care, position your gripper close to the oven tray handle.
Make sure to grip the handle tightly.
Delicately and attentively pull the tray out from the oven.
3.
Kindly get your arm to the topmost drawer handle.
Get a good grip on it, then softly pull to open it.
4.
Gently grip the air fryer handle with your gripper and kindly pull it open by sliding your gripper away from the base.
Gradually bring your gripper in line with the blue bowl and gently capture it, without tightening your grip too much.
Securely place the blue bowl inside the opened air fryer with a steady position.
Liberate your robot's gripper, and withdraw your gripper from the proximity of the air fryer.
...
44.Kindly position your hand close to the oven tray's handle.
Softly yet firmly grasp the handle using your hand.
Slowly and with caution, draw the tray out until it is fully stretched out.",✅,✅perfect,"1.
Carefully navigate your gripper to the air fryer's handle.
Confidently seize the handle using your gripper.
Gradually bring the handle toward yourself to access the air fryer.
2.
With care, position your gripper close to the oven tray handle.
Make sure to grip the handle tightly.
Delicately and attentively pull the tray out from the oven.
3.
Kindly get your arm to the topmost drawer handle.
Get a good grip on it, then softly pull to open it.
4.
Gently grip the air fryer handle with your gripper and kindly pull it open by sliding your gripper away from the base.
Gradually bring your gripper in line with the blue bowl and gently capture it, without tightening your grip too much.
Securely place the blue bowl inside the opened air fryer with a steady position.
Liberate your robot's gripper, and withdraw your gripper from the proximity of the air fryer.
...
44.Kindly position your hand close to the oven tray's handle.
Softly yet firmly grasp the handle using your hand.
Slowly and with caution, draw the tray out until it is fully stretched out.",,1,234,20,2,0,1,1,Franka,Single Arm,,"object on table like book,mug","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [6x end effector delta pose, 1x gripper position]),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(128, 128, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(24,), dtype=float32, description=Robot state, consists of [7x robot joint angles, 1x gripper position, 16x robot end-effector homogeneous matrix].),
            'wrist_image': Image(shape=(128, 128, 3), dtype=uint8, description=Wrist camera RGB observation.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,,,,,,,(need simplify the instructions),,,,
USC Jaco Play,jaco_play,2,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,"human
down sample",FALSE,976,TRUE,0.1.0,https://github.com/clvrai/clvr_jaco_play_dataset,"Templated
- place the milk dairy in the white plate (4.0%)
- pick up the long bread (4.0%)
- pick up the steak meat (4.0%)
- place the gray bowl in the table (2.0%)
- place the green cup in the oven (2.0%)
...
- place the long bread in the gray bowl (2.0%)
- place the butter dairy in the gray bowl (2.0%)
- pick up the green cup (2.0%)
- place the green cup in the dish rack (2.0%)",✅,1. Adding visual/process details for task repeatition,"1. pick up the {}
2. place {} in the {}",,1,71,10,2,0,1,1,Jaco 2 Arm,Single Arm,,"dairy, bowl, rack, bread, fruit, table, cup, oven, meat, plate, sink","FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'gripper_closedness_action': Tensor(shape=(1,), dtype=float32),
            'terminate_episode': Tensor(shape=(3,), dtype=int32),
            'world_vector': Tensor(shape=(3,), dtype=float32),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'end_effector_cartesian_pos': Tensor(shape=(7,), dtype=float32),
            'end_effector_cartesian_velocity': Tensor(shape=(6,), dtype=float32),
            'image': Image(shape=(224, 224, 3), dtype=uint8),
            'image_wrist': Image(shape=(224, 224, 3), dtype=uint8),
            'joint_pos': Tensor(shape=(8,), dtype=float32),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})


ee_cartesian_pos_ob : end effector cartesian position. ee_cartesian_pos_ob[0:3] corresponds to position and ee_cartesian_pos_ob[3:7] corresponds to orientation in quarternian format
ee_cartesian_vel_ob : end effector cartesian velocity. ee_cartesian_pos_ob[0:3] corresponds to change in position and ee_cartesian_pos_ob[3:6] corresponds to change in orientation in roll, pitch yaw format
",EEF Position,,NA,,,Jaco 2 Arm,,,,,,
Freiburg Franka Play,taco_play,2,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,"human
down sample",FALSE,"3,242",TRUE,0.1.0,https://www.kaggle.com/datasets/oiermees/taco-robot,"Templated
- turn off the blue led light
- grasp the drawer handle, then open it
- go push the pink block into the drawer
- rotate the pink block towards the right
- push the green button to turn off the green light
- turn off the blue light lamp
- turn off the red light lamp
...
- turn the yellow block right
- stack the yellow block on top of the purple block
- put the pink object on the table
- close the drawer",✅,"Seems to be good
- the instructions are diverse enough","pick up the pink block: 41,
grasp the drawer handle and open it: 37,
pull the drawer: 35,
grasp the handle of the drawer and open it: 34,
go open the drawer: 34,
grasp the pink block: 31,
grasp the drawer handle, then open it: 30,
open the cabinet drawer: 30,
......
grasp the pink block and rotate it left: 1,
push the purple block inside the drawer: 1,
place the pink block on top of the yellow block: 1,",,1,66,15,2,2,1,1,Franka Emika Panda,Single Arm,,"blocks, drawer, light button","FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'actions': Tensor(shape=(7,), dtype=float32, description=absolute desired values for gripper pose (first 6 dimensions are x, y, z, yaw, pitch, roll), last dimension is open_gripper (-1 is open gripper, 1 is close)),
            'rel_actions_gripper': Tensor(shape=(7,), dtype=float32, description=relative actions for gripper pose in the gripper camera frame (first 6 dimensions are x, y, z, yaw, pitch, roll), last dimension is open_gripper (-1 is open gripper, 1 is close)),
            'rel_actions_world': Tensor(shape=(7,), dtype=float32, description=relative actions for gripper pose in the robot base frame (first 6 dimensions are x, y, z, yaw, pitch, roll), last dimension is open_gripper (-1 is open gripper, 1 is close)),
            'terminate_episode': float32,
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'depth_gripper': Tensor(shape=(84, 84), dtype=float32),
            'depth_static': Tensor(shape=(150, 200), dtype=float32),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
            'rgb_gripper': Image(shape=(84, 84, 3), dtype=uint8),
            'rgb_static': Image(shape=(150, 200, 3), dtype=uint8, description=RGB static image of shape. (150, 200, 3). Subsampled from (200,200, 3) image.),
            'robot_obs': Tensor(shape=(15,), dtype=float32, description=EE position (3), EE orientation in euler angles (3), gripper width (1), joint positions (7), gripper action (1)),
            'structured_language_instruction': string,
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})","EEF Position

(rel_actions_world is used in OXE)",,,,Human VR,,,,,,,
Language Table,language_table,3,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,no need modify,FALSE,"442,226",FALSE,,https://interactive-language.github.io/,"Natural
1.place
2. push
3. move
4. put
5. slide
6.separate {} to/towards {}",✅,"1. The instructions are quite diverse although there are repeated trajectoires under the same task
2. The initial surroundings/end states maybe different

Note:
1. the observation fps maybe 5
2. the left/right top/bottom maybe not from agents' view",1.place/ 2. push/ 3. move/ 4. put/ 5. slide/ 6.separate {} to/towards {},,1,20,10,1,0,0,1,xArm,Single Arm,,"A fixed set of 8 plastic blocks, comprising 4 colors and 6 shapes","action: shape=(2,), <dtype: 'float32'>
is_first: shape=(), <dtype: 'bool'>
is_last: shape=(), <dtype: 'bool'>
is_terminal: shape=(), <dtype: 'bool'>
observation: (dict)
    effector_target_trshape=(2,), <dtype: 'float32'>
    effector_translation: shape=(2,), <dtype: 'float32'>
    instruction: shape=(512,),anslation:  <dtype: 'int32'>
    rgb: shape=(360, 640, 3), <dtype: 'uint8'>
reward: shape=(), <dtype: 'float32'>",EEF Position,,,,Human VR,,,,,,,
UCSD Kitchen,ucsd_kitchen_dataset_converted_externally_to_rlds,3,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,"human
down sample",FALSE,150,TRUE,,https://www.tensorflow.org/datasets/catalog/ucsd_kitchen_dataset_converted_externally_to_rlds,"Natural
- Open the oven door (21.0%)
- Place the teapot on the stove (20.0%)
- Turn on the faucet (19.0%)
- Put the bowl inside the kitchen cabinet (14.0%)
- Put the white box into the sink (7.0%)
- Open the carbinet door (7.0%)
- Put the green box into the sink (6.0%)
- Put the canned spam into the sink (6.0%)",✅,"1. There are difference in the initial layout of the scene
2. Adding details about the final state/position after actions","- Open the oven door (21.0%)
- Place the teapot on the stove (20.0%)
- Turn on the faucet (19.0%)
- Put the bowl inside the kitchen cabinet (14.0%)
- Put the white box into the sink (7.0%)
- Open the carbinet door (7.0%)
- Put the green box into the sink (6.0%)
- Put the canned spam into the sink (6.0%)",,2,28,2,1,0,0,1,xArm,Single Arm,,"teapot, stove, faucet, box, sink, door, spam, bowl, cabinet","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(8,), dtype=float32, description=8-dimensional action, consisting of end-effector position and orientation, gripper open/close and a episode termination action.),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(480, 640, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(21,), dtype=float32, description=21-dimensional joint states, consists of robot joint angles, joint velocity and joint torque.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,,,,Human VR,,,,,,,
Berkeley MVP Data,berkeley_mvp_converted_externally_to_rlds,3,FALSE,TRUE,FALSE,FALSE,FALSE,FALSE,human,TRUE,480,TRUE,,http://arxiv.org/abs/2210.03109,"
push wooden cube
pick detergent from the sink
pick yellow cube
close fridge door
reach red block
pick fruit",✅,✅,,,2,94,5,1,0,1,0,xArm,Single Arm,,"cube, fridge","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(8,), dtype=float32, description=Robot action, consists of [7 delta joint pos,1x gripper binary state].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'gripper': Scalar(shape=(), dtype=bool, description=Binary gripper state (1 - closed, 0 - open)),
            'hand_image': Image(shape=(480, 640, 3), dtype=uint8, description=Hand camera RGB observation.),
            'joint_pos': Tensor(shape=(7,), dtype=float32, description=xArm joint positions (7 DoF).),
            'pose': Tensor(shape=(7,), dtype=float32, description=Gripper pose, robot frame, [3 position, 4 rotation]),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",Joint position (7 joint + 1 gripper),,CoRL 2022,,human VR,,,,,,,
ASU TableTop Manipulation,asu_table_top_converted_externally_to_rlds,3,FALSE,,TRUE,FALSE,FALSE,FALSE,human,FALSE,110,TRUE,,https://link.springer.com/article/10.1007/s10514-023-10129-1,"put
place
pick
move
push",✅,,"put
place
pick
move
push",,,236,"12,5",1,0,0,1,UR5,Single Arm,,"cube, bottle, carton","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [7x joint velocities, 2x gripper velocities, 1x terminate episode].),
        'action_delta': Tensor(shape=(7,), dtype=float32, description=Robot delta action, consists of [7x joint velocities, 2x gripper velocities, 1x terminate episode].),
        'action_inst': Text(shape=(), dtype=string),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'goal_object': Text(shape=(), dtype=string),
        'ground_truth_states': FeaturesDict({
            'EE': Tensor(shape=(6,), dtype=float32, description=xyzrpy),
            'bottle': Tensor(shape=(6,), dtype=float32, description=xyzrpy),
            'bread': Tensor(shape=(6,), dtype=float32, description=xyzrpy),
            'coke': Tensor(shape=(6,), dtype=float32, description=xyzrpy),
            'cube': Tensor(shape=(6,), dtype=float32, description=xyzrpy),
            'milk': Tensor(shape=(6,), dtype=float32, description=xyzrpy),
            'pepsi': Tensor(shape=(6,), dtype=float32, description=xyzrpy),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(224, 224, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(7,), dtype=float32, description=Robot state, consists of [6x robot joint angles, 1x gripper position].),
            'state_vel': Tensor(shape=(7,), dtype=float32, description=Robot joint velocity, consists of [6x robot joint angles, 1x gripper position].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,,,,scripts,,,action description is incorrect,,,,
KAIST Nonprehensile Objects,kaist_nonprehensile_converted_externally_to_rlds,3,FALSE,,FALSE,FALSE,FALSE,FALSE,human,FALSE,201,TRUE,,https://link.springer.com/article/10.1007/s10514-023-10129-1,"rotate
push
topple
flip
make sth stand up
drag
lift
lie
pull",✅,"resize image
take a subset of action
instruction is ✅","rotate
push
topple
flip
make sth stand up
drag
lift
lie
pull",,1,155,10,1,0,0,1,franka,Single Arm,,table top objects,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(20,), dtype=float32, description=Robot action, consists of [3x end-effector position residual, 3x end-effector axis-angle residual, 7x robot joint k_p gain coefficient, 7x robot joint damping ratio coefficient].The action residuals are global, i.e. multiplied on theleft-hand side of the current end-effector state.),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(480, 640, 3), dtype=uint8, description=Main camera RGB observation.),
            'partial_pointcloud': Tensor(shape=(512, 3), dtype=float32, description=Partial pointcloud observation),
            'state': Tensor(shape=(21,), dtype=float32, description=Robot state, consists of [joint_states, end_effector_pose].Joint states are 14-dimensional, formatted in the order of [q_0, w_0, q_1, w_0, ...].In other words,  joint positions and velocities are interleaved.The end-effector pose is 7-dimensional, formatted in the order of [position, quaternion].The quaternion is formatted in (x,y,z,w) order. The end-effector pose references the tool frame, in the center of the two fingers of the gripper.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})","20: 3 delta eef-pos, 3 delta eef-rot, 7 k_p, 7 damping ratio",,,,,Franka,,,,,,
BC-Z,bc_z,2,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,ai,FALSE,"39,350",TRUE,0.1.0,https://sites.google.com/view/bc-z/home,"Templated
- place
- pick up
- wipe
- stack
- knock
- drag
- push
- move
- others",✅,"The difference between trajectoires lies in:
- Variations in object positions
- Variations in scene background due to collecting in multiple locations
- Minor hardware differences between each robot
- Variation in object instances
- Multiple distractor objects (4-5)

Therefore we can add details about these variants

NOTE: there are unsuccessful episodes","Complete list are in Tab.7/8

- place the bottle in the ceramic bowl (8.0%)
- place the white sponge in the ceramic bowl (6.0%)
- place the eraser on the white sponge (6.0%)
- pick up the ceramic bowl (6.0%)
- place the ceramic cup in the ceramic bowl (6.0%)
- place eraser in metal cup (4.0%)
- wipe tray with sponge (4.0%)
- wipe table surface with eraser (4.0%)
- others",,1,132,10,1,0,0,1,Google Robot,Mobile Manipulator,,"sponge, bowl, bottle, tray, cup, eraser, pepper, brush, surface, towel, table, ...","FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'future/axis_angle_residual': Tensor(shape=(30,), dtype=float32, description=The next 10 actions for the rotation. Each action is a 3D delta to add to the current axis angle.),
            'future/target_close': Tensor(shape=(10,), dtype=int64, description=The next 10 actions for the gripper. Each action is the value the gripper closure should be changed to (notably it is *not* a delta.)),
            'future/xyz_residual': Tensor(shape=(30,), dtype=float32, description=The next 10 actions for the positions. Each action is a 3D delta to add to current position.),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'episode_success': float32,
            'image': Image(shape=(171, 213, 3), dtype=uint8, description=Camera image of the robot, downsampled 3x),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32, description=An embedding of the task via Universal Sentence Encoder (https://tfhub.dev/google/universal-sentence-encoder/4)),
            'natural_language_instruction': string,
            'present/autonomous': int64,
            'present/axis_angle': Tensor(shape=(3,), dtype=float32, description=The current rotation of the end effector in axis-angle representation.),
            'present/intervention': int64,
            'present/sensed_close': Tensor(shape=(1,), dtype=float32, description=How much the gripper is currently closed. Scaled from 0 to 1, but not all values from 0 to 1 are reachable. The range in the data is about 0.2 to 1),
            'present/xyz': Tensor(shape=(3,), dtype=float32, description=The current position of the end effector in axis-angle representation, in robot frame),
            'sequence_length': int64,
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",EEF Position,,CoRL 2021,,Human VR,,,,,,,
RT-1 Robot Action,fractal20220817_data,2,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,ai,FALSE,"87,212",TRUE,0.1.0,https://ai.googleblog.com/2022/12/rt-1-robotics-transformer-for-real.html,"Templated
- pick object (17.5%)
- move object near object (45.3%)
- place object upright (1.1%)
- knock object over (1.1%)
- open drawer (0.4%)
- close drawer (0.4%)
- place object into receptacle (11.3%)
- pick object from receptacle and place on the counter (21.8%)",✅,It is quite diverse and there are no much repetition under the same instruction,"- pick object (17.5%)
- move object near object (45.3%)
- place object upright (1.1%)
- knock object over (1.1%)
- open drawer (0.4%)
- close drawer (0.4%)
- place object into receptacle (11.3%)
- pick object from receptacle and place on the counter (21.8%)",,NA,45,3,1,1,0,1,Google Robot,Mobile Manipulator,,"chocolate, drawer, counter, apple, bowl, coke, bottle, bag, sponge, shelf, fridge, orange, banana, blueberry, green, pepsi, can","FeaturesDict({
    'aspects': FeaturesDict({
        'already_success': bool,
        'feasible': bool,
        'has_aspects': bool,
        'success': bool,
        'undesirable': bool,
    }),
    'attributes': FeaturesDict({
        'collection_mode': int64,
        'collection_mode_name': string,
        'data_type': int64,
        'data_type_name': string,
        'env': int64,
        'env_name': string,
        'location': int64,
        'location_name': string,
        'objects_family': int64,
        'objects_family_name': string,
        'task_family': int64,
        'task_family_name': string,
    }),
    'steps': Dataset({
        'action': FeaturesDict({
            'base_displacement_vector': Tensor(shape=(2,), dtype=float32),
            'base_displacement_vertical_rotation': Tensor(shape=(1,), dtype=float32),
            'gripper_closedness_action': Tensor(shape=(1,), dtype=float32, description=continuous gripper position),
            'rotation_delta': Tensor(shape=(3,), dtype=float32, description=rpy commanded orientation displacement, in base-relative frame),
            'terminate_episode': Tensor(shape=(3,), dtype=int32),
            'world_vector': Tensor(shape=(3,), dtype=float32, description=commanded end-effector displacement, in base-relative frame),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'base_pose_tool_reached': Tensor(shape=(7,), dtype=float32, description=end-effector base-relative position+quaternion pose),
            'gripper_closed': Tensor(shape=(1,), dtype=float32),
            'gripper_closedness_commanded': Tensor(shape=(1,), dtype=float32, description=continuous gripper position),
            'height_to_bottom': Tensor(shape=(1,), dtype=float32, description=height of end-effector from ground),
            'image': Image(shape=(256, 320, 3), dtype=uint8),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
            'orientation_box': Tensor(shape=(2, 3), dtype=float32),
            'orientation_start': Tensor(shape=(4,), dtype=float32),
            'robot_orientation_positions_box': Tensor(shape=(3, 3), dtype=float32),
            'rotation_delta_to_go': Tensor(shape=(3,), dtype=float32, description=rotational displacement from current orientation to target),
            'src_rotation': Tensor(shape=(4,), dtype=float32),
            'vector_to_go': Tensor(shape=(3,), dtype=float32, description=displacement from current end-effector position to target),
            'workspace_bounds': Tensor(shape=(3, 3), dtype=float32),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",EEF Position,,,,Human VR,,,,,,,
dobb-e,dobbe,2,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,human,FALSE,"5,620",TRUE,0.0.1,https://dobb-e.com/#,"Templated（many repeat）
1. switching button  521
2. door openining   1258
3. door closing  955 
4. drawer opening 675
5. drawer closing 694
6. pick and place 696
7. handle grasping 409
ignore the. play data ",✅,"1. such robot is not public , quit different from ours
2. Need to re-write the intruction in a fine-grained manner
3. only one first-person image","1. switching button  521
2. door openining   1258
3. door closing  955 
4. drawer opening 675
5. drawer closing 694
6. pick and place 696
7. handle grasping 409
( many type ,anything around their household that they would like to do using the stick， ignore the play data)",,can't calcutate,227,30,1,1,1,0,  Hello Stretch,Single Arm,,anything around household,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': string,
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32),
        'discount': float32,
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32),
        'language_instruction': string,
        'observation': FeaturesDict({
            'gripper': Tensor(shape=(1,), dtype=float32),
            'quat': Tensor(shape=(4,), dtype=float32),
            'rot': Tensor(shape=(3,), dtype=float32),
            'state': Tensor(shape=(7,), dtype=float32),
            'wrist_image': Image(shape=(256, 256, 3), dtype=uint8),
            'xyz': Tensor(shape=(3,), dtype=float32),
        }),
        'reward': float32,
    }),
})

quat is the quaternion e.g ( w, x, y, z)
rot is rotation vector.  both used to describe the rotation",EEF position,✅,,,Human collect using tools ,"
Hello Stretch",,need fine-grad annotation / instruction,,,,
MimicPlay,mimic_play,2,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,human,FALSE,378,TRUE,0.0.1,https://mimic-play.github.io/,"dummy, no specific instruciton ",✅,add fine-grad instructiono,"Random operation in 5 scenes. No specific instructions. 
e.g Put flowers in the vase in the ""flower and vase"" scene",,5,779,15,3,0,1,2,Franka panda,Single Arm,,"flower , vase, toy , ","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': string,
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32),
        'discount': Scalar(shape=(), dtype=float32),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32),
        'language_instruction': string,
        'observation': FeaturesDict({
            'image': FeaturesDict({
                'front_image_1': Image(shape=(120, 120, 3), dtype=uint8),
                'front_image_2': Image(shape=(120, 120, 3), dtype=uint8),
            }),
            'state': FeaturesDict({
                'ee_pose': Tensor(shape=(7,), dtype=float32),
                'gripper_position': float32,
                'joint_positions': Tensor(shape=(7,), dtype=float32),
                'joint_velocities': Tensor(shape=(7,), dtype=float32),
            }),
            'wrist_image': FeaturesDict({
                'wrist_image': Image(shape=(120, 120, 3), dtype=uint8),
            }),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",,,CoRL 2023,human teleoperate,,,,,,,,
UIUCD3Field,uiuc_d3field,2,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,human,FALSE,196,TRUE,1.1.2,https://robopil.github.io/d3fields/,dummy instruction,✅,"1. action sapce need to change , now is 3D, we need 7D
2. write the instruction for each episode ",dummy instruction,,1,70,1,4,4,0,4,Kinova Gen3,Single Arm,,"utensile,mug,shoe and other things in desk top","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(3,), dtype=float32, description=Robot displacement from last frame),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'depth_1': Image(shape=(360, 640, 1), dtype=uint16, description=camera 1 depth observation.),
            'depth_2': Image(shape=(360, 640, 1), dtype=uint16, description=camera 2 depth observation.),
            'depth_3': Image(shape=(360, 640, 1), dtype=uint16, description=camera 3 depth observation.),
            'depth_4': Image(shape=(360, 640, 1), dtype=uint16, description=camera 4 depth observation.),
            'image_1': Image(shape=(360, 640, 3), dtype=uint8, description=camera 1 RGB observation.),
            'image_2': Image(shape=(360, 640, 3), dtype=uint8, description=camera 2 RGB observation.),
            'image_3': Image(shape=(360, 640, 3), dtype=uint8, description=camera 3 RGB observation.),
            'image_4': Image(shape=(360, 640, 3), dtype=uint8, description=camera 4 RGB observation.),
            'state': Tensor(shape=(4, 4), dtype=float32, description=Robot end-effector state),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,,,,,,,,,,,
Berkeley Fanuc Manipulation,berkeley_fanuc_manipulation,2,TRUE,TRUE,TRUE,FALSE,FALSE,FALSE,human,TRUE,415,TRUE,0.1.0,https://sites.google.com/berkeley.edu/fanuc-manipulation,"1.Handover the object on the table to the human.
2.Pour the objects in the blue cup to the white cup.
3.Pick up the tape and put it in the box.
4.Pick up the toy duck and put it on the goal location represented using a cross.
5.Stack cups together.
6.Pick up the banana and place it in the drawer.
7.Press the stapler.
8.Pick up the object and place it in the box.
9.Separate the stacked cups..
10.Use the stick to swipe the cup.
11.Open the drawer and place the battery in the drawer.
12.Pick up the cup and put it on the table.
13.Pick up the screw driver and place it in the box.
14.Pick up the battery and place it in the open drawer.
15.Pick up the battery.
16.Push the cup the goal location represented using a cross.
17.Close the drawer.
18.Open the drawer.
19.Close Water kettle .
20.Pick up the brush and sweep the table.
21.open the box
22.Pick up the object under the table and put it on top of the table.
23.Pick up the brush and put it on the table.
24.Close the laptop.
25.Pick up the two cups together then put them down.
26.Push the cube to the goal location represented using a cross.
27.Disassemble the object
28.Pick up the object on the table and place it in the cup.
29.Pick up the level and place it on the cup.
30.Pick up the object on the table then put it down.
31.Pick up the pen and place it in the cup.
32.Disassemble the object.",✅,1. action is 6D ，without gripper demention,"1.Handover the object on the table to the human.
2.Pour the objects in the blue cup to the white cup.
3.Pick up the tape and put it in the box.
4.Pick up the toy duck and put it on the goal location represented using a cross.
5.Stack cups together.
6.Pick up the banana and place it in the drawer.
7.Press the stapler.
8.Pick up the object and place it in the box.
9.Separate the stacked cups..
10.Use the stick to swipe the cup.
11.Open the drawer and place the battery in the drawer.
12.Pick up the cup and put it on the table.
13.Pick up the screw driver and place it in the box.
14.Pick up the battery and place it in the open drawer.
15.Pick up the battery.
16.Push the cup the goal location represented using a cross.
17.Close the drawer.
18.Open the drawer.
19.Close Water kettle .
20.Pick up the brush and sweep the table.
21.open the box
22.Pick up the object under the table and put it on top of the table.
23.Pick up the brush and put it on the table.
24.Close the laptop.
25.Pick up the two cups together then put them down.
26.Push the cube to the goal location represented using a cross.
27.Disassemble the object
28.Pick up the object on the table and place it in the cup.
29.Pick up the level and place it on the cup.
30.Pick up the object on the table then put it down.
31.Pick up the pen and place it in the cup.
32.Disassemble the object.",,1,150,10,2,0,1,1,Fanuc Mate,Single Arm,,table top objects,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(6,), dtype=float32, description=Robot action, consists of [dx, dy, dz] and [droll, dpitch, dyaw]),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'end_effector_state': Tensor(shape=(7,), dtype=float32, description=Robot gripper end effector state, consists of [x, y, z] and 4x quaternion),
            'image': Image(shape=(224, 224, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(13,), dtype=float32, description=Robot joints state, consists of [6x robot joint angles, 1x gripper open status, 6x robot joint velocities].),
            'wrist_image': Image(shape=(224, 224, 3), dtype=uint8, description=Wrist camera RGB observation.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,,,,,,,(add some details),,,,
QT-Opt,kuka,2,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,no need modify,FALSE,"580,392",FALSE,,https://arxiv.org/abs/1806.10293,None,✅,"1. generating natural language instruction, especially for objects
2. remove repeatition because it is simple task",Only grasp different objects,"Maybe need to filter success-only data, since the trajectory are collected automatically via Neural Networks",1,14,10,1,0,0,1,LBRIIWA arm,Single Arm,❌,NA,"FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'base_displacement_vector': Tensor(shape=(2,), dtype=float32),
            'base_displacement_vertical_rotation': Tensor(shape=(1,), dtype=float32),
            'gripper_closedness_action': Tensor(shape=(1,), dtype=float32),
            'rotation_delta': Tensor(shape=(3,), dtype=float32),
            'terminate_episode': Tensor(shape=(3,), dtype=int32),
            'world_vector': Tensor(shape=(3,), dtype=float32),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'clip_function_input/base_pose_tool_reached': Tensor(shape=(7,), dtype=float32),
            'clip_function_input/workspace_bounds': Tensor(shape=(3, 3), dtype=float32),
            'gripper_closed': Tensor(shape=(1,), dtype=float32),
            'height_to_bottom': Tensor(shape=(1,), dtype=float32),
            'image': Image(shape=(512, 640, 3), dtype=uint8),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
            'task_id': Tensor(shape=(1,), dtype=float32),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
    'success': bool,
})",EEF Position,,CoRL 2018,,"Data was collected with 7 LBR IIWA robots, with 4-10 training objects per robot. The objects were replaced every 4 hours during business hours.

(collected by off-policy QT-Opt algorithm)",,,,,,,
Roboturk,roboturk,2,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,ai,FALSE,"1,796",TRUE,0.1.0,https://roboturk.stanford.edu/dataset_real.html,"Templated
1. object search
2. create tower
3. layout laundry",✅,Adding visual/process details for tasks,"1. object search
2. create tower
3. layout laundry",,1,88,10,1,0,0,1,Sawyer,Single Arm,,"cloth, cups, toys","FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'gripper_closedness_action': Tensor(shape=(1,), dtype=float32),
            'rotation_delta': Tensor(shape=(3,), dtype=float32),
            'terminate_episode': float32,
            'world_vector': Tensor(shape=(3,), dtype=float32),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'front_rgb': Image(shape=(480, 640, 3), dtype=uint8),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",EEF Position,,,,Human VR,,,"1. It seems that the original RGB FPS=1, while the control freq=10",,,,
NYU Franka Play,nyu_franka_play_dataset_converted_externally_to_rlds,2,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,human,FALSE,365,TRUE,0.1.0,https://play-to-policy.github.io/,"Templated
- play with the kitchen",✅,"1. A lot of details can be added, since the palying process has many differences for different trajectoies",play with the kitchen,,1,86,3,2,2,0,2,Franka,Single Arm,,"cabinet, pot, drawer","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(15,), dtype=float32, description=Robot action, consists of [7x joint velocities, 3x EE delta xyz, 3x EE delta rpy, 1x gripper position, 1x terminate episode].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'depth': Tensor(shape=(128, 128, 1), dtype=int32, description=Right camera depth observation.),
            'depth_additional_view': Tensor(shape=(128, 128, 1), dtype=int32, description=Left camera depth observation.),
            'image': Image(shape=(128, 128, 3), dtype=uint8, description=Right camera RGB observation.),
            'image_additional_view': Image(shape=(128, 128, 3), dtype=uint8, description=Left camera RGB observation.),
            'state': Tensor(shape=(13,), dtype=float32, description=Robot state, consists of [7x robot joint angles, 3x EE xyz, 3x EE rpy.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF velocity,,,,Human VR,,,,,,,
Berkeley RPT Data,berkeley_rpt_converted_externally_to_rlds,2,FALSE,TRUE,TRUE,FALSE,FALSE,FALSE,ai,FALSE,908,TRUE,0.1.0,https://arxiv.org/abs/2306.10007,"destack cube
stack cube
pick an object from the bin
pick yellow cube",✅,✅,"pick an object from the bin
pick yellow cube 
destack cube
stack cube",,1,459,30,3,0,1,2,franka,Single Arm,,K cubes,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(8,), dtype=float32, description=Robot action, consists of [7 delta joint pos,1x gripper binary state].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'gripper': Scalar(shape=(), dtype=bool, description=Binary gripper state (1 - closed, 0 - open)),
            'hand_image': Image(shape=(480, 640, 3), dtype=uint8, description=Hand camera RGB observation.),
            'joint_pos': Tensor(shape=(7,), dtype=float32, description=xArm joint positions (7 DoF).),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",Joint position (7 joint + 1 gripper),,CoRL 2023,,scripts,,,,,,,
LSMO Dataset,,0,FALSE,FALSE,,,,,,,50,,,,"avoid obstacle and reach the blue pen
avoid obstacle and reach the scissors",✅,modify action,"avoid obstacle and reach the blue pen
avoid obstacle and reach the scissors",,1,239,10,1,0,0,1,Cobotta,Single Arm,,"obstacle
pen
scissors","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [3x endeffector position, 3x euler angles,1x gripper action].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(120, 120, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(13,), dtype=float32, description=Robot state, consists of [3x endeffector position, 3x euler angles,6x robot joint angles, 1x gripper position].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})","EEF velocity
3 eef pose
3 euler angle
1 gripper action",,,,Expert Policy,,,（可以改但并不diverse),,,,
Austin VIOLA,,0,TRUE,TRUE,,,,,,,135,,,,"Templated
1. Dining-Bowl (pick up the bowl from the lazy Susan and put bowl on the plate)
2. Make-Coffee (make coffee)
3. Dining-PlateFork (arrange plate and fork)",✅,"1. Adding visual/process details for overall tasks
2. Maybe can split into sub-tasks","1. Dining-Bowl (pick up the bowl from the lazy Susan and put bowl on the plate)
2. Make-Coffee (make coffee)
3. Dining-PlateFork (arrange plate and fork)",,1,516,20,2,0,1,1,Franka,Single Arm,,"1. plate
2. bowl
3. fork
4. coffee machine","FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'gripper_closedness_action': float32,
            'rotation_delta': Tensor(shape=(3,), dtype=float32),
            'terminate_episode': float32,
            'world_vector': Tensor(shape=(3,), dtype=float32),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'agentview_rgb': Image(shape=(224, 224, 3), dtype=uint8, description=RGB captured by workspace camera),
            'ee_states': Tensor(shape=(16,), dtype=float32, description=Pose of the end effector specified as a homogenous matrix.),
            'eye_in_hand_rgb': Image(shape=(224, 224, 3), dtype=uint8, description=RGB captured by in hand camera),
            'gripper_states': Tensor(shape=(1,), dtype=float32, description=gripper_states = 0 means the gripper is fully closed. The value represents the gripper width of Franka Panda Gripper.),
            'joint_states': Tensor(shape=(7,), dtype=float32, description=joint values),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",EEF Position,,,,Human Spacemouse,,,,,,,
CMU Franka Pick-Insert Data,,0,FALSE,FALSE,,,,,,,631,,,,"Insert greeen block.
Pick up pink block.
Pick up yellow block.
Pick up narrow blue stick.
Pick up green block.
Pick up blue block.
Pick up pink flower.",✅,modify action,"Insert
pick up",,1,230,20,2,0,1,1,Franka,Single Arm,,,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(8,), dtype=float32, description=Robot action, consists of [3x end-effector position, 4x end-effector quaternion, 1x gripper open/close].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(360, 640, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(20,), dtype=float32, description=Robot state, consists of [7x robot joint angles, 1x gripper status, 6x joint torques, 6x end-effector force].),
            'wrist_image': Image(shape=(240, 320, 3), dtype=uint8, description=Wrist camera RGB observation.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})","EEF Position
3 eef pose
4 eef quat
1 gripper
",,,,,,,同一个Task中episode可以分为几个比较同质化的group,,,,
NYU ROT,,0,FALSE,FALSE,,,,,,,14,,,,"Templated
- insert the peg in the cup (21.4%)
- erase the board (7.1%)
- pour the almonds into the cup (7.1%)
- hang the bag on the hook (7.1%)
- open the box (7.1%)
- hang the mug on the hook (7.1%)
- reach the blue mark on the table (7.1%)
- hang the hanger on the rod (7.1%)
- press the button (7.1%)
- close the door (7.1%)
- turn the knob (7.1%)
- stack the cups (7.1%)",✅,1. The resolution is low and very small-scale,"- insert the peg in the cup (21.4%)
- erase the board (7.1%)
- pour the almonds into the cup (7.1%)
- hang the bag on the hook (7.1%)
- open the box (7.1%)
- hang the mug on the hook (7.1%)
- reach the blue mark on the table (7.1%)
- hang the hanger on the rod (7.1%)
- press the button (7.1%)
- close the door (7.1%)
- turn the knob (7.1%)
- stack the cups (7.1%)",,1,31,3,1,0,0,1,xArm,Single Arm,,"board, almonds, cup, bag, hook, box, mug, mark, table, hanger, rod, button, door, peg, knob, cups
","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [3x robot end effector delta positions, 3x robot end effector rotations (roll, pitch, yaw),1x gripper open/close (0-open, 1-closed)].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(84, 84, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(7,), dtype=float32, description=Robot state, consists of [3x robot end effector positions, 3x robot end effector rotations (roll, pitch, yaw),1x gripper open/close (0-open, 1-closed)].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,,,,Human Joystick,,,,,,,
Stanford HYDRA,,0,TRUE,FALSE,,,,,,,550,,,,"Templated
- make a cup of coffee with the keurig machine
- make a piece of toast with the oven
- palce dishes in the dish rack",✅,1. decompose into atmoic tasks because the avg.frames is large,"make a cup of coffee with the keurig machine
make a piece of toast with the oven
palce dishes in the dish rack",,2,646,10,2,0,1,1,Franka,Single Arm,,"Oven, plate, bread, keurig machine, cup, dish, spoon, rack","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [3x EEF positional delta, 3x EEF orientation delta in euler angle, 1x close gripper].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_dense': Scalar(shape=(), dtype=bool, description=True if state is a waypoint(010) or in dense mode(x111).),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(240, 320, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(27,), dtype=float32, description=Robot state, consists of [3x EEF position,4x EEF orientation in quaternion,3x EEF orientation in euler angle,7x robot joint angles, 7x robot joint velocities,3x gripper state.),
            'wrist_image': Image(shape=(240, 320, 3), dtype=uint8, description=Wrist camera RGB observation.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,,,,Human VR,,,,,,,
Austin Sailor,,0,TRUE,FALSE,,,,,,,250,,,,"None
- Interact with the objects in diverse but meaningful ways (64.0%)
- Place the fish, sausage, and tomato into the frying pan (14.0%)
- Place the pan onto the stove and place the fish and sausage into the pan (12.0%)
- Place the bread, butter, and milk from the table onto the serving area (10.0%)",✅,"1. decompose the task since the episodes are very long
2. adding visual and process details
3. the resolution of this dataset is very low","- Interact with the objects in diverse but meaningful ways (64.0%) 
- Place the fish, sausage, and tomato into the frying pan (14.0%) 
- Place the pan onto the stove and place the fish and sausage into the pan (12.0%) 
- Place the bread, butter, and milk from the table onto the serving area (10.0%)",,2,"1,628",20,2,0,1,1,Franka,Single Arm,,"ways, fish, pan, bread, table, stove","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [3x ee relative pos, 3x ee relative rotation, 1x gripper action].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(128, 128, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(8,), dtype=float32, description=Default robot state, consists of [3x robot ee pos, 3x ee quat, 1x gripper state].),
            'state_ee': Tensor(shape=(16,), dtype=float32, description=End-effector state, represented as 4x4 homogeneous transformation matrix of ee pose.),
            'state_gripper': Tensor(shape=(1,), dtype=float32, description=Robot gripper opening width. Ranges between ~0 (closed) to ~0.077 (open)),
            'state_joint': Tensor(shape=(7,), dtype=float32, description=Robot 7-dof joint information (not used in original SAILOR dataset).),
            'wrist_image': Image(shape=(128, 128, 3), dtype=uint8, description=Wrist camera RGB observation.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=True on last step of the episode.),
    }),
})",EEF velocity,,,,Human Spacemouse,,,,,,,
Furniture Bench,,0,TRUE,FALSE,,,,,,,"5,100",,,,"Templated
- assemble cabinet (6.0%)
- assemble one_leg (52.0%)
- assemble stool (10.0%)
- assemble drawer (10.0%)
- assemble lamp (10.0%)
- assemble round_table (6.0%)
- assemble chair (4.0%)
- assemble square_table (2.0%)",✅,"1. The tasks are long-horizon tasks with high variance of duration, so we need to decompose into fine-grained thoughts and actions
2. The trajectoies under the same task seem to have the same process","- assemble cabinet (6.0%)
- assemble one_leg (52.0%)
- assemble stool (10.0%)
- assemble drawer (10.0%)
- assemble lamp (10.0%)
- assemble round_table (6.0%)
- assemble chair (4.0%)
- assemble square_table (2.0%)",,1,710,10,2,0,1,1,Franka,Single Arm,,"cabinet, stool, drawer, lamp, chair, table","action: shape=(8,), <dtype: 'float32'>
discount: shape=(), <dtype: 'float32'>
is_first: shape=(), <dtype: 'bool'>
is_last: shape=(), <dtype: 'bool'>
is_terminal: shape=(), <dtype: 'bool'>
language_embedding: shape=(512,), <dtype: 'float32'>
language_instruction: shape=(), <dtype: 'string'>
observation: (dict)
    image: shape=(224, 224, 3), <dtype: 'uint8'>
    state: shape=(35,), <dtype: 'float32'>
    wrist_image: shape=(224, 224, 3), <dtype: 'uint8'>
reward: shape=(), <dtype: 'float32'>
skill_completion: shape=(), <dtype: 'float32'>",EEF velocity,,,,Human VR,,,,,,,
UCSD Pick Place,,0,FALSE,FALSE,,,,,,,"1,355",,,,"Templated
- pick up the red object from the table (96.0%)
- place the pot in the sink (4.0%)",✅,"1. The quality is not high because it is not teleoperation
2. only simple pick-and-place","- pick up the red object from the table (525 successed)
- place the pot in the sink (44 successes)",,2,,3,1,0,0,1,xArm,Single Arm,,"table, pot, sink","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'disclaimer': Text(shape=(), dtype=string),
        'file_path': Text(shape=(), dtype=string),
        'n_transitions': Scalar(shape=(), dtype=int32, description=Number of transitions in the episode.),
        'success': Scalar(shape=(), dtype=bool, description=True if the last state of an episode is a success state, False otherwise.),
        'success_labeled_by': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(4,), dtype=float32, description=Robot action, consists of [3x gripper velocities,1x gripper open/close torque].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(224, 224, 3), dtype=uint8, description=Camera RGB observation.),
            'state': Tensor(shape=(7,), dtype=float32, description=Robot state, consists of [3x gripper position,3x gripper orientation, 1x finger distance].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF velocity,,,,Expert Policy,,,,,,,
Tokyo PR2 Tabletop Manipulation,,0,FALSE,FALSE,,,,,,,192,,,,"None
- picking a bread (38.0%)
- folding a cloth (34.0%)
- picking a grape (28.0%)",✅,"The difference between trajectoires lies in:
- grasp point
- final state","- picking a bread (38.0%)
- folding a cloth (34.0%)
- picking a grape (28.0%)",,1,136,10,1,0,0,1,PR2,Single Arm,,"cloth, bread, grape","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(8,), dtype=float32, description=Robot action, consists of [3x end effector pos, 3x robot rpy angles, 1x gripper open/close command, 1x terminal action].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(128, 128, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(7,), dtype=float32, description=Robot state, consists of [3x end effector pos, 3x robot rpy angles, 1x gripper position].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,,,,Human VR,,,,,,,
UTokyo xArm Bimanual,,0,FALSE,FALSE,,,,,,,70,,,,"None
- Reach a towel (50.0%)
- Unfold a wrinkled towel (50.0%)",✅,"The difference lies in the unfolding task, but overall trajectoires are very similar","- Reach a towel (50.0%)
- Unfold a wrinkled towel (50.0%)",,1,24,10,1,0,0,1,xArm Bimanual,Bi-Manual,,towel,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(14,), dtype=float32, description=Robot action, consists of [3x EEF position (L), 3x EEF orientation yaw/pitch/roll (L), 1x gripper open/close position (L), 3x EEF position (R), 3x EEF orientation yaw/pitch/roll (R), 1x gripper open/close position (R)].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'action_l': Tensor(shape=(7,), dtype=float32, description=Left robot action, consists of [3x EEF position, 3x EEF orientation yaw/pitch/roll].),
            'action_r': Tensor(shape=(7,), dtype=float32, description=Right robot action, consists of [3x EEF position, 3x EEF orientation yaw/pitch/roll, 1x gripper open/close position].),
            'image': Image(shape=(256, 256, 3), dtype=uint8, description=Main camera RGB observation.),
            'pose_l': Tensor(shape=(6,), dtype=float32, description=Left robot end effector pose, consists of [3x EEF position, 3x EEF orientation yaw/pitch/roll].),
            'pose_r': Tensor(shape=(6,), dtype=float32, description=Right robot end effector pose, consists of [3x EEF position, 3x EEF orientation yaw/pitch/roll].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,,,,Human Puppeteering,,,,,,,
DLR Sara Pour Dataset,,0,FALSE,,,,,,,,100,,,,Pour into the mug,✅,modify action,Pour into the mug,,1,130,10,1,0,0,1,DLR SARA,Single Arm,,Household objects,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [3x robot EEF position, 3x robot EEF orientation yaw/pitch/roll calculated with scipy Rotation.as_euler(=""zxy"") Class].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(480, 640, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(6,), dtype=float32, description=Robot state, consists of [3x robot EEF position, 3x robot EEF orientation yaw/pitch/roll calculated with scipy Rotation.as_euler(=""zxy"") Class].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})","EEF velocity
3 eef pose
3 euler angle
1 gripper action",,,,Expert Policy,,,"（可以改并不diverse,感觉人已经改完了）",,,,
Imperial Wrist Cam,,0,FALSE,,,,,,,,170,,,,"put apple in pot 
open lid 
pick up apple 
open bottle 
pick up kettle 
hang cup 
swipe 
grasp can 
pour in mug 
pick up mug 
insert cap in bottle 
pick up pan 
put cup in dishwasher 
insert toast 
stack bowls 
pick up bottle 
pick up shoe ",✅,modify action,"put
open
pick up
hang
swipe
grasp
pour
insert
stack",,1,43,10,1,0,1,0,Sawyer,Single Arm,,kitchen household,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(8,), dtype=float32, description=Robot action, consists of 3x delta position in EEF frame, 3x delta ZYX euler angles, 1x gripper open/close, 1x terminate episode.),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(64, 64, 3), dtype=uint8, description=Main camera RGB observation (same as wrist in our case).),
            'state': Tensor(shape=(1,), dtype=float32, description=Gripper state (opened or closed)),
            'wrist_image': Image(shape=(64, 64, 3), dtype=uint8, description=Wrist camera RGB observation.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})","EEF Position
3 eef delta pose
3 zyx euler angles
1 gripper
1 terminate episode",,,,Human Kinesthetic,,,,,,,
"NYU VINN
",,0,FALSE,FALSE,,,,,,,435,,,,"None
- Only open door","✅
","1. Maybe add some details about left/right handler
2. Maybe filter repetition in the same scene
3. Action space is wrong in OXE (EEF velocity instead of position)",Only open door,,NA,41,3,1,0,1,0,Hello Stretch,Mobile Manipulator,,canbinets with doors,"FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'gripper_closedness_action': Tensor(shape=(1,), dtype=float32),
            'rotation_delta': Tensor(shape=(3,), dtype=float32, description=Angular velocity around x, y and z axis.),
            'terminate_episode': float32,
            'world_vector': Tensor(shape=(3,), dtype=float32, description=Velocity in XYZ.),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'image': Image(shape=(720, 960, 3), dtype=uint8),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",EEF velocity,,,,Human Kinesthetic,,,,,,,
Berkeley Autolab UR5,,0,TRUE,FALSE,,,,,,,896,,,,"Templated
1. Take the tiger out of the red bowl and put it in the grey bowl.
2. Sweep the green cloth to the left side of the table.
3. Pick up the blue cup and put it into the brown cup.
4. Put the ranch bottle into the pot.",✅,Adding some details for the specific action,"1. Take the tiger out of the red bowl and put it in the grey bowl.
2. Sweep the green cloth to the left side of the table.
3. Pick up the blue cup and put it into the brown cup.
4. Put the ranch bottle into the pot.",,1,95,5,2,1,1,1,UR5,Single Arm,,"tiger, red bowl, grey bowl, green cloth, blue cup, brown cup, ranch bottle ,pot","FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'gripper_closedness_action': float32,
            'rotation_delta': Tensor(shape=(3,), dtype=float32, description=Delta change in roll, pitch, yaw.),
            'terminate_episode': float32,
            'world_vector': Tensor(shape=(3,), dtype=float32, description=Delta change in XYZ.),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'hand_image': Image(shape=(480, 640, 3), dtype=uint8),
            'image': Image(shape=(480, 640, 3), dtype=uint8),
            'image_with_depth': Image(shape=(480, 640, 1), dtype=float32),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
            'robot_state': Tensor(shape=(15,), dtype=float32, description=Explanation of the robot state can be found at https://sites.google.com/corp/view/berkeley-ur5),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",EEF Position,,,,Human Spacemouse,,,,,,,
TOTO Benchmark,,0,TRUE,FALSE,,,,,,,901,,,,"Templated
1. pour the material into a target cup on the table
2. scoop material from the bowl into the spoon (not in OXE?)",✅,"1. Seems that only pour task is in OXE
2. Adding details","1. pour the material into a target cup on the table
2. scoop material from the bowl into the spoon (not in OXE?)",,1,312,30,1,0,0,1,"Franka

(joint pos)",Single Arm,,"Cup, containers, materials","FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'open_gripper': bool,
            'rotation_delta': Tensor(shape=(3,), dtype=float32),
            'terminate_episode': float32,
            'world_vector': Tensor(shape=(3,), dtype=float32),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'image': Image(shape=(480, 640, 3), dtype=uint8),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
            'state': Tensor(shape=(7,), dtype=float32, description=numpy array of shape (7,). Contains the robot joint states (as absolute joint angles) at each timestep),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",Joint position,,,,"Human teleoperation -- VR Teleop, trained state-based BC policies, and trajectory replay with noise",,,,,,,
Austin BUDS,,0,FALSE,FALSE,,,,,,,50,,,,"Take the lid off the pot, put the pot on the plate, and use the tool to push to pot to the front of the table.",✅,"1. decompose into atmoic tasks because the duration exceeds 600 frames
2. seems there are not so many difference in the 3rd-view trajectories","Take the lid off the pot, put the pot on the plate, and use the tool to push to pot to the front of the table.",,1,682,20,2,0,1,1,Franka,Single Arm,,"pot, plate, spoon, cup","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [6x end effector delta pose, 1x gripper position].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(128, 128, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(24,), dtype=float32, description=Robot state, consists of [7x robot joint angles, 1x gripper position, 16x robot end-effector homogeneous matrix].),
            'wrist_image': Image(shape=(128, 128, 3), dtype=uint8, description=Wrist camera RGB observation.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,,,,Human Spacemouse,,,,,,,
CMU Franka Exploration,,0,FALSE,FALSE,,,,,,,200,,,,"Templated
- lift the knife (38.0%)
- lift the vegetable (24.0%)
- open the cabinet (38.0%)",✅,"1. we may need to use 'highres_image' instead of the default 'image'
2. the tasks are short-horizon tasks w/o too many differences (10 frames)","- lift the knife (38.0%) 
- lift the vegetable (24.0%) 
- open the cabinet (38.0%)",,1,10,10,1,0,0,1,Franka,Single Arm,,"knife, vegetable, cabinet","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(8,), dtype=float32, description=Robot action, consists of [end effector position3x, end effector orientation3x, gripper action1x, episode termination1x].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'highres_image': Image(shape=(480, 640, 3), dtype=uint8, description=High resolution main camera observation),
            'image': Image(shape=(64, 64, 3), dtype=uint8, description=Main camera RGB observation.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
        'structured_action': Tensor(shape=(8,), dtype=float32, description=Structured action, consisting of hybrid affordance and end-effector control, described in Structured World Models from Human Videos.),
    }),
})",EEF Position,,,,Expert Policy,,,"1. The observation frequency may be lower than the control frquency
2. The reward is float number < 1. so we cannot justify the success of the expert policy",,,,
Austin Sirius,,0,TRUE,FALSE,,,,,,,600,,,,"None
- Insert the blue gear onto the right peg, followed by the red gear (54.0%)
- Open the kcup holder, insert the kcup into the holder, and close the kcup holder (46.0%)",✅,"1. Only 2 tasks without too many differences in the different trajectories
2. Very low resolution","- Insert the blue gear onto the right peg, followed by the red gear (54.0%) - Open the kcup holder, insert the kcup into the holder, and close the kcup holder (46.0%)",,1,524,20,2,0,1,1,Franka,Single Arm,,"peg, gear, holder, kcup","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [3x ee relative pos, 3x ee relative rotation, 1x gripper action].),
        'action_mode': Tensor(shape=(1,), dtype=float32, description=Type of interaction. -1: initial human demonstration. 1: intervention. 0: autonomuos robot execution (includes pre-intervention class)),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'intv_label': Tensor(shape=(1,), dtype=float32, description=Same as action_modes, except 15 timesteps preceding intervention are labeled as -10.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(84, 84, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(8,), dtype=float32, description=Default robot state, consists of [7x robot joint state, 1x gripper state].),
            'state_ee': Tensor(shape=(16,), dtype=float32, description=End-effector state, represented as 4x4 homogeneous transformation matrix of ee pose.),
            'state_gripper': Tensor(shape=(1,), dtype=float32, description=Robot gripper opening width. Ranges between ~0 (closed) to ~0.077 (open)),
            'state_joint': Tensor(shape=(7,), dtype=float32, description=Robot 7-dof joint information.),
            'wrist_image': Image(shape=(84, 84, 3), dtype=uint8, description=Wrist camera RGB observation.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF velocity,,,,Human Spacemouse,,,,,,,
USC Cloth Sim,,0,FALSE,FALSE,,,,,,,"1,000",,,,"None
- fold cloth along diagonal (100.0%)",✅,It is simulated trajectoires and only about folding cloth,- fold cloth along diagonal (100.0%),,2,100,10,1,0,0,1,Franka,Single Arm,,cloth,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(4,), dtype=float32, description=Robot action, consists of x,y,z goal and picker commandpicker<0.5 = open, picker>0.5 = close.),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(32, 32, 3), dtype=uint8, description=Image observation of cloth.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward as a normalized performance metric in [0, 1].0 = no change from initial state. 1 = perfect fold.-ve performance means the cloth is worse off than initial state.),
    }),
})",EEF Position,,,,Scripted,,,,,,,
Tokyo PR2 Fridge Opening,,0,FALSE,FALSE,,,,,,,64,,,,"None
- opening the fridge (100.0%)
",✅,Only about opening fridge and the trajectories are highly similar,- opening the fridge (100.0%),,1,144,10,1,0,0,1,PR2,Single Arm,,fridge,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(8,), dtype=float32, description=Robot action, consists of [3x end effector pos, 3x robot rpy angles, 1x gripper open/close command, 1x terminal action].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(128, 128, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(7,), dtype=float32, description=Robot state, consists of [3x end effector pos, 3x robot rpy angles, 1x gripper position].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,,,,Human VR,,,,,,,
Mobile ALOHA,,0,FALSE,FALSE,,,,,,,276,,,,"template , only 7 kinds

1.open the cabinet, put the pot inside, then close it
2.get the cloth and wipe up the spill under the wine glass
3.wash the pan
4.add oil to the pan and cook the shrimp and serve it
5.navigate to elevator, push the call button, then enter the elevator
6.push the chairs under the table
7.high five ",✅,✅(long task ignore),"1. Wipe Wine.  50
2. Cook Shrimp.  20 
3. Rinse Pan. 50 
4. Use Cabinet. 50  
5. Call Elevator. 50 
6. Push Chairs. 50 
7. High Five   20",,1,"1,890",50,3,0,2,1,MobileALOHA,"Dual-arm
human manipulate",,"specific obeject in the 7 tasks  
like oil, shrimp , cabinet , pan and etc.","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': string,
    }),
    'steps': Dataset({
        'action': Tensor(shape=(16,), dtype=float32),
        'discount': Scalar(shape=(), dtype=float32),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_instruction': string,
        'observation': FeaturesDict({
            'cam_high': Image(shape=(480, 640, 3), dtype=uint8),
            'cam_left_wrist': Image(shape=(480, 640, 3), dtype=uint8),
            'cam_right_wrist': Image(shape=(480, 640, 3), dtype=uint8),
            'state': Tensor(shape=(14,), dtype=float32),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})","joint position. 
total 16 = 2* 8（7 joint 1 gripper)",,CoRL 2024,,human puppeteering,Mobile ALOHA,,✅,,,,
UTokyo xArm PickPlace,,0,FALSE,FALSE,,,,,,,95,,,,"None
- Pick up a white plate, and then place it on the red plate (100.0%)",✅,No too many differences between trajectories,"- Pick up a white plate, and then place it on the red plate (100.0%)
",,1,,10,2,0,1,1,xArm,Single Arm,,plate,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [3x EEF position, 3x EEF orientation yaw/pitch/roll, 1x gripper open/close position].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'end_effector_pose': Tensor(shape=(6,), dtype=float32, description=Robot end effector pose, consists of [3x EEF position, 3x EEF orientation yaw/pitch/roll].),
            'hand_image': Image(shape=(224, 224, 3), dtype=uint8, description=Hand camera RGB observation.),
            'image': Image(shape=(224, 224, 3), dtype=uint8, description=Main camera RGB observation.),
            'image2': Image(shape=(224, 224, 3), dtype=uint8, description=Another camera RGB observation from different view point.),
            'joint_state': Tensor(shape=(14,), dtype=float32, description=Robot joint state, consists of [7x robot joint angles, 7x robot joint velocity].),
            'joint_trajectory': Tensor(shape=(21,), dtype=float32, description=Robot joint trajectory, consists of [7x robot joint angles, 7x robot joint velocity, 7x robot joint acceralation].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,,,,Human Puppeteering,,,,,,,
Robonet,,0,FALSE,,,,,,,,"82,775",,,,Interact with the objects in the bin,✅,（可以改出区别，但任务本身没什么质量）,"NM (not mentioned)
fetch, pull, push, move .etc
mainly for moving",,1,30,1,3,0,0,3,"Sawyer (68k), Baxter (18k), WidowX (5k),  Franka (7.9k),  Kuka (1.8k),  Fetch (5k),  GoogleRobot (56k)",Single Arm,,household objects,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
        'robot': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(5,), dtype=float32),
        'discount': Scalar(shape=(), dtype=float32),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(240, 320, 3), dtype=uint8),
            'image1': Image(shape=(240, 320, 3), dtype=uint8),
            'image2': Image(shape=(240, 320, 3), dtype=uint8),
            'state': Tensor(shape=(5,), dtype=float32),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})","EEF Position
delta pose 5(delta x, delta y, delta z, delta theta, gripper)",,CoRL 2019,,scripts,"Sawyer , Baxter , WidowX , Franka , Kuka , Fetch , GoogleRobot","Gripper: Weiss Robotics WSG-50, Robotiq,  WidowX, Baxter,  Franka, Kuka",bad dataset,,,,
Stanford MaskVIT Data,,0,FALSE,,,,,,,,"9,109",,,,"pick
push
reach
close the fridge door",✅,,"pick
push
reach
close the fridge door",,1,31,N/A,1,0,0,1,Sawyer,Single Arm,,household objects (like bowls and cups),"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(5,), dtype=float32, description=Robot action, consists of [3x change in end effector position, 1x gripper yaw, 1x open/close gripper (-1 means to open the gripper, 1 means close)].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'end_effector_pose': Tensor(shape=(5,), dtype=float32, description=Robot end effector pose, consists of [3x Cartesian position, 1x gripper yaw, 1x gripper position]. This is the state used in the MaskViT paper.),
            'finger_sensors': Tensor(shape=(1,), dtype=float32, description=1x Sawyer gripper finger sensors.),
            'high_bound': Tensor(shape=(5,), dtype=float32, description=High bound for end effector pose normalization. Consists of [3x Cartesian position, 1x gripper yaw, 1x gripper position].),
            'image': Image(shape=(480, 480, 3), dtype=uint8, description=Main camera RGB observation.),
            'low_bound': Tensor(shape=(5,), dtype=float32, description=Low bound for end effector pose normalization. Consists of [3x Cartesian position, 1x gripper yaw, 1x gripper position].),
            'state': Tensor(shape=(15,), dtype=float32, description=Robot state, consists of [7x robot joint angles, 7x robot joint velocities,1x gripper position].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})","EEF Position
delta pose 5(delta x, delta y, delta z, delta theta, gripper)",,,,scripts,sawyer,,,,,,
DLR Sara Grid Clamp Dataset,,0,FALSE,,,,,,,,100,,,,Place grid clamp,✅,modify action,Place grid clamp,,1,71,10,1,0,0,1,DLR SARA,Single Arm,,,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [3x robot EEF position, 3x robot EEF orientation yaw/pitch/roll calculated with scipy Rotation.as_euler(=""zxy"") Class].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(480, 640, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(12,), dtype=float32, description=Robot state, consists of [3x robot EEF position, 3x robot EEF orientation yaw/pitch/roll calculated with scipy Rotation.as_euler(""zxy"") Class, 6x robot EEF wrench].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})","EEF velocity
3 eef pose
3 euler angle
1 gripper action",,,,Expert Policy,,,,,,,
DLR Wheelchair Shared Control,,0,FALSE,,,,,,,,104,,,,"pick the mug
pick the glass
pick the yellow ball
pick the orange
pick the tennis ball
pick the red apple
pick the pear
pick the green apple
pick the banana",✅,modify action,pick,,2,86,5,1,0,0,1,DLR EDAN,Single Arm,,,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [3x robot EEF position, 3x robot EEF orientation yaw/pitch/roll calculated with scipy Rotation.as_euler(=""zxy"") Class].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(360, 640, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(7,), dtype=float32, description=Robot state, consists of [3x robot EEF position, 3x robot EEF orientation yaw/pitch/roll calculated with scipy Rotation.as_euler(=""zxy"") Class].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})","EEF Position
3 eef pose
3 eef euler zxy
1 gripper action",,,,Human teleoperating,,,（task很多，但结构上不是很相关-四爪）,,,,
Berkeley Cable Routing,,0,TRUE,FALSE,,,,,,,"1,482",,,,"None
- Only route a cable through a series of clips (pickup, route, perturb)",✅,It is too different from our dataset,"Only route a cable through a series of clips (pickup, route, perturb)",,1,30,10,3,0,2,1,Franka,Single Arm,,the cable & three clips,"FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'rotation_delta': Tensor(shape=(3,), dtype=float32, description=Angular velocity about the z axis.),
            'terminate_episode': float32,
            'world_vector': Tensor(shape=(3,), dtype=float32, description=Velocity in XYZ.),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'image': Image(shape=(128, 128, 3), dtype=uint8),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
            'robot_state': Tensor(shape=(7,), dtype=float32),
            'top_image': Image(shape=(128, 128, 3), dtype=uint8),
            'wrist225_image': Image(shape=(128, 128, 3), dtype=uint8),
            'wrist45_image': Image(shape=(128, 128, 3), dtype=uint8),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",EEF velocity,,T-RO 2024,,Human VR,,,,,,,
Columbia PushT Dataset,,0,FALSE,FALSE,,,,,,,122,,,,None,✅,The task is different from ours,The task requires pushing a T-shaped block (gray) to a fixed target (green) with a circular end-effector (blue).,,1,204,10,2,0,1,1,UR5,Single Arm,,a T-shaped block (gray),"FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'gripper_closedness_action': float32,
            'rotation_delta': Tensor(shape=(3,), dtype=float32, description=Delta change in roll, pitch, yaw.),
            'terminate_episode': float32,
            'world_vector': Tensor(shape=(3,), dtype=float32, description=Delta change in XYZ.),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'image': Image(shape=(240, 320, 3), dtype=uint8),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
            'robot_state': Tensor(shape=(2,), dtype=float32, description=Robot end effector XY state),
            'wrist_image': Image(shape=(240, 320, 3), dtype=uint8),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",EEF Position,,,,Human VR,,,,,,,
Stanford Kuka Multimodal,,0,FALSE,FALSE,,,,,,,"3,000",,,,None,✅,"1. The resolution is low and the task is very different from ours
2. The trajectory is not generated via teleoperation, but expert policy",Insert differently-shaped pegs into differently-shaped holes with low tolerances (~2mm).,,1,50,20,1,1,0,1,Kuka iiwa,Single Arm,,"peg, hole","FeaturesDict({
    'episode_metadata': FeaturesDict({
    }),
    'steps': Dataset({
        'action': Tensor(shape=(4,), dtype=float32, description=Robot action, consists of [3x EEF position, 1x gripper open/close].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'contact': Tensor(shape=(50,), dtype=float32, description=Robot contact information.),
            'depth_image': Tensor(shape=(128, 128, 1), dtype=float32, description=Main depth camera observation.),
            'ee_forces_continuous': Tensor(shape=(50, 6), dtype=float32, description=Robot end-effector forces.),
            'ee_orientation': Tensor(shape=(4,), dtype=float32, description=Robot end-effector orientation quaternion.),
            'ee_orientation_vel': Tensor(shape=(3,), dtype=float32, description=Robot end-effector orientation velocity.),
            'ee_position': Tensor(shape=(3,), dtype=float32, description=Robot end-effector position.),
            'ee_vel': Tensor(shape=(3,), dtype=float32, description=Robot end-effector velocity.),
            'ee_yaw': Tensor(shape=(4,), dtype=float32, description=Robot end-effector yaw.),
            'ee_yaw_delta': Tensor(shape=(4,), dtype=float32, description=Robot end-effector yaw delta.),
            'image': Image(shape=(128, 128, 3), dtype=uint8, description=Main camera RGB observation.),
            'joint_pos': Tensor(shape=(7,), dtype=float32, description=Robot joint positions.),
            'joint_vel': Tensor(shape=(7,), dtype=float32, description=Robot joint velocities.),
            'optical_flow': Tensor(shape=(128, 128, 2), dtype=float32, description=Optical flow.),
            'state': Tensor(shape=(8,), dtype=float32, description=Robot proprioceptive information, [7x joint pos, 1x gripper open/close].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,,,,Expert Policy,,,,,,,
Maniskill,,0,FALSE,FALSE,,,,,,,"30,000",,,,"Templated
- Pick up the object and move it to a goal position (56.0%)
- Plug the charger into the wall socket (6.0%)
- Pick up a designated object from a clutter of objects (20.0%)
- Turn on the faucet by rotating a designated handle (6.0%)
- Insert the peg into the horizontal hole in a box (6.0%)
- Insert a designated object into the corresponding slot on a board (6.0%)",✅,"1. This dataset is not real-world trajectories
2. The templated language instruction is not very clear, like ""object""","- Pick up the object and move it to a goal position (56.0%)
- Plug the charger into the wall socket (6.0%)
- Pick up a designated object from a clutter of objects (20.0%)
- Turn on the faucet by rotating a designated handle (6.0%)
- Insert the peg into the horizontal hole in a box (6.0%)
- Insert a designated object into the corresponding slot on a board (6.0%)",,1,150,20,2,2,2,0,Franka,Single Arm,,articulated objects like peg,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'episode_id': Text(shape=(), dtype=string),
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [3x end effector delta target position, 3x end effector delta target orientation in axis-angle format, 1x gripper target position (mimic for two fingers)]. For delta target position, an action of -1 maps to a robot movement of -0.1m, and action of 1 maps to a movement of 0.1m. For delta target orientation, its encoded angle is mapped to a range of [-0.1rad, 0.1rad] for robot execution. For example, an action of [1, 0, 0] means rotating along the x-axis by 0.1 rad. For gripper target position, an action of -1 means close, and an action of 1 means open.),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'base_pose': Tensor(shape=(7,), dtype=float32, description=Robot base pose in the world frame, consists of [x, y, z, qw, qx, qy, qz]. The first three dimensions represent xyz positions in meters. The last four dimensions are the quaternion representation of rotation.),
            'depth': Image(shape=(256, 256, 1), dtype=uint16, description=Main camera Depth observation. Divide the depth value by 2**10 to get the depth in meters.),
            'image': Image(shape=(256, 256, 3), dtype=uint8, description=Main camera RGB observation.),
            'main_camera_cam2world_gl': Tensor(shape=(4, 4), dtype=float32, description=Transformation from the main camera frame to the world frame in OpenGL/Blender convention.),
            'main_camera_extrinsic_cv': Tensor(shape=(4, 4), dtype=float32, description=Main camera extrinsic matrix in OpenCV convention.),
            'main_camera_intrinsic_cv': Tensor(shape=(3, 3), dtype=float32, description=Main camera intrinsic matrix in OpenCV convention.),
            'state': Tensor(shape=(18,), dtype=float32, description=Robot state, consists of [7x robot joint angles, 2x gripper position, 7x robot joint angle velocity, 2x gripper velocity]. Angle in radians, position in meters.),
            'target_object_or_part_final_pose': Tensor(shape=(7,), dtype=float32, description=The final pose towards which the target object or object part needs be manipulated, consists of [x, y, z, qw, qx, qy, qz]. The pose is represented in the world frame. An episode is considered successful if the target object or object part is manipulated to this pose.),
            'target_object_or_part_final_pose_valid': Tensor(shape=(7,), dtype=uint8, description=Whether each dimension of target_object_or_part_final_pose is valid in an environment. 1 = valid; 0 = invalid (in which case one should ignore the corresponding dimensions in target_object_or_part_final_pose). ""Invalid"" means that there is no success check on the final pose of target object or object part in the corresponding dimensions.),
            'target_object_or_part_initial_pose': Tensor(shape=(7,), dtype=float32, description=The initial pose of the target object or object part to be manipulated, consists of [x, y, z, qw, qx, qy, qz]. The pose is represented in the world frame. This variable is used to specify the target object or object part when multiple objects or object parts are present in an environment),
            'target_object_or_part_initial_pose_valid': Tensor(shape=(7,), dtype=uint8, description=Whether each dimension of target_object_or_part_initial_pose is valid in an environment. 1 = valid; 0 = invalid (in which case one should ignore the corresponding dimensions in target_object_or_part_initial_pose).),
            'tcp_pose': Tensor(shape=(7,), dtype=float32, description=Robot tool-center-point pose in the world frame, consists of [x, y, z, qw, qx, qy, qz]. Tool-center-point is the center between the two gripper fingers.),
            'wrist_camera_cam2world_gl': Tensor(shape=(4, 4), dtype=float32, description=Transformation from the wrist camera frame to the world frame in OpenGL/Blender convention.),
            'wrist_camera_extrinsic_cv': Tensor(shape=(4, 4), dtype=float32, description=Wrist camera extrinsic matrix in OpenCV convention.),
            'wrist_camera_intrinsic_cv': Tensor(shape=(3, 3), dtype=float32, description=Wrist camera intrinsic matrix in OpenCV convention.),
            'wrist_depth': Image(shape=(256, 256, 1), dtype=uint16, description=Wrist camera Depth observation. Divide the depth value by 2**10 to get the depth in meters.),
            'wrist_image': Image(shape=(256, 256, 3), dtype=uint8, description=Wrist camera RGB observation.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,,,,Scripted,,,,,,,
Saytap,,0,FALSE,FALSE,,,,,,,20,,,,"Natural
- trot forward fast (5.0%)
- pace in place (5.0%)
- stand still (5.0%)
- bound forward slowly (5.0%)
- move forward fast in pacing gait (5.0%)
- bound backward fast (5.0%)
- move backward slowly in pacing gait (5.0%)
...
- raise your front right leg (5.0%)
- trot backward fast (5.0%)
- bound backward slowly (5.0%)
- trot backward slowly (5.0%)",✅,"1. Only 20 trajectoires, and the resolution is very low
2. The image seems to be black??","- trot forward fast (5.0%)
- pace in place (5.0%)
- stand still (5.0%)
- bound forward slowly (5.0%)
- move forward fast in pacing gait (5.0%)
- bound backward fast (5.0%)
- move backward slowly in pacing gait (5.0%)
...
- raise your front right leg (5.0%)
- trot backward fast (5.0%)
- bound backward slowly (5.0%)
- trot backward slowly (5.0%)",,1,"1,147",50,0,0,0,0,Unitree A1,Quadrupedal Robot,,"leg, gait","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(12,), dtype=float32, description=Robot action, consists of [12x joint positios].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'desired_pattern': Tensor(shape=(4, 5), dtype=bool, description=Desired foot contact pattern for the 4 legs, the 4 rows are for the front right, front left, rear right and rear left legs, the pattern length is 5 (=0.1s).),
            'desired_vel': Tensor(shape=(3,), dtype=float32, description=Desired velocites. The first 2 are linear velocities along and perpendicular to the heading direction, the 3rd is the desired angular velocity about the yaw axis.),
            'image': Image(shape=(64, 64, 3), dtype=uint8, description=Dummy camera RGB observation.),
            'prev_act': Tensor(shape=(12,), dtype=float32, description=Actions applied in the previous step.),
            'proj_grav_vec': Tensor(shape=(3,), dtype=float32, description=The gravity vector [0, 0, -1] in the robot base frame.),
            'state': Tensor(shape=(30,), dtype=float32, description=Robot state, consists of [3x robot base linear velocity, 3x base angular vel, 12x joint position, 12x joint velocity].),
            'wrist_image': Image(shape=(64, 64, 3), dtype=uint8, description=Dummy wrist camera RGB observation.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",Joint position,,,,Expert Policy,,,,,,,
conqhose,,0,FALSE,,,,,,,,139,,,,"Templated
1.pick up the pillow and place it on the couch
2.grasp hose
3.vacuum the floor",✅,"1. The image needs to be rotated and the black part cut out
2. 2 error in OXE. sheet. , 
   a. Only 3 camera ，not 6 . 
   b. 1 wrist camera, not 0 
   I have emailed the person in charge. mitranopeter @gmail.com
3. Both tasks and scenes are very simple， repeat",grasp hose,,1,56,30,3,0,1,2,Spot,Single Arm,,only hose,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': string,
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32),
        'discount': float32,
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32),
        'language_instruction': string,
        'observation': FeaturesDict({
            'frontleft_fisheye_image': Image(shape=(726, 604, 3), dtype=uint8),
            'frontright_fisheye_image': Image(shape=(726, 604, 3), dtype=uint8),
            'hand_color_image': Image(shape=(480, 640, 3), dtype=uint8),
            'state': Tensor(shape=(66,), dtype=float32),
        }),
        'reward': float32,
    }),
})",EEF velocity,,NA,"
https://github.com/UM-ARM-Lab/conq_hose_manipulation_dataset_builder",scripts,spot,,bad dataset,,,,
TidyBot,,0,FALSE,,,,,,,,24,,,,"1.Put drink cans in the recycling bin, snacks in the plastic storage box, fruit in the black storage box, clothes on the sofa, and  wooden blocks in the drawer. 3
2.Put Rubik's cubes on the coffee table and other toys in the plastic storage box. 3
3.Put clothes in the white laundry basket, bags on the sofa, and snacks in the black storage box. 3
4.Put marker pens in the plastic storage box, clothes on the chair, plush toys on the sofa, and rubber gloves in the drawer.  3
5.Put paper napkins in the trash can, soda cans in the recycling bin, plastic bags in the black storage box, and utensils on the coffee table. 3
6.Put fruits on the left shelf, wooden blocks in the middle shelf, and construction tools in the right shelf. 3
7.Put hats in the drawer, socks in the white laundry basket, other clothes in the seagrass laundry basket, shoes in the black storage box, and plush toys on the sofa. 3 
8.Put dark-colored clothes in the white laundry basket and light-colored clothes in the seagrass laundry basket.3",✅,"1. bad datasets
2. have not vedio , only static image
3. action space is string can not be used","1.Put drink cans in the recycling bin, snacks in the plastic storage box, fruit in the black storage box, clothes on the sofa, and  wooden blocks in the drawer. 3
2.Put Rubik's cubes on the coffee table and other toys in the plastic storage box. 3
3.Put clothes in the white laundry basket, bags on the sofa, and snacks in the black storage box. 3
4.Put marker pens in the plastic storage box, clothes on the chair, plush toys on the sofa, and rubber gloves in the drawer.  3
5.Put paper napkins in the trash can, soda cans in the recycling bin, plastic bags in the black storage box, and utensils on the coffee table. 3
6.Put fruits on the left shelf, wooden blocks in the middle shelf, and construction tools in the right shelf. 3
7.Put hats in the drawer, socks in the white laundry basket, other clothes in the seagrass laundry basket, shoes in the black storage box, and plush toys on the sofa. 3 
8.Put dark-colored clothes in the white laundry basket and light-colored clothes in the seagrass laundry basket.3",,1,9,na,1,0,0,1,Tidybot,mobile Manipulator,,"bin,fruit,socks in the kichenscene","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': string,
    }),
    'steps': Dataset({
        'action': string,
        'discount': Scalar(shape=(), dtype=float32),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32),
        'language_instruction': string,
        'observation': FeaturesDict({
            'image': Image(shape=(360, 640, 3), dtype=uint8),
            'object': string,
            'receptacles': Sequence(string),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",string(quit special),,,,,,,,,,,
VIMA,,0,FALSE,,,,,,,,660103,,,,"Template
1.Put the yellow object in {scene} into the yellow and purple stripe object.
2.Rotate the {dragged_obj} 90 degrees.
3.Stack objects in this order {frame_0} {frame_1} {frame_2}.
4.This is a blicket {dragged_obj_1}. This is a wug {base_obj}. Put a blicket into a wug.
5.Sweep one {swept_obj} into {bounds} without exceeding {constraint}.
6.First put {dragged_obj} into {base_obj} then put the object that was previously at its east into the same {base_obj}.
7.""Twist"" is defined as rotating object a specific angle. For examples: From {before_twist_1} to {after_twist_1}. From {before_twist_2} to {after_twist_2}. From {before_twist_3} to {after_twist_3}. Now twist all polka dot objects.
8.Rotate the {dragged_obj} 60 degrees.
9.{demo_blicker_obj_1} is kobar than {demo_less_blicker_obj_1}. {demo_blicker_obj_2} is kobar than {demo_less_blicker_obj_2}. {demo_blicker_obj_3} is kobar than {demo_less_blicker_obj_3}. Put the less kobar {dragged_obj} into the {base_obj}.
10.Rearrange objects to this setup {scene} and then restore.
11.Rotate the {dragged_obj} 120 degrees.
12.Put the tiger object in {scene} into the blue swirl object.
13.{demo_blicker_obj_1} is kobar than {demo_less_blicker_obj_1}. {demo_blicker_obj_2} is kobar than {demo_less_blicker_obj_2}. {demo_blicker_obj_3} is kobar than {demo_less_blicker_obj_3}. Put the kobar {dragged_obj} into the {base_obj}.
14.This is a dax {base_obj}. This is a blicket {dragged_obj_1}. Put a blicket into a dax.
15.Put the red swirl object in {scene} into the yellow and purple stripe object.
16.""Twist"" is defined as rotating object a specific angle. For examples: From {before_twist_1} to {after_twist_1}. From {before_twist_2} to {after_twist_2}. From {before_twist_3} to {after_twist_3}. Now twist all green objects.
...
39.Rearrange to this {scene}.
40.This is a wug {base_obj}. This is a dax {dragged_obj_1}. Put a dax into a wug.
41.Put {dragged_obj} into {base_obj_1} then {base_obj_2}. Finally restore it into its original container.",✅,"1. there is ""Semantic Segmentation Mask"" infor
2.only the static first image 
3. bad dataset","1.Put the yellow object in {scene} into the yellow and purple stripe object.
2.Rotate the {dragged_obj} 90 degrees.
3.Stack objects in this order {frame_0} {frame_1} {frame_2}.
4.This is a blicket {dragged_obj_1}. This is a wug {base_obj}. Put a blicket into a wug.
5.Sweep one {swept_obj} into {bounds} without exceeding {constraint}.
6.First put {dragged_obj} into {base_obj} then put the object that was previously at its east into the same {base_obj}.
7.""Twist"" is defined as rotating object a specific angle. For examples: From {before_twist_1} to {after_twist_1}. From {before_twist_2} to {after_twist_2}. From {before_twist_3} to {after_twist_3}. Now twist all polka dot objects.
8.Rotate the {dragged_obj} 60 degrees.
9.{demo_blicker_obj_1} is kobar than {demo_less_blicker_obj_1}. {demo_blicker_obj_2} is kobar than {demo_less_blicker_obj_2}. {demo_blicker_obj_3} is kobar than {demo_less_blicker_obj_3}. Put the less kobar {dragged_obj} into the {base_obj}.
10.Rearrange objects to this setup {scene} and then restore.
11.Rotate the {dragged_obj} 120 degrees.
12.Put the tiger object in {scene} into the blue swirl object.
13.{demo_blicker_obj_1} is kobar than {demo_less_blicker_obj_1}. {demo_blicker_obj_2} is kobar than {demo_less_blicker_obj_2}. {demo_blicker_obj_3} is kobar than {demo_less_blicker_obj_3}. Put the kobar {dragged_obj} into the {base_obj}.
14.This is a dax {base_obj}. This is a blicket {dragged_obj_1}. Put a blicket into a dax.
15.Put the red swirl object in {scene} into the yellow and purple stripe object.
16.""Twist"" is defined as rotating object a specific angle. For examples: From {before_twist_1} to {after_twist_1}. From {before_twist_2} to {after_twist_2}. From {before_twist_3} to {after_twist_3}. Now twist all green objects.
...
39.Rearrange to this {scene}.
40.This is a wug {base_obj}. This is a dax {dragged_obj_1}. Put a dax into a wug.
41.Put {dragged_obj} into {base_obj_1} then {base_obj_2}. Finally restore it into its original container.",,1,2,na,2,0,0,2,UR5,single Arm,,object in simualte environment,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'action_bounds': FeaturesDict({
            'high': Tensor(shape=(3,), dtype=float32),
            'low': Tensor(shape=(3,), dtype=float32),
        }),
        'end-effector type': string,
        'failure': Scalar(shape=(), dtype=bool),
        'file_path': string,
        'n_objects': Scalar(shape=(), dtype=int64),
        'num_steps': Scalar(shape=(), dtype=int64),
        'robot_components_seg_ids': Sequence(Scalar(shape=(), dtype=int64)),
        'seed': Scalar(shape=(), dtype=int64),
        'success': Scalar(shape=(), dtype=bool),
        'task': string,
    }),
    'steps': Dataset({
        'action': FeaturesDict({
            'pose0_position': Tensor(shape=(3,), dtype=float32),
            'pose0_rotation': Tensor(shape=(4,), dtype=float32),
            'pose1_position': Tensor(shape=(3,), dtype=float32),
            'pose1_rotation': Tensor(shape=(4,), dtype=float32),
        }),
        'discount': Scalar(shape=(), dtype=float32),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'multimodal_instruction': string,
        'multimodal_instruction_assets': FeaturesDict({
            'asset_type': Sequence(string),
            'frontal_image': Sequence(Tensor(shape=(128, 256, 3), dtype=uint8)),
            'frontal_segmentation': Sequence(Tensor(shape=(128, 256), dtype=uint8)),
            'image': Sequence(Tensor(shape=(128, 256, 3), dtype=uint8)),
            'key_name': Sequence(string),
            'segmentation': Sequence(Tensor(shape=(128, 256), dtype=uint8)),
            'segmentation_obj_info': Sequence({
                'obj_name': Sequence(string),
                'segm_id': Sequence(Scalar(shape=(), dtype=int64)),
                'texture_name': Sequence(string),
            }),
        }),
        'observation': FeaturesDict({
            'ee': int64,
            'frontal_image': Tensor(shape=(128, 256, 3), dtype=uint8),
            'frontal_segmentation': Tensor(shape=(128, 256), dtype=uint8),
            'image': Tensor(shape=(128, 256, 3), dtype=uint8),
            'segmentation': Tensor(shape=(128, 256), dtype=uint8),
            'segmentation_obj_info': FeaturesDict({
                'obj_name': Sequence(string),
                'segm_id': Sequence(Scalar(shape=(), dtype=int64)),
                'texture_name': Sequence(string),
            }),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",EEF Position,,,,,,,,,,,
SPOC,,0,FALSE,,,,,,,,2333000,,,,"1.search for a toilet
2.take the bottle with a lot of writing printed on it has a pump top to dispense the contents
3.go to the smallest electronic equipment in the bedroom
4.go to a kitchen utensil that can best be used for making hot water for tea and grasp that kitchen utensil
5.search for a candlestick and clutch that candlestick
6.search for an alarm clock and pick up that alarm clock
7.go to an alarm clock on a bed and grab that alarm clock
8.search for a vessel on a chest of drawers and grab that vessel
9.go to a mug and grab that mug
10.find a wine bottle and grab that wine bottle
11.navigate to 2 pans in the kitchen
12.hold a bowl
13.navigate to 2 foods in the kitchen
14.locate a timepiece that can best be used for waking up in the morning and grasp that timepiece
15.search for a pan and a step ladder, in that order
16.find the orange basketball with black lines and no other markings and pick it up
...
98.find the calendar furthest from the sofa in the livingroom
99.locate a bag on a chest of drawers, take that bag, and place it on a bed
100.grab the saw with a small blade and a red handle",✅,"1. totally simulate data
2. simulate data guide real world ","1.search for a toilet
2.take the bottle with a lot of writing printed on it has a pump top to dispense the contents
3.go to the smallest electronic equipment in the bedroom
4.go to a kitchen utensil that can best be used for making hot water for tea and grasp that kitchen utensil
5.search for a candlestick and clutch that candlestick
6.search for an alarm clock and pick up that alarm clock
7.go to an alarm clock on a bed and grab that alarm clock
8.search for a vessel on a chest of drawers and grab that vessel
9.go to a mug and grab that mug
10.find a wine bottle and grab that wine bottle
11.navigate to 2 pans in the kitchen
12.hold a bowl
13.navigate to 2 foods in the kitchen
14.locate a timepiece that can best be used for waking up in the morning and grasp that timepiece
15.search for a pan and a step ladder, in that order
16.find the orange basketball with black lines and no other markings and pick it up
...
98.find the calendar furthest from the sofa in the livingroom
99.locate a bag on a chest of drawers, take that bag, and place it on a bed
100.grab the saw with a small blade and a red handle",,1,76,10,2,0,0,2,Hello Stretch,single Arm,,object in simualte home environment,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': string,
        'task_target_split': string,
        'task_type': string,
    }),
    'steps': Dataset({
        'action': Tensor(shape=(9,), dtype=float32),
        'discount': Scalar(shape=(), dtype=float32),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_instruction': string,
        'observation': FeaturesDict({
            'an_object_is_in_hand': Scalar(shape=(), dtype=bool),
            'house_index': Scalar(shape=(), dtype=int64),
            'hypothetical_task_success': Scalar(shape=(), dtype=bool),
            'image': Image(shape=(224, 384, 3), dtype=uint8),
            'image_manipulation': Image(shape=(224, 384, 3), dtype=uint8),
            'last_action_is_random': Scalar(shape=(), dtype=bool),
            'last_action_str': string,
            'last_action_success': Scalar(shape=(), dtype=bool),
            'last_agent_location': Tensor(shape=(6,), dtype=float32),
            'manip_object_bbox': Tensor(shape=(10,), dtype=float32),
            'minimum_l2_target_distance': Scalar(shape=(), dtype=float32),
            'minimum_visible_target_alignment': Scalar(shape=(), dtype=float32),
            'nav_object_bbox': Tensor(shape=(10,), dtype=float32),
            'relative_arm_location_metadata': Tensor(shape=(4,), dtype=float32),
            'room_current_seen': Scalar(shape=(), dtype=bool),
            'rooms_seen': Scalar(shape=(), dtype=int64),
            'visible_target_4m_count': Scalar(shape=(), dtype=int64),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",joint position ：but 9 d,,,,,,,Sim2Real paper!,,,,
PlexRoboSuite,,0,FALSE,,,,,,,,402,,,,"1.Place the loaf of bread into the appropriate compartment.  67
2.Place the milk carton into the appropriate compartment.   67
3.Open the door by pressing down and pulling on the handle.  67
4.Place the red cube on top of the green cube. 67
5.Place the box of cereal into the appropriate compartment. 67
6.Put the loop with the handle onto the peg. 67",✅,1. good simulate data,"1.Place the loaf of bread into the appropriate compartment.  67
2.Place the milk carton into the appropriate compartment.   67
3.Open the door by pressing down and pulling on the handle.  67
4.Place the red cube on top of the green cube. 67
5.Place the box of cereal into the appropriate compartment. 67
6.Put the loop with the handle onto the peg. 67",,5,191,20,2,0,1,1,Franka Panda,single Arm,,"bread,milk carton,door,handel ,red,cube and so on","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'episode_id': string,
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float64),
        'discount': Scalar(shape=(), dtype=float32),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32),
        'language_instruction': string,
        'observation': FeaturesDict({
            'image': Image(shape=(128, 128, 3), dtype=uint8),
            'state': Tensor(shape=(32,), dtype=float64),
            'wrist_image': Image(shape=(128, 128, 3), dtype=uint8),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",EEF Position,,,,,,,,,,,
RECON,,0,FALSE,,,,,,,,11830,,,,"only 1 :
Navigate to the goal.",✅,navigation,"only 1 :
Navigate to the goal.",,1,51,3,1,0,1,0,Jackal,Wheeled Robot,,navigation task without objects,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(2,), dtype=float64, description=Robot action, consists of 2x position),
        'action_angle': Tensor(shape=(3,), dtype=float64, description=Robot action, consists of 2x position, 1x yaw),
        'discount': Scalar(shape=(), dtype=float64, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(120, 160, 3), dtype=uint8, description=Main camera RGB observation.),
            'position': Tensor(shape=(2,), dtype=float64, description=Robot position),
            'state': Tensor(shape=(3,), dtype=float64, description=Robot state, consists of [2x position, 1x yaw]),
            'yaw': Tensor(shape=(1,), dtype=float64, description=Robot yaw),
        }),
        'reward': Scalar(shape=(), dtype=float64, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF velocity,,CORL2021,,,,,,,,,
CoryHall,,0,FALSE,,,,,,,,7331,,,,"only 1 :
Navigate to the goal.",✅,navigation,"only 1 :
Navigate to the goal.",,1,20,5,1,0,1,0,RC Car,Wheeled Robot,,navigation task without objects,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(2,), dtype=float64, description=Robot action, consists of 2x position),
        'action_angle': Tensor(shape=(3,), dtype=float64, description=Robot action, consists of 2x position, 1x yaw),
        'discount': Scalar(shape=(), dtype=float64, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(64, 85, 3), dtype=uint8, description=Main camera RGB observation.),
            'position': Tensor(shape=(2,), dtype=float64, description=Robot position),
            'state': Tensor(shape=(3,), dtype=float64, description=Robot state, consists of [2x position, 1x yaw]),
            'yaw': Tensor(shape=(1,), dtype=float64, description=Robot yaw),
        }),
        'reward': Scalar(shape=(), dtype=float64, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF velocity,,,,,,,,,,,
SACSoN,,0,FALSE,,,,,,,,2995,,,,"only 1 :
Navigate to the goal.",✅,navigation,"only 1 :
Navigate to the goal.",,1,85,10,1,0,1,0,TurtleBot 2,Wheeled Robot,,navigation task without objects,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(2,), dtype=float64, description=Robot action, consists of 2x position),
        'action_angle': Tensor(shape=(3,), dtype=float64, description=Robot action, consists of 2x position, 1x yaw),
        'discount': Scalar(shape=(), dtype=float64, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(120, 160, 3), dtype=uint8, description=Main camera RGB observation.),
            'position': Tensor(shape=(2,), dtype=float64, description=Robot position),
            'state': Tensor(shape=(3,), dtype=float64, description=Robot state, consists of [2x position, 1x yaw]),
            'yaw': Tensor(shape=(1,), dtype=float64, description=Robot yaw),
        }),
        'reward': Scalar(shape=(), dtype=float64, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,,,,,,,,,,,
RoboVQA,,0,FALSE,,,,,,,,3331523,,,,dummy,✅,"image information is wrong , need to process in raw data",dummy instruction,,1,TODO,10,1,0,0,1,Google Robot,"3 embodiments: single-armed robot, single-armed human, single-armed human using grasping tools",,NA,"FeaturesDict({
    'timestamp_start': Tensor(shape=(), dtype=float32, description=""Start timestamp of the episode""),
    'timestamp_end': Tensor(shape=(), dtype=float32, description=""End timestamp of the episode""),
    'task_type': Tensor(shape=(), dtype=string, description=""Please see task type in https://g3doc.corp.google.com/experimental/robotics/grounded_dialog/data/dialog/synthetic_dialog/vqa/README.md?cl=head""),
    'timestamps': Sequence(
        Tensor(shape=(), dtype=float32, description=""Timestamps of the images""),
        length=-1
    ),
    'steps': Dataset({
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'raw_text_question': Tensor(shape=(), dtype=string, description=""Robot task question""),
            'raw_text_answer': Tensor(shape=(), dtype=string, description=""Robot task answer""),
            'images': Sequence(
                Image(shape=(288, 288, 3), dtype=uint8, encodingFormat=""jpeg""),
                length=-1
            ),
        }),
    }),
})",EEF Position,,,,,,,,,,,
Stanford Robocook,,0,FALSE,,,,,,,,"2,460",,,,"
pinch the dough with an asymmetric gripper
press the dough with a circle punch
pinch the dough with a two-rod symmetric gripper
press the dough with a square press
roll the dough with a small rolling pin
press the dough with a square punch
pinch the dough with a two-plane symmetric gripper
press the dough with a circle press
roll the dough with a large rolling pin",✅,modify action,"pinch with gripper
press with punch
press with press
roll with rolling pin",,2,45,5,4,4,0,4,Franka,Single Arm,,"kitchen tools
dough","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'extrinsics_1': Tensor(shape=(4, 4), dtype=float32, description=Camera 1 Extrinsic Matrix.),
        'extrinsics_2': Tensor(shape=(4, 4), dtype=float32, description=Camera 2 Extrinsic Matrix.),
        'extrinsics_3': Tensor(shape=(4, 4), dtype=float32, description=Camera 3 Extrinsic Matrix.),
        'extrinsics_4': Tensor(shape=(4, 4), dtype=float32, description=Camera 4 Extrinsic Matrix.),
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [3x robot end-effector velocities, 3x robot end-effector angular velocities, 1x gripper velocity].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'depth_1': Tensor(shape=(256, 256), dtype=float32, description=Camera 1 Depth observation.),
            'depth_2': Tensor(shape=(256, 256), dtype=float32, description=Camera 2 Depth observation.),
            'depth_3': Tensor(shape=(256, 256), dtype=float32, description=Camera 3 Depth observation.),
            'depth_4': Tensor(shape=(256, 256), dtype=float32, description=Camera 4 Depth observation.),
            'image_1': Image(shape=(256, 256, 3), dtype=uint8, description=Camera 1 RGB observation.),
            'image_2': Image(shape=(256, 256, 3), dtype=uint8, description=Camera 2 RGB observation.),
            'image_3': Image(shape=(256, 256, 3), dtype=uint8, description=Camera 3 RGB observation.),
            'image_4': Image(shape=(256, 256, 3), dtype=uint8, description=Camera 4 RGB observation.),
            'state': Tensor(shape=(7,), dtype=float32, description=Robot state, consists of [3x robot end-effector position, 3x robot end-effector euler angles, 1x gripper position].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})","EEF Position
3 eef velocity
3 eef angular velocity
1 gripper velocity",,CoRL 2023,,scripts,,,,,,,
QUT Dynamic Grasping,,0,FALSE,,,,,,,,812,,,,,✅,,,,,,30,3,0,2,1,franka,Single Arm,,,NA,EEF Position,,,,scripts,Franka,,cannot get it,,,,
ETH Agent Affordances,,0,FALSE,,,,,,,,118,,,,"close the oven
open the oven",✅,modify action,"close the oven
open the oven",,1,"1,265",66.6,1,1,0,1,Franka,Mobile manipulation,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
        'input_point_cloud': Tensor(shape=(10000, 3), dtype=float16, description=Point cloud (geometry only) of the object at the beginning of the episode (world frame) as a numpy array (10000,3).),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(6,), dtype=float32, description=Robot action, consists of [end-effector velocity (v_x,v_y,v_z,omega_x,omega_y,omega_z) in world frame),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(64, 64, 3), dtype=uint8, description=Main camera RGB observation. Not available for this dataset, will be set to np.zeros.),
            'input_point_cloud': Tensor(shape=(10000, 3), dtype=float16, description=Point cloud (geometry only) of the object at the beginning of the episode (world frame) as a numpy array (10000,3).),
            'state': Tensor(shape=(8,), dtype=float32, description=State, consists of [end-effector pose (x,y,z,yaw,pitch,roll) in world frame, 1x gripper open/close, 1x door opening angle].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",,,"EEF velocity
v_x, v_y, v_z
omega_x, omega_y, omega_z",,,,Expert Policy,,,"input point-cloud
depth image（场景比较diverse，但action没有很diverse但质量较好）",,,,
QUT Dexterous Manpulation,,0,FALSE,,,,,,,,,,,,,✅,,,,,,30,2,0,1,,Franka,Mobile manipulation,,,NA,EEF Position,,,,,,,cannot get it,,,,
MPI Muscular Proprioception,,0,FALSE,,,,,,,,,,,,,,,,,,,500,0,0,0,,PAMY2,Single Arm,,,NA,Desired pressures for the artificial muscles,,,,,,,cannot get it,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AgiBot World,,,FALSE,,,,,,,,"1,003,672 (#slices)",,,,Natural,❌,"1. There are some dirty tasks. 
",Detailed task list,"1. AgiBot World Beta: complete dataset featuring 1,003,672 trajectories (~43.8T)
AgiBot World Alpha: curated subset of Beta, containing 92,214 trajectories (~8.5T)

2. Official Visualizations

3. Faliure recovery: Retain them and manually annotate each with corresponding failure reasons and timestamps",106,"1,000",30,4,1,2,2,AgiBot-G1,Humanoid,"(Gelsight video)
Some of them","1. Daily necessities (e.g. wardrobe, dishwasher, fridge, toaster, washing machine), food, clothing, etc.

2. Restaurant - Food, Tableware, Kitchenware, etc.

3. Daily necessities (e.g., box, trash bag, toothpick, smart charger, rice cakes), Food, etc.

4. Daily necessities (trash, tissues, bottled water, microwave), Office supplies (whiteboard, eraser, remote controller), etc.

5. Snacks, Personal care products, etc.",Detailed Explanation ,,,IROS 2025,"Official processing codes are not released

Agibot to Lerobot",,"AgiBot G1

Platform: dual 7-DoF arms, a mobile chassis, and an adjustable waist
End-effectors: a standard gripper (visuo-tactile sensors) or a 6-DoF dexterous hand
Cameras: 1 RGB-D camera and 3 fisheye cameras for the front view, RGB-D or fisheye cameras mounted on each end-effector, and 2 fisheye cameras positioned at the rear.",,,,,,
RH20T,RH20T,3,FALSE,,,,,,,,"110, 000",,,,eg: pull out a napkin / pick up a block on the left and move it to the right,❌,,see paper appendix,,50+,,10,9-12,1,1-2,8-10,Flexiv/UR5/Franka/Kuka,Stationary Single-Arm Manipulator,,,"{
  ""RH20T"": {
    ""RH20T_cfg1"": {
      ""calib"": {
        ""gripper_pose.npy"": ""Gripper Cartesian pose during calibration"",
        ""intrinsics_[cam_id].npy"": ""Camera intrinsic matrix"",
        ""extrinsics_[cam_id].npy"": ""Camera extrinsic matrix (Aruco marker pose in camera frame)""
      },
      ""task_xxxx_user_xxxx_scene_xxxx_cfg_xxxx"": {
        ""metadata.json"": ""Robot manipulation metadata (scene timestamp, task completion rating, calibration quality)"",
        ""cam_[serial_number]"": {
          ""color.mp4"": ""RGB video stream"" (1280x720x3),
          ""depth.mp4 (optional)"": ""Depth video stream""(1280x720),
          ""timestamps.npy"": ""Timestamps for each frame""
        },
        ""transformed"": {
          ""tcp.npy"": ""End-effector Cartesian pose in camera frame (7D: xyz+quat)"",
          ""tcp_base.npy"": ""End-effector Cartesian pose in robot base frame (7D: xyz+quat)"",
          ""joint.npy"": ""Robot joint angles"",
          ""gripper.npy"": ""Gripper command and info (0–110 mm opening width)"",
          ""force_torque.npy"": ""6-DoF force/torque in camera frame (zeroed and raw values)"",
          ""force_torque_base.npy"": ""6-DoF force/torque in base frame (zeroed and raw values)"",
          ""high_freq_data.npy"": ""High frequency sensor data (timestamp, tcp, force/torque)""
        },
        ""audio_mixed"": {}
      },
      ""task_xxxx_user_xxxx_scene_xxxx_cfg_xxxx_human"": {
        ""metadata.json"": ""Human demonstration metadata (scene start/finish timestamps, calibration quality)"",
        ""cam_[serial_number]"": {
          ""color.mp4"": ""RGB video stream"",
          ""depth.mp4 (optional)"": ""Depth video stream"",
          ""timestamps.npy"": ""Timestamps for each frame""
        },
        ""audio_mixed"": {}
      }
    },
    ""RH20T_cfg2"": { ""..."": ""same structure as RH20T_cfg1"" },
    ""RH20T_cfg3"": { ""..."": ""same structure as RH20T_cfg1"" },
    ""..."": {},
    ""RH20T_cfg7"": { ""..."": ""same structure as RH20T_cfg1"" }
  }
}",EEF Position,,ICRA2024,,,,,,,,,
DexMimicGen,,,FALSE,,,,,,,,7500,,,,,❌,,,,,,,,,,,,,,,,,,,,,,,,,,,
GraspNet,,3,,,,,,,,,190,,,,,✅,,"Object Grasping, 6D Pose Estimation",,190,512,unspecified,2,2(Intel RealSense 435 and Kinect Azure),1,1,Flexiv Rizon arm,Single arm (serial robot arm),,,,EEF Position(相对位移+方向),,,,,,,,Real,3,"1280*720, very distinct","144.4G+(rough calculation)
"
unstated,,3,,,,,,,,,around 1200,,,,,❌,,"1个为主, occluded grasping,有变式（不同物体重量，位置，摩擦etc.）",,unspecified,unspecified,unspecified,1,1,0,1,Franka Emika Panda arm with 2-finger parallel jaw gripper.,Single arm (stationary 7-DOF arm for table-top manipulation).,,,,EEF Position/pose(相对位移),,,,,,,,Not Real,2,very distinct,unstated
RoboTAP,,3,,,,,,,,,265,,,,,❌,,"~10, including stencil insertion, shape-matching, stacking etc..",,265,271.9,10,1,0,1-2,0,Franka Panda Emika with 2f-85 Robotiq gripper and FT 300 Force Torque Sensor,Single arm,,,,EEF Position,,,,,,,,Real,2,"DeepMind robot video, quite distinct","zip file，265short videos+annotations，so around several G
"
OSU PRMTF,,3,,,,,,,,,1020,,,,,❌,,"4 main objects（rectangular prism, triangular prism, cylinder, cone），变式包括top/side抓取类型和姿势偏差",,1020,unspecified,unspecified,3,1,1,2,Kinova Gen3 robot arm with Robotiq 2F-85 Adaptive Gripper,Single arm,,,,EEF Position + Velocity,,,,,,,,Real,3,"Standard robot cameras, quite distinct","1,020experimental videos and status data, approximately several G"
RAGNet,,3,,,,,,,,,"mainly static images, e.g. RLBench sim has 25 episodes/task ",,,,,❌,,180 affordance categories,,unspecified,Not applicable (primarily static images),unspecified,2,unspecified,1,1,Not specified (dataset covers multiple robot domains without detailing specific hardware),Single arm,,,,EEF Position,,,,,,,,"Mixed (mainly real, few virtual (RLBench))",3,unstated but quite distinct,273k images + annotations，around 10G?
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,Score,Real or Not?,,Granularity,# of Images,# of Angle of View,# of Cameras, Fixed/Moving + 1/3 person perspective,Pixel Quality,# of Dimensions of Actions,# of Trajectory,Position or Velocity?,# of Tasks,GB,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,3,Real,,3(提供grasp poses的密集标注，包括6DoF姿势、置信分数和摩擦系数etc.，真的很细了),"97,280","190 scenes, each 256 viewpoints",2 popular RGBD cameras (Kinect Azure and RealSense D435),摄像头固定在机器人臂上沿固定轨迹移动; 第三人称,"1280*720, very distinct",6维（3维平移 + 3维旋转）+1维gripper宽度（总7维）,190,end effector position(相对位移+方向),"2(Object Grasping, 6D Pose Estimation)","144.4G+(rough calculation)
",,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,3,Not Real,,2,unstated,1(overhead/top-down),1 (Kinect v2),"fixed; 3
",very distinct,"高级代理-离散（像素坐标+3个primitives+16个旋转）；低级代理-3维（d, z, θ_y，表示位移和旋转adjustment）",约1200 episodes,end effector position/pose(相对位移),"1个为主, occluded grasping,有变式（不同物体重量，位置，摩擦etc.）",unstated,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,3,Real,,2,265,1-2,1(fixed external or wrist-mounted-moving with arm）,Fixed on the camera; 3,"DeepMind机器人视频, 还是quite distinct"," 4维（x, y, z, rz，denoting end-effector的translation和绕z轴rotate）",265,end effector position,"~10+，包括stencil insertion, shape-matching, stacking etc..","zip文件，265个短视频+标注，so around个位数G
",,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,3,Real,,3,"总1,020个试验，每个~1分钟视频","3个视角/摄像头：顶部RGB(fixed, third-person overhead), 侧面RGB(fixed, third-person side), 腕上RGBD(moving with arm, first-person egocentric)",3,stated in # of Angle of View,"Standard robot cameras, quite distinct",end executer姿势6维(3平移 + 3旋转)+gripper1维（共7维）,1020,位置和速度,"4 main objects（rectangular prism, triangular prism, cylinder, cone），变式包括top/side抓取类型和姿势偏差","1,020个试验的视频和状态数据, approximately several G",,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,3,"Mixed (mainly real, few virtual (RLBench))",,3,273k,5~6视角多样,2(RGB-D如Intel RealSense),Depending on the source datasets,unstated but quite distinct,"grasping pose为3D pose（3维位置x,y,z + 3维orientation，共6维）","mainly static images, RLBench sim has 25 episodes/task tho（e.g., open drawer）","position, no velocity",180 affordance categories,273k images + annotations，around 10G?,,,,,,,,,,,,,,,,,,,,,,,,,,,,