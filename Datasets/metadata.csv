Datasets,Score,In OpenVLA?,FInish Modify,How many different task  after modify,Tasks / Categories,#UniqueTasks,Comments,Scenes,#Scenes,#Trajectories,Avg. frames/trajectory,Control frequency,In OXE?,NickName,Language instructions,# Total Cams,# Depth Cams,# First-person Cams,# Third-person Cams,Robot type,Robot Morphology,Tactile sensor?,Objects,Full data structure,Action space,How to modify it to align with our overall objectives?,whether annotation,Venue,How to change the raw data to RDLS format data,How is the dataset collected?,Robot ,,Comments
AgiBot World,,FALSE,,,Detailed task list,"217

(96 atmoic actions)","1. AgiBot World Beta: complete dataset featuring 1,003,672 trajectories (~43.8T)
AgiBot World Alpha: curated subset of Beta, containing 92,214 trajectories (~8.5T)

2. Official Visualizations

3. Faliure recovery: Retain them and manually annotate each with corresponding failure reasons and timestamps","1. Home (51%)

2. Restaurant (17%)

3. Industry (8%)

4. Office (14%)

5. Supermarket (10%)",106,"1,003,672 (#slices)","1,000",30,❌,NA,Natural,4,1,2,2,AgiBot-G1,Humanoid,"(Gelsight video)
Some of them","1. Daily necessities (e.g. wardrobe, dishwasher, fridge, toaster, washing machine), food, clothing, etc.

2. Restaurant - Food, Tableware, Kitchenware, etc.

3. Daily necessities (e.g., box, trash bag, toothpick, smart charger, rice cakes), Food, etc.

4. Daily necessities (trash, tissues, bottled water, microwave), Office supplies (whiteboard, eraser, remote controller), etc.

5. Snacks, Personal care products, etc.",Detailed Explanation ,,"1. There are some dirty tasks. 
",,IROS 2025,"Official processing codes are not released

Agibot to Lerobot",,"AgiBot G1

Platform: dual 7-DoF arms, a mobile chassis, and an adjustable waist
End-effectors: a standard gripper (visuo-tactile sensors) or a 6-DoF dexterous hand
Cameras: 1 RGB-D camera and 3 fisheye cameras for the front view, RGB-D or fisheye cameras mounted on each end-effector, and 2 fisheye cameras positioned at the rear.",,
DROID,5,TRUE,TRUE,,"1.put

2. move 

3. pick

4. take 

5. open 

6. close 

7. place 

8. turn 

9. remove

10. other",217,"76k demonstration trajectories  
350 hours of interaction data
564 scenes
84 tasks
Visualization","1. industrial office 

2. kichen

3. office

4. living room

5. dinning room

6. bashroom

7. badroom

8. hallway

9. laundry room",564,92233,300,15,✅,droid,Natural,3,0,2,1,Franka Panda,Single Arm,,"1. Food

2. personal care

3. kichen tools

4. sports

5. accessories

6. hardware

7. clothes

8. appliances

9. utensils

10. furniture

11. textile

12. containers

13. stationary ","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'episode_id': Scalar(shape=(), dtype=int32),
        'file_path': string,
        'recording_folderpath': string,
        'has_exterior_image_1_left': Scalar(shape=(), dtype=bool),
        'has_exterior_image_2_left': Scalar(shape=(), dtype=bool),
        'has_wrist_image_left': Scalar(shape=(), dtype=bool),
        'has_language': Scalar(shape=(), dtype=bool),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32),
        'action_dict': FeaturesDict({
            'cartesian_position': Tensor(shape=(6,), dtype=float32),
            'cartesian_velocity': Tensor(shape=(6,), dtype=float32),
            'gripper_position': Tensor(shape=(1,), dtype=float32),
            'gripper_velocity': Tensor(shape=(1,), dtype=float32),
            'joint_position': Tensor(shape=(7,), dtype=float32),
            'joint_velocity': Tensor(shape=(7,), dtype=float32),
        }),
        'discount': Scalar(shape=(), dtype=float32),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32),
        'language_instruction': string,
        'language_instruction_2': string,
        'language_instruction_3': string,
        'observation': FeaturesDict({
            'exterior_image_1_left': Image(shape=(180, 320, 3), dtype=uint8),
            'exterior_image_2_left': Image(shape=(180, 320, 3), dtype=uint8),
            'wrist_image_left': Image(shape=(180, 320, 3), dtype=uint8),
            'cartesian_position': Tensor(shape=(6,), dtype=float32),
            'joint_position': Tensor(shape=(7,), dtype=float32),
            'gripper_position': Tensor(shape=(1,), dtype=float32),
            'state': Tensor(shape=(14,), dtype=float32),  
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",EEF position,"1. contain the detail velocity imforamtion,but it is redundant    ---- >. delete 
2. need to change the  image (180,320, 3 ) to. ( 224,224,3)
3. only one  third-person image is enough , chose the left or the right one is enough 
4. there are 3 instruction. for example : language_instruction :""Put the marker in the pot"" ,  ""language_instruction_2 "":""Get the marker from the table and put it inside the silver pot"" ;  "" language_instruction_3"" : ""Put the marker inside the silver pot"";  
5.  some task have not the second and third language instruction , so we only get the first instrucition ,
6. wash the data without instruction  ",,RSS 2024,"Github link about how to process rawdata

https://github.com/kpertsch/droid_dataset_builder","1. hardware setup ($20,000)
2. softwate setup
3. how to load and visualize samples


https://droid-dataset.github.io/droid/the-droid-dataset.html",,"Robot Platform: 
1. Franka Panda 7DoF robot arm
2. two adjustable Zed 2 stereo cameras
3. one  wristmounted Zed Mini stereo camera,
3. one  Oculus Quest 2 headset 

        
",
BridgeData,5,TRUE,FALSE,,"1. Pick-and-place (includes reorienting objects in place)
2. Pushing objects
3. Wiping (e.g., wiping the table with a cloth)
4. Sweeping (e.g., sweeping beans into a pile)
5. Stacking
6. Folding cloths
7. Opening and closing drawers
8. Only grasp different objects
9. Twisting knobs
10. Flipping switches
11. Zipping and unzipping
12. Opening and closing doors
13. Opening and closing cardboard box flaps",13 categoires,"Important: For the BridgeData V2 component, the version in OXE is out of date (as of 12/20/2023). Instead, you should download the dataset from the official website and place it under the subdirectory bridge_orig/. Replace any reference to bridge in the OXE code with bridge_orig.","1. Toy Kitchens (65.7%)
2. Tabletops (24.4%)
3. Toy sinks (6.2%)
4. Others (4.0%)",24 (4 types),"60,096 (50,365 human / 9,731 rollouts)",41,5,✅,bridge,"Natural
1. Pick-and-place (includes reorienting objects in place)
2. Pushing objects
3. Wiping (e.g., wiping the table with a cloth)
4. Sweeping (e.g., sweeping beans into a pile)
5. Stacking
6. Folding cloths
7. Opening and closing drawers
8. Only grasp different objects
9. Twisting knobs
10. Flipping switches
11. Zipping and unzipping
12. Opening and closing doors
13. Opening and closing cardboard box flaps",4,1,1,3,WidowX,Single Arm,❌,"can, pot, towel, table, cloth, spoon, knife, board, kadai, banana, napkin, eggplant, wedge, rag, pepper, sink, strawberry, vessel, colander, potato, sushi, plate, fork, cucumber, srewdriver, bowl, mushroom, peeler, lid, cardboardfence, fence, pear, egg, microwave, spatula, cheese, broccoli","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'episode_id': Scalar(shape=(), dtype=int32),
        'file_path': string,
        'has_image_0': Scalar(shape=(), dtype=bool),
        'has_image_1': Scalar(shape=(), dtype=bool),
        'has_image_2': Scalar(shape=(), dtype=bool),
        'has_image_3': Scalar(shape=(), dtype=bool),
        'has_language': Scalar(shape=(), dtype=bool),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32),
        'discount': Scalar(shape=(), dtype=float32),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32),
        'language_instruction': string,
        'observation': FeaturesDict({
            'image_0': Image(shape=(256, 256, 3), dtype=uint8),
            'image_1': Image(shape=(256, 256, 3), dtype=uint8),
            'image_2': Image(shape=(256, 256, 3), dtype=uint8),
            'image_3': Image(shape=(256, 256, 3), dtype=uint8),
            'state': Tensor(shape=(7,), dtype=float32),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",EEF Position,"Seems to be good
- very diverse, with diverse instructions & objects",,CoRL 2023,"1. Raw to Numpy to TFrecords processing code

2. Downloading RLDS from OXE & OpenVLA preprocessing code (resize_and_jpeg_encode)","1. Robot setup costs ~$4, 000
2. VR teleoperation
3. To support the evaluation of multi-task learning methods, collect data for many possible tasks simultaneously in each environment.
",,"Cameras: (640x480, 5Hz) 
1 RGBD camera fixed in an over-the-shoulder view
2 RGB cameras with poses that are randomized during data collection 
1 RGB camera attached to the robot’s wrist.",
QT-Opt,4,TRUE,FALSE,,Only grasp different objects,1,"Maybe need to filter success-only data, since the trajectory are collected automatically via Neural Networks",Tabletop,1,"580,392",14,10,✅,kuka,None,1,0,0,1,LBRIIWA arm,Single Arm,❌,NA,"FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'base_displacement_vector': Tensor(shape=(2,), dtype=float32),
            'base_displacement_vertical_rotation': Tensor(shape=(1,), dtype=float32),
            'gripper_closedness_action': Tensor(shape=(1,), dtype=float32),
            'rotation_delta': Tensor(shape=(3,), dtype=float32),
            'terminate_episode': Tensor(shape=(3,), dtype=int32),
            'world_vector': Tensor(shape=(3,), dtype=float32),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'clip_function_input/base_pose_tool_reached': Tensor(shape=(7,), dtype=float32),
            'clip_function_input/workspace_bounds': Tensor(shape=(3, 3), dtype=float32),
            'gripper_closed': Tensor(shape=(1,), dtype=float32),
            'height_to_bottom': Tensor(shape=(1,), dtype=float32),
            'image': Image(shape=(512, 640, 3), dtype=uint8),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
            'task_id': Tensor(shape=(1,), dtype=float32),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
    'success': bool,
})",EEF Position,"1. generating natural language instruction, especially for objects
2. remove repeatition because it is simple task",,CoRL 2018,,"Data was collected with 7 LBR IIWA robots, with 4-10 training objects per robot. The objects were replaced every 4 hours during business hours.

(collected by off-policy QT-Opt algorithm)",,,
USC Jaco Play,5,TRUE,TRUE,,"1. pick up the {}
2. place {} in the {}",88,,Tabletop,1,976,71,10,✅,jaco_play,"Templated
- place the milk dairy in the white plate (4.0%)
- pick up the long bread (4.0%)
- pick up the steak meat (4.0%)
- place the gray bowl in the table (2.0%)
- place the green cup in the oven (2.0%)
...
- place the long bread in the gray bowl (2.0%)
- place the butter dairy in the gray bowl (2.0%)
- pick up the green cup (2.0%)
- place the green cup in the dish rack (2.0%)",2,0,1,1,Jaco 2 Arm,Single Arm,,"dairy, bowl, rack, bread, fruit, table, cup, oven, meat, plate, sink","FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'gripper_closedness_action': Tensor(shape=(1,), dtype=float32),
            'terminate_episode': Tensor(shape=(3,), dtype=int32),
            'world_vector': Tensor(shape=(3,), dtype=float32),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'end_effector_cartesian_pos': Tensor(shape=(7,), dtype=float32),
            'end_effector_cartesian_velocity': Tensor(shape=(6,), dtype=float32),
            'image': Image(shape=(224, 224, 3), dtype=uint8),
            'image_wrist': Image(shape=(224, 224, 3), dtype=uint8),
            'joint_pos': Tensor(shape=(8,), dtype=float32),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})


ee_cartesian_pos_ob : end effector cartesian position. ee_cartesian_pos_ob[0:3] corresponds to position and ee_cartesian_pos_ob[3:7] corresponds to orientation in quarternian format
ee_cartesian_vel_ob : end effector cartesian velocity. ee_cartesian_pos_ob[0:3] corresponds to change in position and ee_cartesian_pos_ob[3:6] corresponds to change in orientation in roll, pitch yaw format
",EEF Position,1. Adding visual/process details for task repeatition,,NA,,,Jaco 2 Arm,,
Roboturk,4,TRUE,FALSE,,"1. object search
2. create tower
3. layout laundry",3,,Tabletop,1,"2,144",88,10,✅,roboturk,"Templated
1. object search
2. create tower
3. layout laundry",2,0,0,2,Sawyer,Single Arm,,"cloth, cups, toys","FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'gripper_closedness_action': Tensor(shape=(1,), dtype=float32),
            'rotation_delta': Tensor(shape=(3,), dtype=float32),
            'terminate_episode': float32,
            'world_vector': Tensor(shape=(3,), dtype=float32),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'front_rgb': Image(shape=(480, 640, 3), dtype=uint8),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",EEF Position,Adding visual/process details for tasks,,,,Human VR,,,"1. It seems that the original RGB FPS=1, while the control freq=10"
Berkeley Cable Routing,1,TRUE,FALSE,,"Only route a cable through a series of clips (pickup, route, perturb)",1,,Tabletop,1,"1,482",30,10,✅,berkeley_cable_routing,"None
- Only route a cable through a series of clips (pickup, route, perturb)",3,0,2,1,Franka,Single Arm,,the cable & three clips,"FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'rotation_delta': Tensor(shape=(3,), dtype=float32, description=Angular velocity about the z axis.),
            'terminate_episode': float32,
            'world_vector': Tensor(shape=(3,), dtype=float32, description=Velocity in XYZ.),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'image': Image(shape=(128, 128, 3), dtype=uint8),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
            'robot_state': Tensor(shape=(7,), dtype=float32),
            'top_image': Image(shape=(128, 128, 3), dtype=uint8),
            'wrist225_image': Image(shape=(128, 128, 3), dtype=uint8),
            'wrist45_image': Image(shape=(128, 128, 3), dtype=uint8),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",EEF velocity,It is too different from our dataset,,T-RO 2024,,Human VR,,,
"NYU VINN
",2,FALSE,FALSE,,Only open door,1,,Different cabinet,NA,435,41,3,"✅
",nyu_door_opening_surprising_effectiveness,"None
- Only open door",1,0,1,0,Hello Stretch,Mobile Manipulator,,canbinets with doors,"FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'gripper_closedness_action': Tensor(shape=(1,), dtype=float32),
            'rotation_delta': Tensor(shape=(3,), dtype=float32, description=Angular velocity around x, y and z axis.),
            'terminate_episode': float32,
            'world_vector': Tensor(shape=(3,), dtype=float32, description=Velocity in XYZ.),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'image': Image(shape=(720, 960, 3), dtype=uint8),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",EEF velocity,"1. Maybe add some details about left/right handler
2. Maybe filter repetition in the same scene
3. Action space is wrong in OXE (EEF velocity instead of position)",,,,Human Kinesthetic,,,
Freiburg Franka Play,5,TRUE,FALSE,,"pick up the pink block: 41,
grasp the drawer handle and open it: 37,
pull the drawer: 35,
grasp the handle of the drawer and open it: 34,
go open the drawer: 34,
grasp the pink block: 31,
grasp the drawer handle, then open it: 30,
open the cabinet drawer: 30,
......
grasp the pink block and rotate it left: 1,
push the purple block inside the drawer: 1,
place the pink block on top of the yellow block: 1,",401,,"A a structured tabletop environment with an openable drawer, a reachable sliding door, three color-coded LED buttons (with only the green one interacted to toggle a light), and three lettered blocks of different colors.",1,"3,242",66,15,✅,taco_play,"Templated
- turn off the blue led light
- grasp the drawer handle, then open it
- go push the pink block into the drawer
- rotate the pink block towards the right
- push the green button to turn off the green light
- turn off the blue light lamp
- turn off the red light lamp
...
- turn the yellow block right
- stack the yellow block on top of the purple block
- put the pink object on the table
- close the drawer",2,2,2,0,Franka Emika Panda,Single Arm,,"blocks, drawer, light button","FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'actions': Tensor(shape=(7,), dtype=float32, description=absolute desired values for gripper pose (first 6 dimensions are x, y, z, yaw, pitch, roll), last dimension is open_gripper (-1 is open gripper, 1 is close)),
            'rel_actions_gripper': Tensor(shape=(7,), dtype=float32, description=relative actions for gripper pose in the gripper camera frame (first 6 dimensions are x, y, z, yaw, pitch, roll), last dimension is open_gripper (-1 is open gripper, 1 is close)),
            'rel_actions_world': Tensor(shape=(7,), dtype=float32, description=relative actions for gripper pose in the robot base frame (first 6 dimensions are x, y, z, yaw, pitch, roll), last dimension is open_gripper (-1 is open gripper, 1 is close)),
            'terminate_episode': float32,
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'depth_gripper': Tensor(shape=(84, 84), dtype=float32),
            'depth_static': Tensor(shape=(150, 200), dtype=float32),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
            'rgb_gripper': Image(shape=(84, 84, 3), dtype=uint8),
            'rgb_static': Image(shape=(150, 200, 3), dtype=uint8, description=RGB static image of shape. (150, 200, 3). Subsampled from (200,200, 3) image.),
            'robot_obs': Tensor(shape=(15,), dtype=float32, description=EE position (3), EE orientation in euler angles (3), gripper width (1), joint positions (7), gripper action (1)),
            'structured_language_instruction': string,
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})","EEF Position

(rel_actions_world is used in OXE)","Seems to be good
- the instructions are diverse enough",,,,Human VR,,,
Austin VIOLA,3,TRUE,TRUE,,"1. Dining-Bowl (pick up the bowl from the lazy Susan and put bowl on the plate)
2. Make-Coffee (make coffee)
3. Dining-PlateFork (arrange plate and fork)",3,,Tabletop,1,135,516,20,✅,viola,"Templated
1. Dining-Bowl (pick up the bowl from the lazy Susan and put bowl on the plate)
2. Make-Coffee (make coffee)
3. Dining-PlateFork (arrange plate and fork)",2,0,1,1,Franka,Single Arm,,"1. plate
2. bowl
3. fork
4. coffee machine","FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'gripper_closedness_action': float32,
            'rotation_delta': Tensor(shape=(3,), dtype=float32),
            'terminate_episode': float32,
            'world_vector': Tensor(shape=(3,), dtype=float32),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'agentview_rgb': Image(shape=(224, 224, 3), dtype=uint8, description=RGB captured by workspace camera),
            'ee_states': Tensor(shape=(16,), dtype=float32, description=Pose of the end effector specified as a homogenous matrix.),
            'eye_in_hand_rgb': Image(shape=(224, 224, 3), dtype=uint8, description=RGB captured by in hand camera),
            'gripper_states': Tensor(shape=(1,), dtype=float32, description=gripper_states = 0 means the gripper is fully closed. The value represents the gripper width of Franka Panda Gripper.),
            'joint_states': Tensor(shape=(7,), dtype=float32, description=joint values),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",EEF Position,"1. Adding visual/process details for overall tasks
2. Maybe can split into sub-tasks",,,,Human Spacemouse,,,
Berkeley Autolab UR5,2,TRUE,FALSE,,"1. Take the tiger out of the red bowl and put it in the grey bowl.
2. Sweep the green cloth to the left side of the table.
3. Pick up the blue cup and put it into the brown cup.
4. Put the ranch bottle into the pot.",4,,Tabletop,1,896,95,5,✅,berkeley_autolab_ur5,"Templated
1. Take the tiger out of the red bowl and put it in the grey bowl.
2. Sweep the green cloth to the left side of the table.
3. Pick up the blue cup and put it into the brown cup.
4. Put the ranch bottle into the pot.",2,1,1,1,UR5,Single Arm,,"tiger, red bowl, grey bowl, green cloth, blue cup, brown cup, ranch bottle ,pot","FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'gripper_closedness_action': float32,
            'rotation_delta': Tensor(shape=(3,), dtype=float32, description=Delta change in roll, pitch, yaw.),
            'terminate_episode': float32,
            'world_vector': Tensor(shape=(3,), dtype=float32, description=Delta change in XYZ.),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'hand_image': Image(shape=(480, 640, 3), dtype=uint8),
            'image': Image(shape=(480, 640, 3), dtype=uint8),
            'image_with_depth': Image(shape=(480, 640, 1), dtype=float32),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
            'robot_state': Tensor(shape=(15,), dtype=float32, description=Explanation of the robot state can be found at https://sites.google.com/corp/view/berkeley-ur5),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",EEF Position,Adding some details for the specific action,,,,Human Spacemouse,,,
TOTO Benchmark,2,TRUE,FALSE,,"1. pour the material into a target cup on the table
2. scoop material from the bowl into the spoon (not in OXE?)",2,,Tabletop,1,901,312,30,✅,toto,"Templated
1. pour the material into a target cup on the table
2. scoop material from the bowl into the spoon (not in OXE?)",1,0,0,1,"Franka

(joint pos)",Single Arm,,"Cup, containers, materials","FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'open_gripper': bool,
            'rotation_delta': Tensor(shape=(3,), dtype=float32),
            'terminate_episode': float32,
            'world_vector': Tensor(shape=(3,), dtype=float32),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'image': Image(shape=(480, 640, 3), dtype=uint8),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
            'state': Tensor(shape=(7,), dtype=float32, description=numpy array of shape (7,). Contains the robot joint states (as absolute joint angles) at each timestep),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",Joint position,"1. Seems that only pour task is in OXE
2. Adding details",,,,"Human teleoperation -- VR Teleop, trained state-based BC policies, and trajectory replay with noise",,,
Language Table,5,TRUE,FALSE,,1.place/ 2. push/ 3. move/ 4. put/ 5. slide/ 6.separate {} to/towards {},NA,,"A smooth wooden board with a fixed set of 8 plastic blocks, comprising 4 colors and 6 shapes",1,"442,226",20,10,✅,language_table,"Natural
1.place
2. push
3. move
4. put
5. slide
6.separate {} to/towards {}",1,0,0,1,xArm,Single Arm,,"A fixed set of 8 plastic blocks, comprising 4 colors and 6 shapes","action: shape=(2,), <dtype: 'float32'>
is_first: shape=(), <dtype: 'bool'>
is_last: shape=(), <dtype: 'bool'>
is_terminal: shape=(), <dtype: 'bool'>
observation: (dict)
    effector_target_trshape=(2,), <dtype: 'float32'>
    effector_translation: shape=(2,), <dtype: 'float32'>
    instruction: shape=(512,),anslation:  <dtype: 'int32'>
    rgb: shape=(360, 640, 3), <dtype: 'uint8'>
reward: shape=(), <dtype: 'float32'>",EEF Position,"1. The instructions are quite diverse although there are repeated trajectoires under the same task
2. The initial surroundings/end states maybe different

Note:
1. the observation fps maybe 5
2. the left/right top/bottom maybe not from agents' view",,,,Human VR,,,
Columbia PushT Dataset,1,FALSE,FALSE,,The task requires pushing a T-shaped block (gray) to a fixed target (green) with a circular end-effector (blue).,1,,Tabletop,1,122,204,10,✅,columbia_cairlab_pusht_real,None,2,0,1,1,UR5,Single Arm,,a T-shaped block (gray),"FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'gripper_closedness_action': float32,
            'rotation_delta': Tensor(shape=(3,), dtype=float32, description=Delta change in roll, pitch, yaw.),
            'terminate_episode': float32,
            'world_vector': Tensor(shape=(3,), dtype=float32, description=Delta change in XYZ.),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'image': Image(shape=(240, 320, 3), dtype=uint8),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
            'robot_state': Tensor(shape=(2,), dtype=float32, description=Robot end effector XY state),
            'wrist_image': Image(shape=(240, 320, 3), dtype=uint8),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",EEF Position,The task is different from ours,,,,Human VR,,,
Stanford Kuka Multimodal,1,FALSE,FALSE,,Insert differently-shaped pegs into differently-shaped holes with low tolerances (~2mm).,1,,Tabletop,1,"3,000",50,20,✅,stanford_kuka_multimodal_dataset_converted_externally_to_rlds,None,1,1,0,1,Kuka iiwa,Single Arm,,"peg, hole","FeaturesDict({
    'episode_metadata': FeaturesDict({
    }),
    'steps': Dataset({
        'action': Tensor(shape=(4,), dtype=float32, description=Robot action, consists of [3x EEF position, 1x gripper open/close].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'contact': Tensor(shape=(50,), dtype=float32, description=Robot contact information.),
            'depth_image': Tensor(shape=(128, 128, 1), dtype=float32, description=Main depth camera observation.),
            'ee_forces_continuous': Tensor(shape=(50, 6), dtype=float32, description=Robot end-effector forces.),
            'ee_orientation': Tensor(shape=(4,), dtype=float32, description=Robot end-effector orientation quaternion.),
            'ee_orientation_vel': Tensor(shape=(3,), dtype=float32, description=Robot end-effector orientation velocity.),
            'ee_position': Tensor(shape=(3,), dtype=float32, description=Robot end-effector position.),
            'ee_vel': Tensor(shape=(3,), dtype=float32, description=Robot end-effector velocity.),
            'ee_yaw': Tensor(shape=(4,), dtype=float32, description=Robot end-effector yaw.),
            'ee_yaw_delta': Tensor(shape=(4,), dtype=float32, description=Robot end-effector yaw delta.),
            'image': Image(shape=(128, 128, 3), dtype=uint8, description=Main camera RGB observation.),
            'joint_pos': Tensor(shape=(7,), dtype=float32, description=Robot joint positions.),
            'joint_vel': Tensor(shape=(7,), dtype=float32, description=Robot joint velocities.),
            'optical_flow': Tensor(shape=(128, 128, 2), dtype=float32, description=Optical flow.),
            'state': Tensor(shape=(8,), dtype=float32, description=Robot proprioceptive information, [7x joint pos, 1x gripper open/close].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,"1. The resolution is low and the task is very different from ours
2. The trajectory is not generated via teleoperation, but expert policy",,,,Expert Policy,,,
NYU ROT,3,FALSE,FALSE,,"- insert the peg in the cup (21.4%)
- erase the board (7.1%)
- pour the almonds into the cup (7.1%)
- hang the bag on the hook (7.1%)
- open the box (7.1%)
- hang the mug on the hook (7.1%)
- reach the blue mark on the table (7.1%)
- hang the hanger on the rod (7.1%)
- press the button (7.1%)
- close the door (7.1%)
- turn the knob (7.1%)
- stack the cups (7.1%)",12,,Table Top,1,14,31,3,✅,nyu_rot_dataset_converted_externally_to_rlds,"Templated
- insert the peg in the cup (21.4%)
- erase the board (7.1%)
- pour the almonds into the cup (7.1%)
- hang the bag on the hook (7.1%)
- open the box (7.1%)
- hang the mug on the hook (7.1%)
- reach the blue mark on the table (7.1%)
- hang the hanger on the rod (7.1%)
- press the button (7.1%)
- close the door (7.1%)
- turn the knob (7.1%)
- stack the cups (7.1%)",1,0,0,1,xArm,Single Arm,,"board, almonds, cup, bag, hook, box, mug, mark, table, hanger, rod, button, door, peg, knob, cups
","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [3x robot end effector delta positions, 3x robot end effector rotations (roll, pitch, yaw),1x gripper open/close (0-open, 1-closed)].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(84, 84, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(7,), dtype=float32, description=Robot state, consists of [3x robot end effector positions, 3x robot end effector rotations (roll, pitch, yaw),1x gripper open/close (0-open, 1-closed)].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,1. The resolution is low and very small-scale,,,,Human Joystick,,,
Stanford HYDRA,3,TRUE,FALSE,,"make a cup of coffee with the keurig machine
make a piece of toast with the oven
palce dishes in the dish rack",3,,"Table Top, Kitchen (also toy kitchen)",2,550,646,10,✅,stanford_hydra_dataset_converted_externally_to_rlds,"Templated
- make a cup of coffee with the keurig machine
- make a piece of toast with the oven
- palce dishes in the dish rack",2,0,1,1,Franka,Single Arm,,"Oven, plate, bread, keurig machine, cup, dish, spoon, rack","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [3x EEF positional delta, 3x EEF orientation delta in euler angle, 1x close gripper].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_dense': Scalar(shape=(), dtype=bool, description=True if state is a waypoint(010) or in dense mode(x111).),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(240, 320, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(27,), dtype=float32, description=Robot state, consists of [3x EEF position,4x EEF orientation in quaternion,3x EEF orientation in euler angle,7x robot joint angles, 7x robot joint velocities,3x gripper state.),
            'wrist_image': Image(shape=(240, 320, 3), dtype=uint8, description=Wrist camera RGB observation.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,1. decompose into atmoic tasks because the avg.frames is large,,,,Human VR,,,
Austin BUDS,2,FALSE,FALSE,,"Take the lid off the pot, put the pot on the plate, and use the tool to push to pot to the front of the table.",1,,Table Top,1,50,682,20,✅,austin_buds_dataset_converted_externally_to_rlds,"Take the lid off the pot, put the pot on the plate, and use the tool to push to pot to the front of the table.",2,0,1,1,Franka,Single Arm,,"pot, plate, spoon, cup","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [6x end effector delta pose, 1x gripper position].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(128, 128, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(24,), dtype=float32, description=Robot state, consists of [7x robot joint angles, 1x gripper position, 16x robot end-effector homogeneous matrix].),
            'wrist_image': Image(shape=(128, 128, 3), dtype=uint8, description=Wrist camera RGB observation.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,"1. decompose into atmoic tasks because the duration exceeds 600 frames
2. seems there are not so many difference in the 3rd-view trajectories",,,,Human Spacemouse,,,
NYU Franka Play,4,TRUE,FALSE,,play with the kitchen,1,,Kitchen (also toy kitchen),1,456,86,3,✅,nyu_franka_play_dataset_converted_externally_to_rlds,"Templated
- play with the kitchen",2,2,0,2,Franka,Single Arm,,"cabinet, pot, drawer","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(15,), dtype=float32, description=Robot action, consists of [7x joint velocities, 3x EE delta xyz, 3x EE delta rpy, 1x gripper position, 1x terminate episode].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'depth': Tensor(shape=(128, 128, 1), dtype=int32, description=Right camera depth observation.),
            'depth_additional_view': Tensor(shape=(128, 128, 1), dtype=int32, description=Left camera depth observation.),
            'image': Image(shape=(128, 128, 3), dtype=uint8, description=Right camera RGB observation.),
            'image_additional_view': Image(shape=(128, 128, 3), dtype=uint8, description=Left camera RGB observation.),
            'state': Tensor(shape=(13,), dtype=float32, description=Robot state, consists of [7x robot joint angles, 3x EE xyz, 3x EE rpy.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF velocity,"1. A lot of details can be added, since the palying process has many differences for different trajectoies",,,,Human VR,,,
Maniskill,1,FALSE,FALSE,,"- Pick up the object and move it to a goal position (56.0%)
- Plug the charger into the wall socket (6.0%)
- Pick up a designated object from a clutter of objects (20.0%)
- Turn on the faucet by rotating a designated handle (6.0%)
- Insert the peg into the horizontal hole in a box (6.0%)
- Insert a designated object into the corresponding slot on a board (6.0%)",6,,Table Top,1,"30,000",150,20,✅,maniskill_dataset_converted_externally_to_rlds,"Templated
- Pick up the object and move it to a goal position (56.0%)
- Plug the charger into the wall socket (6.0%)
- Pick up a designated object from a clutter of objects (20.0%)
- Turn on the faucet by rotating a designated handle (6.0%)
- Insert the peg into the horizontal hole in a box (6.0%)
- Insert a designated object into the corresponding slot on a board (6.0%)",2,2,2,0,Franka,Single Arm,,articulated objects like peg,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'episode_id': Text(shape=(), dtype=string),
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [3x end effector delta target position, 3x end effector delta target orientation in axis-angle format, 1x gripper target position (mimic for two fingers)]. For delta target position, an action of -1 maps to a robot movement of -0.1m, and action of 1 maps to a movement of 0.1m. For delta target orientation, its encoded angle is mapped to a range of [-0.1rad, 0.1rad] for robot execution. For example, an action of [1, 0, 0] means rotating along the x-axis by 0.1 rad. For gripper target position, an action of -1 means close, and an action of 1 means open.),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'base_pose': Tensor(shape=(7,), dtype=float32, description=Robot base pose in the world frame, consists of [x, y, z, qw, qx, qy, qz]. The first three dimensions represent xyz positions in meters. The last four dimensions are the quaternion representation of rotation.),
            'depth': Image(shape=(256, 256, 1), dtype=uint16, description=Main camera Depth observation. Divide the depth value by 2**10 to get the depth in meters.),
            'image': Image(shape=(256, 256, 3), dtype=uint8, description=Main camera RGB observation.),
            'main_camera_cam2world_gl': Tensor(shape=(4, 4), dtype=float32, description=Transformation from the main camera frame to the world frame in OpenGL/Blender convention.),
            'main_camera_extrinsic_cv': Tensor(shape=(4, 4), dtype=float32, description=Main camera extrinsic matrix in OpenCV convention.),
            'main_camera_intrinsic_cv': Tensor(shape=(3, 3), dtype=float32, description=Main camera intrinsic matrix in OpenCV convention.),
            'state': Tensor(shape=(18,), dtype=float32, description=Robot state, consists of [7x robot joint angles, 2x gripper position, 7x robot joint angle velocity, 2x gripper velocity]. Angle in radians, position in meters.),
            'target_object_or_part_final_pose': Tensor(shape=(7,), dtype=float32, description=The final pose towards which the target object or object part needs be manipulated, consists of [x, y, z, qw, qx, qy, qz]. The pose is represented in the world frame. An episode is considered successful if the target object or object part is manipulated to this pose.),
            'target_object_or_part_final_pose_valid': Tensor(shape=(7,), dtype=uint8, description=Whether each dimension of target_object_or_part_final_pose is valid in an environment. 1 = valid; 0 = invalid (in which case one should ignore the corresponding dimensions in target_object_or_part_final_pose). ""Invalid"" means that there is no success check on the final pose of target object or object part in the corresponding dimensions.),
            'target_object_or_part_initial_pose': Tensor(shape=(7,), dtype=float32, description=The initial pose of the target object or object part to be manipulated, consists of [x, y, z, qw, qx, qy, qz]. The pose is represented in the world frame. This variable is used to specify the target object or object part when multiple objects or object parts are present in an environment),
            'target_object_or_part_initial_pose_valid': Tensor(shape=(7,), dtype=uint8, description=Whether each dimension of target_object_or_part_initial_pose is valid in an environment. 1 = valid; 0 = invalid (in which case one should ignore the corresponding dimensions in target_object_or_part_initial_pose).),
            'tcp_pose': Tensor(shape=(7,), dtype=float32, description=Robot tool-center-point pose in the world frame, consists of [x, y, z, qw, qx, qy, qz]. Tool-center-point is the center between the two gripper fingers.),
            'wrist_camera_cam2world_gl': Tensor(shape=(4, 4), dtype=float32, description=Transformation from the wrist camera frame to the world frame in OpenGL/Blender convention.),
            'wrist_camera_extrinsic_cv': Tensor(shape=(4, 4), dtype=float32, description=Wrist camera extrinsic matrix in OpenCV convention.),
            'wrist_camera_intrinsic_cv': Tensor(shape=(3, 3), dtype=float32, description=Wrist camera intrinsic matrix in OpenCV convention.),
            'wrist_depth': Image(shape=(256, 256, 1), dtype=uint16, description=Wrist camera Depth observation. Divide the depth value by 2**10 to get the depth in meters.),
            'wrist_image': Image(shape=(256, 256, 3), dtype=uint8, description=Wrist camera RGB observation.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,"1. This dataset is not real-world trajectories
2. The templated language instruction is not very clear, like ""object""",,,,Scripted,,,
Furniture Bench,3,TRUE,FALSE,,"- assemble cabinet (6.0%)
- assemble one_leg (52.0%)
- assemble stool (10.0%)
- assemble drawer (10.0%)
- assemble lamp (10.0%)
- assemble round_table (6.0%)
- assemble chair (4.0%)
- assemble square_table (2.0%)",8,,Table Top,1,"5,100",710,10,✅,furniture_bench_dataset_converted_externally_to_rlds,"Templated
- assemble cabinet (6.0%)
- assemble one_leg (52.0%)
- assemble stool (10.0%)
- assemble drawer (10.0%)
- assemble lamp (10.0%)
- assemble round_table (6.0%)
- assemble chair (4.0%)
- assemble square_table (2.0%)",2,0,1,1,Franka,Single Arm,,"cabinet, stool, drawer, lamp, chair, table","action: shape=(8,), <dtype: 'float32'>
discount: shape=(), <dtype: 'float32'>
is_first: shape=(), <dtype: 'bool'>
is_last: shape=(), <dtype: 'bool'>
is_terminal: shape=(), <dtype: 'bool'>
language_embedding: shape=(512,), <dtype: 'float32'>
language_instruction: shape=(), <dtype: 'string'>
observation: (dict)
    image: shape=(224, 224, 3), <dtype: 'uint8'>
    state: shape=(35,), <dtype: 'float32'>
    wrist_image: shape=(224, 224, 3), <dtype: 'uint8'>
reward: shape=(), <dtype: 'float32'>
skill_completion: shape=(), <dtype: 'float32'>",EEF velocity,"1. The tasks are long-horizon tasks with high variance of duration, so we need to decompose into fine-grained thoughts and actions
2. The trajectoies under the same task seem to have the same process",,,,Human VR,,,
CMU Franka Exploration,2,FALSE,FALSE,,"- lift the knife (38.0%) 
- lift the vegetable (24.0%) 
- open the cabinet (38.0%)",3,,Kitchen (also toy kitchen),1,200,10,10,✅,cmu_franka_exploration_dataset_converted_externally_to_rlds,"Templated
- lift the knife (38.0%)
- lift the vegetable (24.0%)
- open the cabinet (38.0%)",1,0,0,1,Franka,Single Arm,,"knife, vegetable, cabinet","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(8,), dtype=float32, description=Robot action, consists of [end effector position3x, end effector orientation3x, gripper action1x, episode termination1x].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'highres_image': Image(shape=(480, 640, 3), dtype=uint8, description=High resolution main camera observation),
            'image': Image(shape=(64, 64, 3), dtype=uint8, description=Main camera RGB observation.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
        'structured_action': Tensor(shape=(8,), dtype=float32, description=Structured action, consisting of hybrid affordance and end-effector control, described in Structured World Models from Human Videos.),
    }),
})",EEF Position,"1. we may need to use 'highres_image' instead of the default 'image'
2. the tasks are short-horizon tasks w/o too many differences (10 frames)",,,,Expert Policy,,,"1. The observation frequency may be lower than the control frquency
2. The reward is float number < 1. so we cannot justify the success of the expert policy"
UCSD Kitchen,5,TRUE,FALSE,,"- Open the oven door (21.0%)
- Place the teapot on the stove (20.0%)
- Turn on the faucet (19.0%)
- Put the bowl inside the kitchen cabinet (14.0%)
- Put the white box into the sink (7.0%)
- Open the carbinet door (7.0%)
- Put the green box into the sink (6.0%)
- Put the canned spam into the sink (6.0%)",8,,Kitchen (also toy kitchen),2,150,28,2,✅,ucsd_kitchen_dataset_converted_externally_to_rlds,"Natural
- Open the oven door (21.0%)
- Place the teapot on the stove (20.0%)
- Turn on the faucet (19.0%)
- Put the bowl inside the kitchen cabinet (14.0%)
- Put the white box into the sink (7.0%)
- Open the carbinet door (7.0%)
- Put the green box into the sink (6.0%)
- Put the canned spam into the sink (6.0%)",1,0,0,1,xArm,Single Arm,,"teapot, stove, faucet, box, sink, door, spam, bowl, cabinet","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(8,), dtype=float32, description=8-dimensional action, consisting of end-effector position and orientation, gripper open/close and a episode termination action.),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(480, 640, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(21,), dtype=float32, description=21-dimensional joint states, consists of robot joint angles, joint velocity and joint torque.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,"1. There are difference in the initial layout of the scene
2. Adding details about the final state/position after actions",,,,Human VR,,,
UCSD Pick Place,3,FALSE,FALSE,,"- pick up the red object from the table (525 successed)
- place the pot in the sink (44 successes)",2,,"Table Top, Kitchen (also toy kitchen)",2,"1,355",,3,✅,ucsd_pick_and_place_dataset_converted_externally_to_rlds,"Templated
- pick up the red object from the table (96.0%)
- place the pot in the sink (4.0%)",1,0,0,1,xArm,Single Arm,,"table, pot, sink","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'disclaimer': Text(shape=(), dtype=string),
        'file_path': Text(shape=(), dtype=string),
        'n_transitions': Scalar(shape=(), dtype=int32, description=Number of transitions in the episode.),
        'success': Scalar(shape=(), dtype=bool, description=True if the last state of an episode is a success state, False otherwise.),
        'success_labeled_by': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(4,), dtype=float32, description=Robot action, consists of [3x gripper velocities,1x gripper open/close torque].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(224, 224, 3), dtype=uint8, description=Camera RGB observation.),
            'state': Tensor(shape=(7,), dtype=float32, description=Robot state, consists of [3x gripper position,3x gripper orientation, 1x finger distance].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF velocity,"1. The quality is not high because it is not teleoperation
2. only simple pick-and-place",,,,Expert Policy,,,
Austin Sailor,4,TRUE,FALSE,,"- Interact with the objects in diverse but meaningful ways (64.0%) 
- Place the fish, sausage, and tomato into the frying pan (14.0%) 
- Place the pan onto the stove and place the fish and sausage into the pan (12.0%) 
- Place the bread, butter, and milk from the table onto the serving area (10.0%)",4,,"Table Top, Kitchen (also toy kitchen)",2,250,"1,628",20,✅,austin_sailor_dataset_converted_externally_to_rlds,"None
- Interact with the objects in diverse but meaningful ways (64.0%)
- Place the fish, sausage, and tomato into the frying pan (14.0%)
- Place the pan onto the stove and place the fish and sausage into the pan (12.0%)
- Place the bread, butter, and milk from the table onto the serving area (10.0%)",2,0,1,1,Franka,Single Arm,,"ways, fish, pan, bread, table, stove","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [3x ee relative pos, 3x ee relative rotation, 1x gripper action].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(128, 128, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(8,), dtype=float32, description=Default robot state, consists of [3x robot ee pos, 3x ee quat, 1x gripper state].),
            'state_ee': Tensor(shape=(16,), dtype=float32, description=End-effector state, represented as 4x4 homogeneous transformation matrix of ee pose.),
            'state_gripper': Tensor(shape=(1,), dtype=float32, description=Robot gripper opening width. Ranges between ~0 (closed) to ~0.077 (open)),
            'state_joint': Tensor(shape=(7,), dtype=float32, description=Robot 7-dof joint information (not used in original SAILOR dataset).),
            'wrist_image': Image(shape=(128, 128, 3), dtype=uint8, description=Wrist camera RGB observation.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=True on last step of the episode.),
    }),
})",EEF velocity,"1. decompose the task since the episodes are very long
2. adding visual and process details
3. the resolution of this dataset is very low",,,,Human Spacemouse,,,
Austin Sirius,2,TRUE,FALSE,,"- Insert the blue gear onto the right peg, followed by the red gear (54.0%) - Open the kcup holder, insert the kcup into the holder, and close the kcup holder (46.0%)",2,,Table Top,1,600,524,20,✅,austin_sirius_dataset_converted_externally_to_rlds,"None
- Insert the blue gear onto the right peg, followed by the red gear (54.0%)
- Open the kcup holder, insert the kcup into the holder, and close the kcup holder (46.0%)",2,0,1,1,Franka,Single Arm,,"peg, gear, holder, kcup","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [3x ee relative pos, 3x ee relative rotation, 1x gripper action].),
        'action_mode': Tensor(shape=(1,), dtype=float32, description=Type of interaction. -1: initial human demonstration. 1: intervention. 0: autonomuos robot execution (includes pre-intervention class)),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'intv_label': Tensor(shape=(1,), dtype=float32, description=Same as action_modes, except 15 timesteps preceding intervention are labeled as -10.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(84, 84, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(8,), dtype=float32, description=Default robot state, consists of [7x robot joint state, 1x gripper state].),
            'state_ee': Tensor(shape=(16,), dtype=float32, description=End-effector state, represented as 4x4 homogeneous transformation matrix of ee pose.),
            'state_gripper': Tensor(shape=(1,), dtype=float32, description=Robot gripper opening width. Ranges between ~0 (closed) to ~0.077 (open)),
            'state_joint': Tensor(shape=(7,), dtype=float32, description=Robot 7-dof joint information.),
            'wrist_image': Image(shape=(84, 84, 3), dtype=uint8, description=Wrist camera RGB observation.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF velocity,"1. Only 2 tasks without too many differences in the different trajectories
2. Very low resolution",,,,Human Spacemouse,,,
BC-Z,5,TRUE,FALSE,,"Complete list are in Tab.7/8

- place the bottle in the ceramic bowl (8.0%)
- place the white sponge in the ceramic bowl (6.0%)
- place the eraser on the white sponge (6.0%)
- pick up the ceramic bowl (6.0%)
- place the ceramic cup in the ceramic bowl (6.0%)
- place eraser in metal cup (4.0%)
- wipe tray with sponge (4.0%)
- wipe table surface with eraser (4.0%)
- others",100,,Table Top,1,"39,350",132,10,✅,bc_z,"Templated
- place
- pick up
- wipe
- stack
- knock
- drag
- push
- move
- others",1,0,0,1,Google Robot,Mobile Manipulator,,"sponge, bowl, bottle, tray, cup, eraser, pepper, brush, surface, towel, table, ...","FeaturesDict({
    'steps': Dataset({
        'action': FeaturesDict({
            'future/axis_angle_residual': Tensor(shape=(30,), dtype=float32, description=The next 10 actions for the rotation. Each action is a 3D delta to add to the current axis angle.),
            'future/target_close': Tensor(shape=(10,), dtype=int64, description=The next 10 actions for the gripper. Each action is the value the gripper closure should be changed to (notably it is *not* a delta.)),
            'future/xyz_residual': Tensor(shape=(30,), dtype=float32, description=The next 10 actions for the positions. Each action is a 3D delta to add to current position.),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'episode_success': float32,
            'image': Image(shape=(171, 213, 3), dtype=uint8, description=Camera image of the robot, downsampled 3x),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32, description=An embedding of the task via Universal Sentence Encoder (https://tfhub.dev/google/universal-sentence-encoder/4)),
            'natural_language_instruction': string,
            'present/autonomous': int64,
            'present/axis_angle': Tensor(shape=(3,), dtype=float32, description=The current rotation of the end effector in axis-angle representation.),
            'present/intervention': int64,
            'present/sensed_close': Tensor(shape=(1,), dtype=float32, description=How much the gripper is currently closed. Scaled from 0 to 1, but not all values from 0 to 1 are reachable. The range in the data is about 0.2 to 1),
            'present/xyz': Tensor(shape=(3,), dtype=float32, description=The current position of the end effector in axis-angle representation, in robot frame),
            'sequence_length': int64,
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",EEF Position,"The difference between trajectoires lies in:
- Variations in object positions
- Variations in scene background due to collecting in multiple locations
- Minor hardware differences between each robot
- Variation in object instances
- Multiple distractor objects (4-5)

Therefore we can add details about these variants

NOTE: there are unsuccessful episodes",,CoRL 2021,,Human VR,,,
USC Cloth Sim,2,FALSE,FALSE,,- fold cloth along diagonal (100.0%),1,,"Table Top, Kitchen (also toy kitchen)",2,"1,000",100,10,✅,usc_cloth_sim_converted_externally_to_rlds,"None
- fold cloth along diagonal (100.0%)",1,0,0,1,Franka,Single Arm,,cloth,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(4,), dtype=float32, description=Robot action, consists of x,y,z goal and picker commandpicker<0.5 = open, picker>0.5 = close.),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(32, 32, 3), dtype=uint8, description=Image observation of cloth.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward as a normalized performance metric in [0, 1].0 = no change from initial state. 1 = perfect fold.-ve performance means the cloth is worse off than initial state.),
    }),
})",EEF Position,It is simulated trajectoires and only about folding cloth,,,,Scripted,,,
Tokyo PR2 Fridge Opening,2,FALSE,FALSE,,- opening the fridge (100.0%),1,,Kitchen (also toy kitchen),1,64,144,10,✅,utokyo_pr2_opening_fridge_converted_externally_to_rlds,"None
- opening the fridge (100.0%)
",1,0,0,1,PR2,Single Arm,,fridge,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(8,), dtype=float32, description=Robot action, consists of [3x end effector pos, 3x robot rpy angles, 1x gripper open/close command, 1x terminal action].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(128, 128, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(7,), dtype=float32, description=Robot state, consists of [3x end effector pos, 3x robot rpy angles, 1x gripper position].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,Only about opening fridge and the trajectories are highly similar,,,,Human VR,,,
Tokyo PR2 Tabletop Manipulation,3,FALSE,FALSE,,"- picking a bread (38.0%)
- folding a cloth (34.0%)
- picking a grape (28.0%)",3,,Table Top,1,192,136,10,✅,utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds,"None
- picking a bread (38.0%)
- folding a cloth (34.0%)
- picking a grape (28.0%)",1,0,0,1,PR2,Single Arm,,"cloth, bread, grape","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(8,), dtype=float32, description=Robot action, consists of [3x end effector pos, 3x robot rpy angles, 1x gripper open/close command, 1x terminal action].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(128, 128, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(7,), dtype=float32, description=Robot state, consists of [3x end effector pos, 3x robot rpy angles, 1x gripper position].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,"The difference between trajectoires lies in:
- grasp point
- final state",,,,Human VR,,,
Saytap,5,FALSE,FALSE,,"- trot forward fast (5.0%)
- pace in place (5.0%)
- stand still (5.0%)
- bound forward slowly (5.0%)
- move forward fast in pacing gait (5.0%)
- bound backward fast (5.0%)
- move backward slowly in pacing gait (5.0%)
...
- raise your front right leg (5.0%)
- trot backward fast (5.0%)
- bound backward slowly (5.0%)
- trot backward slowly (5.0%)",20,,"Indoor, on a flat floor",1,20,"1,147",50,✅,utokyo_saytap_converted_externally_to_rlds,"Natural
- trot forward fast (5.0%)
- pace in place (5.0%)
- stand still (5.0%)
- bound forward slowly (5.0%)
- move forward fast in pacing gait (5.0%)
- bound backward fast (5.0%)
- move backward slowly in pacing gait (5.0%)
...
- raise your front right leg (5.0%)
- trot backward fast (5.0%)
- bound backward slowly (5.0%)
- trot backward slowly (5.0%)",0,0,0,0,Unitree A1,Quadrupedal Robot,,"leg, gait","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(12,), dtype=float32, description=Robot action, consists of [12x joint positios].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'desired_pattern': Tensor(shape=(4, 5), dtype=bool, description=Desired foot contact pattern for the 4 legs, the 4 rows are for the front right, front left, rear right and rear left legs, the pattern length is 5 (=0.1s).),
            'desired_vel': Tensor(shape=(3,), dtype=float32, description=Desired velocites. The first 2 are linear velocities along and perpendicular to the heading direction, the 3rd is the desired angular velocity about the yaw axis.),
            'image': Image(shape=(64, 64, 3), dtype=uint8, description=Dummy camera RGB observation.),
            'prev_act': Tensor(shape=(12,), dtype=float32, description=Actions applied in the previous step.),
            'proj_grav_vec': Tensor(shape=(3,), dtype=float32, description=The gravity vector [0, 0, -1] in the robot base frame.),
            'state': Tensor(shape=(30,), dtype=float32, description=Robot state, consists of [3x robot base linear velocity, 3x base angular vel, 12x joint position, 12x joint velocity].),
            'wrist_image': Image(shape=(64, 64, 3), dtype=uint8, description=Dummy wrist camera RGB observation.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",Joint position,"1. Only 20 trajectoires, and the resolution is very low
2. The image seems to be black??",,,,Expert Policy,,,
UTokyo xArm PickPlace,2,FALSE,FALSE,,"- Pick up a white plate, and then place it on the red plate (100.0%)
",1,,Table Top,1,95,,10,✅,utokyo_xarm_pick_and_place_converted_externally_to_rlds,"None
- Pick up a white plate, and then place it on the red plate (100.0%)",2,0,1,1,xArm,Single Arm,,plate,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [3x EEF position, 3x EEF orientation yaw/pitch/roll, 1x gripper open/close position].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'end_effector_pose': Tensor(shape=(6,), dtype=float32, description=Robot end effector pose, consists of [3x EEF position, 3x EEF orientation yaw/pitch/roll].),
            'hand_image': Image(shape=(224, 224, 3), dtype=uint8, description=Hand camera RGB observation.),
            'image': Image(shape=(224, 224, 3), dtype=uint8, description=Main camera RGB observation.),
            'image2': Image(shape=(224, 224, 3), dtype=uint8, description=Another camera RGB observation from different view point.),
            'joint_state': Tensor(shape=(14,), dtype=float32, description=Robot joint state, consists of [7x robot joint angles, 7x robot joint velocity].),
            'joint_trajectory': Tensor(shape=(21,), dtype=float32, description=Robot joint trajectory, consists of [7x robot joint angles, 7x robot joint velocity, 7x robot joint acceralation].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,No too many differences between trajectories,,,,Human Puppeteering,,,
UTokyo xArm Bimanual,3,FALSE,FALSE,,"- Reach a towel (50.0%)
- Unfold a wrinkled towel (50.0%)",2,,Table Top,1,70,24,10,✅,utokyo_xarm_bimanual_converted_externally_to_rlds,"None
- Reach a towel (50.0%)
- Unfold a wrinkled towel (50.0%)",1,0,0,1,xArm Bimanual,Bi-Manual,,towel,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(14,), dtype=float32, description=Robot action, consists of [3x EEF position (L), 3x EEF orientation yaw/pitch/roll (L), 1x gripper open/close position (L), 3x EEF position (R), 3x EEF orientation yaw/pitch/roll (R), 1x gripper open/close position (R)].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'action_l': Tensor(shape=(7,), dtype=float32, description=Left robot action, consists of [3x EEF position, 3x EEF orientation yaw/pitch/roll].),
            'action_r': Tensor(shape=(7,), dtype=float32, description=Right robot action, consists of [3x EEF position, 3x EEF orientation yaw/pitch/roll, 1x gripper open/close position].),
            'image': Image(shape=(256, 256, 3), dtype=uint8, description=Main camera RGB observation.),
            'pose_l': Tensor(shape=(6,), dtype=float32, description=Left robot end effector pose, consists of [3x EEF position, 3x EEF orientation yaw/pitch/roll].),
            'pose_r': Tensor(shape=(6,), dtype=float32, description=Right robot end effector pose, consists of [3x EEF position, 3x EEF orientation yaw/pitch/roll].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,"The difference lies in the unfolding task, but overall trajectoires are very similar",,,,Human Puppeteering,,,
RT-1 Robot Action,5,TRUE,FALSE,,"- pick object (17.5%)
- move object near object (45.3%)
- place object upright (1.1%)
- knock object over (1.1%)
- open drawer (0.4%)
- close drawer (0.4%)
- place object into receptacle (11.3%)
- pick object from receptacle and place on the counter (21.8%)",744 (8 types),,"Table Top, Kitchen (also toy kitchen)",NA,"73,499",45,3,✅,fractal20220817_data,"Templated
- pick object (17.5%)
- move object near object (45.3%)
- place object upright (1.1%)
- knock object over (1.1%)
- open drawer (0.4%)
- close drawer (0.4%)
- place object into receptacle (11.3%)
- pick object from receptacle and place on the counter (21.8%)",1,1,0,1,Google Robot,Mobile Manipulator,,"chocolate, drawer, counter, apple, bowl, coke, bottle, bag, sponge, shelf, fridge, orange, banana, blueberry, green, pepsi, can","FeaturesDict({
    'aspects': FeaturesDict({
        'already_success': bool,
        'feasible': bool,
        'has_aspects': bool,
        'success': bool,
        'undesirable': bool,
    }),
    'attributes': FeaturesDict({
        'collection_mode': int64,
        'collection_mode_name': string,
        'data_type': int64,
        'data_type_name': string,
        'env': int64,
        'env_name': string,
        'location': int64,
        'location_name': string,
        'objects_family': int64,
        'objects_family_name': string,
        'task_family': int64,
        'task_family_name': string,
    }),
    'steps': Dataset({
        'action': FeaturesDict({
            'base_displacement_vector': Tensor(shape=(2,), dtype=float32),
            'base_displacement_vertical_rotation': Tensor(shape=(1,), dtype=float32),
            'gripper_closedness_action': Tensor(shape=(1,), dtype=float32, description=continuous gripper position),
            'rotation_delta': Tensor(shape=(3,), dtype=float32, description=rpy commanded orientation displacement, in base-relative frame),
            'terminate_episode': Tensor(shape=(3,), dtype=int32),
            'world_vector': Tensor(shape=(3,), dtype=float32, description=commanded end-effector displacement, in base-relative frame),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'base_pose_tool_reached': Tensor(shape=(7,), dtype=float32, description=end-effector base-relative position+quaternion pose),
            'gripper_closed': Tensor(shape=(1,), dtype=float32),
            'gripper_closedness_commanded': Tensor(shape=(1,), dtype=float32, description=continuous gripper position),
            'height_to_bottom': Tensor(shape=(1,), dtype=float32, description=height of end-effector from ground),
            'image': Image(shape=(256, 320, 3), dtype=uint8),
            'natural_language_embedding': Tensor(shape=(512,), dtype=float32),
            'natural_language_instruction': string,
            'orientation_box': Tensor(shape=(2, 3), dtype=float32),
            'orientation_start': Tensor(shape=(4,), dtype=float32),
            'robot_orientation_positions_box': Tensor(shape=(3, 3), dtype=float32),
            'rotation_delta_to_go': Tensor(shape=(3,), dtype=float32, description=rotational displacement from current orientation to target),
            'src_rotation': Tensor(shape=(4,), dtype=float32),
            'vector_to_go': Tensor(shape=(3,), dtype=float32, description=displacement from current end-effector position to target),
            'workspace_bounds': Tensor(shape=(3, 3), dtype=float32),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",EEF Position,It is quite diverse and there are no much repetition under the same instruction,,,,Human VR,,,
dobb-e,4,TRUE,TRUE,NA,"1. switching button  521
2. door openining   1258
3. door closing  955 
4. drawer opening 675
5. drawer closing 694
6. pick and place 696
7. handle grasping 409
( many type ,anything around their household that they would like to do using the stick， ignore the play data)",7,,different scene at home,can't calcutate,"5,620",227,30,✅,dobbe,"Templated（many repeat）
1. switching button  521
2. door openining   1258
3. door closing  955 
4. drawer opening 675
5. drawer closing 694
6. pick and place 696
7. handle grasping 409
ignore the. play data ",1,1,1,0,  Hello Stretch,Single Arm,,anything around household,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': string,
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32),
        'discount': float32,
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32),
        'language_instruction': string,
        'observation': FeaturesDict({
            'gripper': Tensor(shape=(1,), dtype=float32),
            'quat': Tensor(shape=(4,), dtype=float32),
            'rot': Tensor(shape=(3,), dtype=float32),
            'state': Tensor(shape=(7,), dtype=float32),
            'wrist_image': Image(shape=(256, 256, 3), dtype=uint8),
            'xyz': Tensor(shape=(3,), dtype=float32),
        }),
        'reward': float32,
    }),
})

quat is the quaternion e.g ( w, x, y, z)
rot is rotation vector.  both used to describe the rotation",EEF position,"1. such robot is not public , quit different from ours
2. Need to re-write the intruction in a fine-grained manner
3. only one first-person image",✅,,,Human collect using tools ,"
Hello Stretch",,need fine-grad annotation / instruction
conqhose,1,FALSE,,,grasp hose,3,," Household environments, Hallways",1,139,56,30,✅,conq_hose_manipulation,"Templated
1.pick up the pillow and place it on the couch
2.grasp hose
3.vacuum the floor",3,0,1,2,Spot,Single Arm,,only hose,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': string,
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32),
        'discount': float32,
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32),
        'language_instruction': string,
        'observation': FeaturesDict({
            'frontleft_fisheye_image': Image(shape=(726, 604, 3), dtype=uint8),
            'frontright_fisheye_image': Image(shape=(726, 604, 3), dtype=uint8),
            'hand_color_image': Image(shape=(480, 640, 3), dtype=uint8),
            'state': Tensor(shape=(66,), dtype=float32),
        }),
        'reward': float32,
    }),
})",EEF velocity,"1. The image needs to be rotated and the black part cut out
2. 2 error in OXE. sheet. , 
   a. Only 3 camera ，not 6 . 
   b. 1 wrist camera, not 0 
   I have emailed the person in charge. mitranopeter @gmail.com
3. Both tasks and scenes are very simple， repeat",,NA,"
https://github.com/UM-ARM-Lab/conq_hose_manipulation_dataset_builder",scripts,spot,,bad dataset
FMB,5,TRUE,TRUE,,"1. Place 
2. Grasp
3. Insert
4. Move to board
5. Rotate 
6. Regrasp",6,,one specific place ,1,"8,611",227,10,✅,fmb,"template
{verb} the {color/shape} object
eg.
Insert the hexagon object.
Pick up the round object.",4,4,2,2,Franka Panda,Single Arm,,54  assembly objects + 3 assembly boards,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'episode_language_embedding': Tensor(shape=(512,), dtype=float32),
        'episode_language_instruction': string,
        'episode_task': string,
        'file_path': string,
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32),
        'discount': Scalar(shape=(), dtype=float32),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32),
        'language_instruction': string,
        'observation': FeaturesDict({
            'color_id': Scalar(shape=(), dtype=uint8),
            'eef_force': Tensor(shape=(3,), dtype=float32),
            'eef_pose': Tensor(shape=(7,), dtype=float32),
            'eef_torque': Tensor(shape=(3,), dtype=float32),
            'eef_vel': Tensor(shape=(6,), dtype=float32),
            'image_side_1': Image(shape=(256, 256, 3), dtype=uint8),
            'image_side_1_depth': Tensor(shape=(256, 256), dtype=float32),
            'image_side_2': Image(shape=(256, 256, 3), dtype=uint8),
            'image_side_2_depth': Tensor(shape=(256, 256), dtype=float32),
            'image_wrist_1': Image(shape=(256, 256, 3), dtype=uint8),
            'image_wrist_1_depth': Tensor(shape=(256, 256), dtype=float32),
            'image_wrist_2': Image(shape=(256, 256, 3), dtype=uint8),
            'image_wrist_2_depth': Tensor(shape=(256, 256), dtype=float32),
            'joint_pos': Tensor(shape=(7,), dtype=float32),
            'joint_vel': Tensor(shape=(7,), dtype=float32),
            'length': string,
            'object_id': Scalar(shape=(), dtype=uint8),
            'primitive': string,
            'shape_id': Scalar(shape=(), dtype=uint8),
            'size': string,
            'state_gripper_pose': Scalar(shape=(), dtype=float32),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",EEF velocity,"The component  instructions need to be rewrite.  Some instruction is coarse-grained, like the second task.",,IJRR 2024,,,Franka Panda,,"not general.This task requires an auxiliary bracket ,the black one 
"
Mobile ALOHA,3(long task ignore),FALSE,FALSE,,"1. Wipe Wine.  50
2. Cook Shrimp.  20 
3. Rinse Pan. 50 
4. Use Cabinet. 50  
5. Call Elevator. 50 
6. Push Chairs. 50 
7. High Five   20",7,,Kitchen space such as tables and cabinets,1,276,"1,890",50,✅,aloha_mobile,"template , only 7 kinds

1.open the cabinet, put the pot inside, then close it
2.get the cloth and wipe up the spill under the wine glass
3.wash the pan
4.add oil to the pan and cook the shrimp and serve it
5.navigate to elevator, push the call button, then enter the elevator
6.push the chairs under the table
7.high five ",3,0,2,1,MobileALOHA,"Dual-arm
human manipulate",,"specific obeject in the 7 tasks  
like oil, shrimp , cabinet , pan and etc.","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': string,
    }),
    'steps': Dataset({
        'action': Tensor(shape=(16,), dtype=float32),
        'discount': Scalar(shape=(), dtype=float32),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_instruction': string,
        'observation': FeaturesDict({
            'cam_high': Image(shape=(480, 640, 3), dtype=uint8),
            'cam_left_wrist': Image(shape=(480, 640, 3), dtype=uint8),
            'cam_right_wrist': Image(shape=(480, 640, 3), dtype=uint8),
            'state': Tensor(shape=(14,), dtype=float32),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})","joint position. 
total 16 = 2* 8（7 joint 1 gripper)",✅,,CoRL 2024,,human puppeteering,Mobile ALOHA,,✅
MimicPlay,4,FALSE,FALSE,,"Random operation in 5 scenes. No specific instructions. 
e.g Put flowers in the vase in the ""flower and vase"" scene",NA,,"5 table top
1.  kichen with an oven 
2. study desk with a bookshelf and lamp 
3. flowers and vase 
4. toy sandwich making 
5. cloth folding ",5,378,779,15,✅,mimic_play,"dummy, no specific instruciton ",3,0,1,2,Franka panda,Single Arm,,"flower , vase, toy , ","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': string,
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32),
        'discount': Scalar(shape=(), dtype=float32),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32),
        'language_instruction': string,
        'observation': FeaturesDict({
            'image': FeaturesDict({
                'front_image_1': Image(shape=(120, 120, 3), dtype=uint8),
                'front_image_2': Image(shape=(120, 120, 3), dtype=uint8),
            }),
            'state': FeaturesDict({
                'ee_pose': Tensor(shape=(7,), dtype=float32),
                'gripper_position': float32,
                'joint_positions': Tensor(shape=(7,), dtype=float32),
                'joint_velocities': Tensor(shape=(7,), dtype=float32),
            }),
            'wrist_image': FeaturesDict({
                'wrist_image': Image(shape=(120, 120, 3), dtype=uint8),
            }),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",,add fine-grad instructiono,,CoRL 2023,human teleoperate,,,,
IO-AI,5,FALSE,TRUE,141,"1. pick
2. place",2,,2 table-top,2,3847,84,30,✅,io_ai_tech,"template {pick/place} {object name}
1. pick
2. place",4,1,2,2,human,Single Arm,,NA(anything in the table-top),"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': string,
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32),
        'discount': Scalar(shape=(), dtype=float32),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32),
        'language_instruction': string,
        'observation': FeaturesDict({
            'depth': Image(shape=(720, 1280, 1), dtype=uint8),
            'fisheye_camera_extrinsic': Tensor(shape=(4, 4), dtype=float32),
            'fisheye_camera_intrinsic': Tensor(shape=(3, 3), dtype=float32),
            'image': Image(shape=(360, 640, 3), dtype=uint8),
            'image_fisheye': Image(shape=(640, 800, 3), dtype=uint8),
            'image_left_side': Image(shape=(360, 640, 3), dtype=uint8),
            'image_right_side': Image(shape=(360, 640, 3), dtype=uint8),
            'left_camera_extrinsic': Tensor(shape=(4, 4), dtype=float32),
            'left_camera_intrinsic': Tensor(shape=(3, 3), dtype=float32),
            'main_camera_intrinsic': Tensor(shape=(3, 3), dtype=float32),
            'right_camera_extrinsic': Tensor(shape=(4, 4), dtype=float32),
            'right_camera_intrinsic': Tensor(shape=(3, 3), dtype=float32),
            'state': Tensor(shape=(8,), dtype=float32),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})

action : EE x y z rx zy rz. gripper
state : 7 joint + gripper ",EEF Position,✅,,,,,,,"✅
although collected by human"
RoboSet,5,FALSE,TRUE,41,"1. pick
2. place 
3. flap-close 
4. flap-open  
5. cap
6. uncap
7. slide-close
8. slide-open 
9. press 
10. slide-in
11. slide-put
12. wipe",12,,tabletop,1,18250,<100,5,✅,robo_set,"fine-grad natural language 
1. pick
2. place 
3. flap-close 
4. flap-open  
5. cap
6. uncap
7. slide-close
8. slide-open 
9. press 
10. slide-in
11. slide-put
12. wipe",4,0,1,3,Franka panda,Single Arm,,"NA(anything in the kichen  table-top , like the oven , toy , mug , cup and etc.)","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': string,
        'trial_id': string,
    }),
    'steps': Dataset({
        'action': Tensor(shape=(8,), dtype=float32),
        'discount': Scalar(shape=(), dtype=float32),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_instruction': string,
        'observation': FeaturesDict({
            'image_left': Image(shape=(240, 424, 3), dtype=uint8),
            'image_right': Image(shape=(240, 424, 3), dtype=uint8),
            'image_top': Image(shape=(240, 424, 3), dtype=uint8),
            'image_wrist': Image(shape=(240, 424, 3), dtype=uint8),
            'state': Tensor(shape=(8,), dtype=float32),
            'state_velocity': Tensor(shape=(8,), dtype=float32),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})
",joint position ：7 joint position + 1 gripper,✅,,,human VR,,,,
TidyBot,1,FALSE,,,"1.Put drink cans in the recycling bin, snacks in the plastic storage box, fruit in the black storage box, clothes on the sofa, and  wooden blocks in the drawer. 3
2.Put Rubik's cubes on the coffee table and other toys in the plastic storage box. 3
3.Put clothes in the white laundry basket, bags on the sofa, and snacks in the black storage box. 3
4.Put marker pens in the plastic storage box, clothes on the chair, plush toys on the sofa, and rubber gloves in the drawer.  3
5.Put paper napkins in the trash can, soda cans in the recycling bin, plastic bags in the black storage box, and utensils on the coffee table. 3
6.Put fruits on the left shelf, wooden blocks in the middle shelf, and construction tools in the right shelf. 3
7.Put hats in the drawer, socks in the white laundry basket, other clothes in the seagrass laundry basket, shoes in the black storage box, and plush toys on the sofa. 3 
8.Put dark-colored clothes in the white laundry basket and light-colored clothes in the seagrass laundry basket.3",8,,kitchen,1,24,9,na,✅,tidybot,"1.Put drink cans in the recycling bin, snacks in the plastic storage box, fruit in the black storage box, clothes on the sofa, and  wooden blocks in the drawer. 3
2.Put Rubik's cubes on the coffee table and other toys in the plastic storage box. 3
3.Put clothes in the white laundry basket, bags on the sofa, and snacks in the black storage box. 3
4.Put marker pens in the plastic storage box, clothes on the chair, plush toys on the sofa, and rubber gloves in the drawer.  3
5.Put paper napkins in the trash can, soda cans in the recycling bin, plastic bags in the black storage box, and utensils on the coffee table. 3
6.Put fruits on the left shelf, wooden blocks in the middle shelf, and construction tools in the right shelf. 3
7.Put hats in the drawer, socks in the white laundry basket, other clothes in the seagrass laundry basket, shoes in the black storage box, and plush toys on the sofa. 3 
8.Put dark-colored clothes in the white laundry basket and light-colored clothes in the seagrass laundry basket.3",1,0,0,1,Tidybot,mobile Manipulator,,"bin,fruit,socks in the kichenscene","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': string,
    }),
    'steps': Dataset({
        'action': string,
        'discount': Scalar(shape=(), dtype=float32),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32),
        'language_instruction': string,
        'observation': FeaturesDict({
            'image': Image(shape=(360, 640, 3), dtype=uint8),
            'object': string,
            'receptacles': Sequence(string),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",string(quit special),"1. bad datasets
2. have not vedio , only static image
3. action space is string can not be used",,,,,,,
VIMA,1,FALSE,,,"1.Put the yellow object in {scene} into the yellow and purple stripe object.
2.Rotate the {dragged_obj} 90 degrees.
3.Stack objects in this order {frame_0} {frame_1} {frame_2}.
4.This is a blicket {dragged_obj_1}. This is a wug {base_obj}. Put a blicket into a wug.
5.Sweep one {swept_obj} into {bounds} without exceeding {constraint}.
6.First put {dragged_obj} into {base_obj} then put the object that was previously at its east into the same {base_obj}.
7.""Twist"" is defined as rotating object a specific angle. For examples: From {before_twist_1} to {after_twist_1}. From {before_twist_2} to {after_twist_2}. From {before_twist_3} to {after_twist_3}. Now twist all polka dot objects.
8.Rotate the {dragged_obj} 60 degrees.
9.{demo_blicker_obj_1} is kobar than {demo_less_blicker_obj_1}. {demo_blicker_obj_2} is kobar than {demo_less_blicker_obj_2}. {demo_blicker_obj_3} is kobar than {demo_less_blicker_obj_3}. Put the less kobar {dragged_obj} into the {base_obj}.
10.Rearrange objects to this setup {scene} and then restore.
11.Rotate the {dragged_obj} 120 degrees.
12.Put the tiger object in {scene} into the blue swirl object.
13.{demo_blicker_obj_1} is kobar than {demo_less_blicker_obj_1}. {demo_blicker_obj_2} is kobar than {demo_less_blicker_obj_2}. {demo_blicker_obj_3} is kobar than {demo_less_blicker_obj_3}. Put the kobar {dragged_obj} into the {base_obj}.
14.This is a dax {base_obj}. This is a blicket {dragged_obj_1}. Put a blicket into a dax.
15.Put the red swirl object in {scene} into the yellow and purple stripe object.
16.""Twist"" is defined as rotating object a specific angle. For examples: From {before_twist_1} to {after_twist_1}. From {before_twist_2} to {after_twist_2}. From {before_twist_3} to {after_twist_3}. Now twist all green objects.
...
39.Rearrange to this {scene}.
40.This is a wug {base_obj}. This is a dax {dragged_obj_1}. Put a dax into a wug.
41.Put {dragged_obj} into {base_obj_1} then {base_obj_2}. Finally restore it into its original container.",17 representative tasks ,,tablt-top,1,660103,2,na,✅,vima_converted_externally_to_rlds,"Template
1.Put the yellow object in {scene} into the yellow and purple stripe object.
2.Rotate the {dragged_obj} 90 degrees.
3.Stack objects in this order {frame_0} {frame_1} {frame_2}.
4.This is a blicket {dragged_obj_1}. This is a wug {base_obj}. Put a blicket into a wug.
5.Sweep one {swept_obj} into {bounds} without exceeding {constraint}.
6.First put {dragged_obj} into {base_obj} then put the object that was previously at its east into the same {base_obj}.
7.""Twist"" is defined as rotating object a specific angle. For examples: From {before_twist_1} to {after_twist_1}. From {before_twist_2} to {after_twist_2}. From {before_twist_3} to {after_twist_3}. Now twist all polka dot objects.
8.Rotate the {dragged_obj} 60 degrees.
9.{demo_blicker_obj_1} is kobar than {demo_less_blicker_obj_1}. {demo_blicker_obj_2} is kobar than {demo_less_blicker_obj_2}. {demo_blicker_obj_3} is kobar than {demo_less_blicker_obj_3}. Put the less kobar {dragged_obj} into the {base_obj}.
10.Rearrange objects to this setup {scene} and then restore.
11.Rotate the {dragged_obj} 120 degrees.
12.Put the tiger object in {scene} into the blue swirl object.
13.{demo_blicker_obj_1} is kobar than {demo_less_blicker_obj_1}. {demo_blicker_obj_2} is kobar than {demo_less_blicker_obj_2}. {demo_blicker_obj_3} is kobar than {demo_less_blicker_obj_3}. Put the kobar {dragged_obj} into the {base_obj}.
14.This is a dax {base_obj}. This is a blicket {dragged_obj_1}. Put a blicket into a dax.
15.Put the red swirl object in {scene} into the yellow and purple stripe object.
16.""Twist"" is defined as rotating object a specific angle. For examples: From {before_twist_1} to {after_twist_1}. From {before_twist_2} to {after_twist_2}. From {before_twist_3} to {after_twist_3}. Now twist all green objects.
...
39.Rearrange to this {scene}.
40.This is a wug {base_obj}. This is a dax {dragged_obj_1}. Put a dax into a wug.
41.Put {dragged_obj} into {base_obj_1} then {base_obj_2}. Finally restore it into its original container.",2,0,0,2,UR5,single Arm,,object in simualte environment,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'action_bounds': FeaturesDict({
            'high': Tensor(shape=(3,), dtype=float32),
            'low': Tensor(shape=(3,), dtype=float32),
        }),
        'end-effector type': string,
        'failure': Scalar(shape=(), dtype=bool),
        'file_path': string,
        'n_objects': Scalar(shape=(), dtype=int64),
        'num_steps': Scalar(shape=(), dtype=int64),
        'robot_components_seg_ids': Sequence(Scalar(shape=(), dtype=int64)),
        'seed': Scalar(shape=(), dtype=int64),
        'success': Scalar(shape=(), dtype=bool),
        'task': string,
    }),
    'steps': Dataset({
        'action': FeaturesDict({
            'pose0_position': Tensor(shape=(3,), dtype=float32),
            'pose0_rotation': Tensor(shape=(4,), dtype=float32),
            'pose1_position': Tensor(shape=(3,), dtype=float32),
            'pose1_rotation': Tensor(shape=(4,), dtype=float32),
        }),
        'discount': Scalar(shape=(), dtype=float32),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'multimodal_instruction': string,
        'multimodal_instruction_assets': FeaturesDict({
            'asset_type': Sequence(string),
            'frontal_image': Sequence(Tensor(shape=(128, 256, 3), dtype=uint8)),
            'frontal_segmentation': Sequence(Tensor(shape=(128, 256), dtype=uint8)),
            'image': Sequence(Tensor(shape=(128, 256, 3), dtype=uint8)),
            'key_name': Sequence(string),
            'segmentation': Sequence(Tensor(shape=(128, 256), dtype=uint8)),
            'segmentation_obj_info': Sequence({
                'obj_name': Sequence(string),
                'segm_id': Sequence(Scalar(shape=(), dtype=int64)),
                'texture_name': Sequence(string),
            }),
        }),
        'observation': FeaturesDict({
            'ee': int64,
            'frontal_image': Tensor(shape=(128, 256, 3), dtype=uint8),
            'frontal_segmentation': Tensor(shape=(128, 256), dtype=uint8),
            'image': Tensor(shape=(128, 256, 3), dtype=uint8),
            'segmentation': Tensor(shape=(128, 256), dtype=uint8),
            'segmentation_obj_info': FeaturesDict({
                'obj_name': Sequence(string),
                'segm_id': Sequence(Scalar(shape=(), dtype=int64)),
                'texture_name': Sequence(string),
            }),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",EEF Position,"1. there is ""Semantic Segmentation Mask"" infor
2.only the static first image 
3. bad dataset",,,,,,,
SPOC,1,FALSE,,,"1.search for a toilet
2.take the bottle with a lot of writing printed on it has a pump top to dispense the contents
3.go to the smallest electronic equipment in the bedroom
4.go to a kitchen utensil that can best be used for making hot water for tea and grasp that kitchen utensil
5.search for a candlestick and clutch that candlestick
6.search for an alarm clock and pick up that alarm clock
7.go to an alarm clock on a bed and grab that alarm clock
8.search for a vessel on a chest of drawers and grab that vessel
9.go to a mug and grab that mug
10.find a wine bottle and grab that wine bottle
11.navigate to 2 pans in the kitchen
12.hold a bowl
13.navigate to 2 foods in the kitchen
14.locate a timepiece that can best be used for waking up in the morning and grasp that timepiece
15.search for a pan and a step ladder, in that order
16.find the orange basketball with black lines and no other markings and pick it up
...
98.find the calendar furthest from the sofa in the livingroom
99.locate a bag on a chest of drawers, take that bag, and place it on a bed
100.grab the saw with a small blade and a red handle",NA,,household environment,1,2333000,76,10,✅,spoc,"1.search for a toilet
2.take the bottle with a lot of writing printed on it has a pump top to dispense the contents
3.go to the smallest electronic equipment in the bedroom
4.go to a kitchen utensil that can best be used for making hot water for tea and grasp that kitchen utensil
5.search for a candlestick and clutch that candlestick
6.search for an alarm clock and pick up that alarm clock
7.go to an alarm clock on a bed and grab that alarm clock
8.search for a vessel on a chest of drawers and grab that vessel
9.go to a mug and grab that mug
10.find a wine bottle and grab that wine bottle
11.navigate to 2 pans in the kitchen
12.hold a bowl
13.navigate to 2 foods in the kitchen
14.locate a timepiece that can best be used for waking up in the morning and grasp that timepiece
15.search for a pan and a step ladder, in that order
16.find the orange basketball with black lines and no other markings and pick it up
...
98.find the calendar furthest from the sofa in the livingroom
99.locate a bag on a chest of drawers, take that bag, and place it on a bed
100.grab the saw with a small blade and a red handle",2,0,0,2,Hello Stretch,single Arm,,object in simualte home environment,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': string,
        'task_target_split': string,
        'task_type': string,
    }),
    'steps': Dataset({
        'action': Tensor(shape=(9,), dtype=float32),
        'discount': Scalar(shape=(), dtype=float32),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_instruction': string,
        'observation': FeaturesDict({
            'an_object_is_in_hand': Scalar(shape=(), dtype=bool),
            'house_index': Scalar(shape=(), dtype=int64),
            'hypothetical_task_success': Scalar(shape=(), dtype=bool),
            'image': Image(shape=(224, 384, 3), dtype=uint8),
            'image_manipulation': Image(shape=(224, 384, 3), dtype=uint8),
            'last_action_is_random': Scalar(shape=(), dtype=bool),
            'last_action_str': string,
            'last_action_success': Scalar(shape=(), dtype=bool),
            'last_agent_location': Tensor(shape=(6,), dtype=float32),
            'manip_object_bbox': Tensor(shape=(10,), dtype=float32),
            'minimum_l2_target_distance': Scalar(shape=(), dtype=float32),
            'minimum_visible_target_alignment': Scalar(shape=(), dtype=float32),
            'nav_object_bbox': Tensor(shape=(10,), dtype=float32),
            'relative_arm_location_metadata': Tensor(shape=(4,), dtype=float32),
            'room_current_seen': Scalar(shape=(), dtype=bool),
            'rooms_seen': Scalar(shape=(), dtype=int64),
            'visible_target_4m_count': Scalar(shape=(), dtype=int64),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",joint position ：but 9 d,"1. totally simulate data
2. simulate data guide real world ",,,,,,,Sim2Real paper!
PlexRoboSuite,1,FALSE,,,"1.Place the loaf of bread into the appropriate compartment.  67
2.Place the milk carton into the appropriate compartment.   67
3.Open the door by pressing down and pulling on the handle.  67
4.Place the red cube on top of the green cube. 67
5.Place the box of cereal into the appropriate compartment. 67
6.Put the loop with the handle onto the peg. 67",6,,table top,5,402,191,20,✅,plex_robosuite,"1.Place the loaf of bread into the appropriate compartment.  67
2.Place the milk carton into the appropriate compartment.   67
3.Open the door by pressing down and pulling on the handle.  67
4.Place the red cube on top of the green cube. 67
5.Place the box of cereal into the appropriate compartment. 67
6.Put the loop with the handle onto the peg. 67",2,0,1,1,Franka Panda,single Arm,,"bread,milk carton,door,handel ,red,cube and so on","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'episode_id': string,
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float64),
        'discount': Scalar(shape=(), dtype=float32),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32),
        'language_instruction': string,
        'observation': FeaturesDict({
            'image': Image(shape=(128, 128, 3), dtype=uint8),
            'state': Tensor(shape=(32,), dtype=float64),
            'wrist_image': Image(shape=(128, 128, 3), dtype=uint8),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})",EEF Position,1. good simulate data,,,,,,,
UIUCD3Field,4,FALSE,TRUE,15,dummy instruction,NA,,table top,1,196,70,1,✅,uiuc_d3field,dummy instruction,4,4,0,4,Kinova Gen3,Single Arm,,"utensile,mug,shoe and other things in desk top","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(3,), dtype=float32, description=Robot displacement from last frame),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'depth_1': Image(shape=(360, 640, 1), dtype=uint16, description=camera 1 depth observation.),
            'depth_2': Image(shape=(360, 640, 1), dtype=uint16, description=camera 2 depth observation.),
            'depth_3': Image(shape=(360, 640, 1), dtype=uint16, description=camera 3 depth observation.),
            'depth_4': Image(shape=(360, 640, 1), dtype=uint16, description=camera 4 depth observation.),
            'image_1': Image(shape=(360, 640, 3), dtype=uint8, description=camera 1 RGB observation.),
            'image_2': Image(shape=(360, 640, 3), dtype=uint8, description=camera 2 RGB observation.),
            'image_3': Image(shape=(360, 640, 3), dtype=uint8, description=camera 3 RGB observation.),
            'image_4': Image(shape=(360, 640, 3), dtype=uint8, description=camera 4 RGB observation.),
            'state': Tensor(shape=(4, 4), dtype=float32, description=Robot end-effector state),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,"1. action sapce need to change , now is 3D, we need 7D
2. write the instruction for each episode ",,,,,,,
Austin Mutex,4(need simplify the instructions),FALSE,TRUE,38,"1.
Carefully navigate your gripper to the air fryer's handle.
Confidently seize the handle using your gripper.
Gradually bring the handle toward yourself to access the air fryer.
2.
With care, position your gripper close to the oven tray handle.
Make sure to grip the handle tightly.
Delicately and attentively pull the tray out from the oven.
3.
Kindly get your arm to the topmost drawer handle.
Get a good grip on it, then softly pull to open it.
4.
Gently grip the air fryer handle with your gripper and kindly pull it open by sliding your gripper away from the base.
Gradually bring your gripper in line with the blue bowl and gently capture it, without tightening your grip too much.
Securely place the blue bowl inside the opened air fryer with a steady position.
Liberate your robot's gripper, and withdraw your gripper from the proximity of the air fryer.
...
44.Kindly position your hand close to the oven tray's handle.
Softly yet firmly grasp the handle using your hand.
Slowly and with caution, draw the tray out until it is fully stretched out.",NA,,table top,1,1500,234,20,✅,utaustin_mutex,"1.
Carefully navigate your gripper to the air fryer's handle.
Confidently seize the handle using your gripper.
Gradually bring the handle toward yourself to access the air fryer.
2.
With care, position your gripper close to the oven tray handle.
Make sure to grip the handle tightly.
Delicately and attentively pull the tray out from the oven.
3.
Kindly get your arm to the topmost drawer handle.
Get a good grip on it, then softly pull to open it.
4.
Gently grip the air fryer handle with your gripper and kindly pull it open by sliding your gripper away from the base.
Gradually bring your gripper in line with the blue bowl and gently capture it, without tightening your grip too much.
Securely place the blue bowl inside the opened air fryer with a steady position.
Liberate your robot's gripper, and withdraw your gripper from the proximity of the air fryer.
...
44.Kindly position your hand close to the oven tray's handle.
Softly yet firmly grasp the handle using your hand.
Slowly and with caution, draw the tray out until it is fully stretched out.",2,0,1,1,Franka,Single Arm,,"object on table like book,mug","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [6x end effector delta pose, 1x gripper position]),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(128, 128, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(24,), dtype=float32, description=Robot state, consists of [7x robot joint angles, 1x gripper position, 16x robot end-effector homogeneous matrix].),
            'wrist_image': Image(shape=(128, 128, 3), dtype=uint8, description=Wrist camera RGB observation.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,✅perfect,,,,,,,
Berkeley Fanuc Manipulation,4(add some details),TRUE,TRUE,42,"1.Handover the object on the table to the human.
2.Pour the objects in the blue cup to the white cup.
3.Pick up the tape and put it in the box.
4.Pick up the toy duck and put it on the goal location represented using a cross.
5.Stack cups together.
6.Pick up the banana and place it in the drawer.
7.Press the stapler.
8.Pick up the object and place it in the box.
9.Separate the stacked cups..
10.Use the stick to swipe the cup.
11.Open the drawer and place the battery in the drawer.
12.Pick up the cup and put it on the table.
13.Pick up the screw driver and place it in the box.
14.Pick up the battery and place it in the open drawer.
15.Pick up the battery.
16.Push the cup the goal location represented using a cross.
17.Close the drawer.
18.Open the drawer.
19.Close Water kettle .
20.Pick up the brush and sweep the table.
21.open the box
22.Pick up the object under the table and put it on top of the table.
23.Pick up the brush and put it on the table.
24.Close the laptop.
25.Pick up the two cups together then put them down.
26.Push the cube to the goal location represented using a cross.
27.Disassemble the object
28.Pick up the object on the table and place it in the cup.
29.Pick up the level and place it on the cup.
30.Pick up the object on the table then put it down.
31.Pick up the pen and place it in the cup.
32.Disassemble the object.",32,,table top,1,415,150,10,✅,berkeley_fanuc_manipulation,"1.Handover the object on the table to the human.
2.Pour the objects in the blue cup to the white cup.
3.Pick up the tape and put it in the box.
4.Pick up the toy duck and put it on the goal location represented using a cross.
5.Stack cups together.
6.Pick up the banana and place it in the drawer.
7.Press the stapler.
8.Pick up the object and place it in the box.
9.Separate the stacked cups..
10.Use the stick to swipe the cup.
11.Open the drawer and place the battery in the drawer.
12.Pick up the cup and put it on the table.
13.Pick up the screw driver and place it in the box.
14.Pick up the battery and place it in the open drawer.
15.Pick up the battery.
16.Push the cup the goal location represented using a cross.
17.Close the drawer.
18.Open the drawer.
19.Close Water kettle .
20.Pick up the brush and sweep the table.
21.open the box
22.Pick up the object under the table and put it on top of the table.
23.Pick up the brush and put it on the table.
24.Close the laptop.
25.Pick up the two cups together then put them down.
26.Push the cube to the goal location represented using a cross.
27.Disassemble the object
28.Pick up the object on the table and place it in the cup.
29.Pick up the level and place it on the cup.
30.Pick up the object on the table then put it down.
31.Pick up the pen and place it in the cup.
32.Disassemble the object.",2,0,1,1,Fanuc Mate,Single Arm,,table top objects,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(6,), dtype=float32, description=Robot action, consists of [dx, dy, dz] and [droll, dpitch, dyaw]),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'end_effector_state': Tensor(shape=(7,), dtype=float32, description=Robot gripper end effector state, consists of [x, y, z] and 4x quaternion),
            'image': Image(shape=(224, 224, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(13,), dtype=float32, description=Robot joints state, consists of [6x robot joint angles, 1x gripper open status, 6x robot joint velocities].),
            'wrist_image': Image(shape=(224, 224, 3), dtype=uint8, description=Wrist camera RGB observation.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,1. action is 6D ，without gripper demention,,,,,,,
CMU Food Manipulation,5,FALSE,TRUE,61,"{grasp , press , release } the {21 kind food}
for example: 
1.Grasp the tomato slice.
2.Release the bread slice.
3.Release the mozzarella slice.
4.Grasp the boiled_carrot slice.
5.Press down on the boiled_bell_pepper slice.
6.Grasp the boiled_pear slice.
7.Grasp the cheddar slice.
8.Press down on the cucumber slice.
9.Grasp the apple slice.
10.Grasp the celery slice.
11.Press down on the onion slice.
12.Grasp the raw_steak slice.
13.Grasp the kiwi slice.
14.Press down on the boiled_pear slice.
15.Press down on the jalapeno slice.
16.Release the cheddar slice.
...
59.Release the onion slice.
60.Grasp the boiled_apple slice.
61.Press down on the boiled_potato slice","only 3 kind:
1. grasp
2. release
3. press",,table top,1,4200,,10,✅,cmu_playing_with_food,"{grasp , press , release } the {21 kind food}
for example: 
1.Grasp the tomato slice.
2.Release the bread slice.
3.Release the mozzarella slice.
4.Grasp the boiled_carrot slice.
5.Press down on the boiled_bell_pepper slice.
6.Grasp the boiled_pear slice.
7.Grasp the cheddar slice.
8.Press down on the cucumber slice.
9.Grasp the apple slice.
10.Grasp the celery slice.
11.Press down on the onion slice.
12.Grasp the raw_steak slice.
13.Grasp the kiwi slice.
14.Press down on the boiled_pear slice.
15.Press down on the jalapeno slice.
16.Release the cheddar slice.
...
59.Release the onion slice.
60.Grasp the boiled_apple slice.
61.Press down on the boiled_potato slice",3,0,2,1,Franka,Single Arm,,21 unique food items with varying slices and properties,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(8,), dtype=float32, description=""Robot action, consists of [3x end-effector position, 4x end-effector quaternion, 1x gripper open/close].""),
        'discount': Scalar(shape=(), dtype=float32, description=""Discount if provided, default to 1.""),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=""Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5""),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(480, 640, 3), dtype=uint8, description=""Main camera RGB observation.""),
            'state': Tensor(shape=(6,), dtype=float32, description=""Robot state, consists of [6x end-effector force].""),
            'finger_vision_1': Image(shape=(480, 640, 3), dtype=uint8, description=""Finger Vision 1 RGB observation.""),
            'finger_vision_2': Image(shape=(480, 640, 3), dtype=uint8, description=""Finger Vision 2 RGB observation.""),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=""Reward if provided, 1 on final step for demos.""),
    }),
})",EEF Position,"1. the action here is 8D, 3x end-effector position, 4x end-effector quaternion, 1x gripper open/close].   quaternion need to change to 3D delta rotation",,,,,,,
CMU Play Fusion,5,FALSE,TRUE,35,"some example:
1.pick pot and place in sink
2.pick spoon and place in red cup
3.open grill, then pick pineapple and place insid grill
4.pick pot and place on wooden hanger
5.open pot, then pick pineapple and place insid pot
6.open oven, then pick chicken and place insid oven
7.pick yellow cup and place in dish rack
8.pick banana and place in blue plate
9.pick knife and place in blue plate
10.pick knife and place in red cup
11.pick knife and place in pink bowl
12.pick radish and place in pink bowl
13.pick radish and place in blue plate
14.pick banana and place in pink bowl
15.open pot, then pick bread and place insid pot
16.pick yellow bowl and place in sink
17.open grill, then pick carrot and place insid grill
18.open oven, then pick carrot and place insid oven
19.pick radish and place in red cup
20.pick yellow bowl and place on wooden hanger
21.pick yellow bowl and place in dish rack
22.open oven, then pick steak and place insid oven
23.pick orange cup and place on wooden hanger
24.open oven, then pick pineapple and place insid oven
25.pick yellow cup and place in sink
26.pick orange cup and place in sink
27.pick spoon and place in pink bowl
28.open grill, then pick chicken and place insid grill
29.pick yellow cup and place on wooden hanger
30.open pot, then pick carrot and place insid pot
31.pick carrot and place in blue plate
32.open pot, then pick chicken and place insid pot
33.pick pot and place in dish rack
34.pick orange cup and place in dish rack
35.open oven, then pick bread and place insid oven
","mainly 3 task :
pick 
open
place",,"table top,kitchen",7,576,394,5,✅,cmu_play_fusion,"some example:
1.pick pot and place in sink
2.pick spoon and place in red cup
3.open grill, then pick pineapple and place insid grill
4.pick pot and place on wooden hanger
5.open pot, then pick pineapple and place insid pot
6.open oven, then pick chicken and place insid oven
7.pick yellow cup and place in dish rack
8.pick banana and place in blue plate
9.pick knife and place in blue plate
10.pick knife and place in red cup
11.pick knife and place in pink bowl
12.pick radish and place in pink bowl
13.pick radish and place in blue plate
14.pick banana and place in pink bowl
15.open pot, then pick bread and place insid pot
16.pick yellow bowl and place in sink
17.open grill, then pick carrot and place insid grill
18.open oven, then pick carrot and place insid oven
19.pick radish and place in red cup
20.pick yellow bowl and place on wooden hanger
21.pick yellow bowl and place in dish rack
22.open oven, then pick steak and place insid oven
23.pick orange cup and place on wooden hanger
24.open oven, then pick pineapple and place insid oven
25.pick yellow cup and place in sink
26.pick orange cup and place in sink
27.pick spoon and place in pink bowl
28.open grill, then pick chicken and place insid grill
29.pick yellow cup and place on wooden hanger
30.open pot, then pick carrot and place insid pot
31.pick carrot and place in blue plate
32.open pot, then pick chicken and place insid pot
33.pick pot and place in dish rack
34.pick orange cup and place in dish rack
35.open oven, then pick bread and place insid oven
",1,0,0,0,Franka,Single Arm,,table top objects,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(9,), dtype=float32, description=Robot action, consists of [7x delta eef (pos + quat), 1x gripper open/close (binary), 1x terminate episode].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(128, 128, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(8,), dtype=float32, description=Robot state, consists of [7x robot joint angles, 1x gripper position.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,"1. action space need to modify 
2. only third-person vedio , which need to consider",,CoRL2023,,,,,
CMU Stretch,5,TRUE,TRUE,5,"only 5 instruction
1.lift open green garbage can lid
2.open door
3.open drawer
4.pull open a dishwasher
5.lift up a lid from the pot",5,,Kitchen(also toy kitchen),1,135,185,10,✅,cmu_stretch,"only 5 instruction
1.lift open green garbage can lid
2.open door
3.open drawer
4.pull open a dishwasher
5.lift up a lid from the pot",1,0,1,0,Hello Stretch,Mobile Manipulator,,"only 5 object：
1.green garbage can lid
2.door
3.drawer
4. dishwasher
5.lid and pot","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(8,), dtype=float32, description=Robot action, consists of [3x ee pos, 3x ee rot 1x gripper binary action, 1x terminate episode].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(128, 128, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(4,), dtype=float32, description=Robot state, consists of [3x robot joint angles/ee pos, 1x gripper position].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,"1. action space : only aciton[:8] , first 7 demention is used
2. fine annotation  wihtin the same task
3. many trajectory are failed , need to filter",,,,,,,
RECON,1,FALSE,,,"only 1 :
Navigate to the goal.",1,,outdoors,1,11830,51,3,✅,berkeley_gnm_recon,"only 1 :
Navigate to the goal.",1,0,1,0,Jackal,Wheeled Robot,,navigation task without objects,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(2,), dtype=float64, description=Robot action, consists of 2x position),
        'action_angle': Tensor(shape=(3,), dtype=float64, description=Robot action, consists of 2x position, 1x yaw),
        'discount': Scalar(shape=(), dtype=float64, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(120, 160, 3), dtype=uint8, description=Main camera RGB observation.),
            'position': Tensor(shape=(2,), dtype=float64, description=Robot position),
            'state': Tensor(shape=(3,), dtype=float64, description=Robot state, consists of [2x position, 1x yaw]),
            'yaw': Tensor(shape=(1,), dtype=float64, description=Robot yaw),
        }),
        'reward': Scalar(shape=(), dtype=float64, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF velocity,navigation,,CORL2021,,,,,
CoryHall,1,FALSE,,,"only 1 :
Navigate to the goal.",1,,hallways,1,7331,20,5,✅,berkeley_gnm_cory_hall,"only 1 :
Navigate to the goal.",1,0,1,0,RC Car,Wheeled Robot,,navigation task without objects,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(2,), dtype=float64, description=Robot action, consists of 2x position),
        'action_angle': Tensor(shape=(3,), dtype=float64, description=Robot action, consists of 2x position, 1x yaw),
        'discount': Scalar(shape=(), dtype=float64, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(64, 85, 3), dtype=uint8, description=Main camera RGB observation.),
            'position': Tensor(shape=(2,), dtype=float64, description=Robot position),
            'state': Tensor(shape=(3,), dtype=float64, description=Robot state, consists of [2x position, 1x yaw]),
            'yaw': Tensor(shape=(1,), dtype=float64, description=Robot yaw),
        }),
        'reward': Scalar(shape=(), dtype=float64, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF velocity,navigation,,,,,,,
SACSoN,1,FALSE,,,"only 1 :
Navigate to the goal.",1,,hallways,1,2995,85,10,✅,berkeley_gnm_sac_son,"only 1 :
Navigate to the goal.",1,0,1,0,TurtleBot 2,Wheeled Robot,,navigation task without objects,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(2,), dtype=float64, description=Robot action, consists of 2x position),
        'action_angle': Tensor(shape=(3,), dtype=float64, description=Robot action, consists of 2x position, 1x yaw),
        'discount': Scalar(shape=(), dtype=float64, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(120, 160, 3), dtype=uint8, description=Main camera RGB observation.),
            'position': Tensor(shape=(2,), dtype=float64, description=Robot position),
            'state': Tensor(shape=(3,), dtype=float64, description=Robot state, consists of [2x position, 1x yaw]),
            'yaw': Tensor(shape=(1,), dtype=float64, description=Robot yaw),
        }),
        'reward': Scalar(shape=(), dtype=float64, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,navigation,,,,,,,
RoboVQA,1,FALSE,,,dummy instruction,dummy instruction,,"table top,kitchen",1,3331523,TODO,10,✅,robot_vqa,dummy,1,0,0,1,Google Robot,"3 embodiments: single-armed robot, single-armed human, single-armed human using grasping tools",,NA,"FeaturesDict({
    'timestamp_start': Tensor(shape=(), dtype=float32, description=""Start timestamp of the episode""),
    'timestamp_end': Tensor(shape=(), dtype=float32, description=""End timestamp of the episode""),
    'task_type': Tensor(shape=(), dtype=string, description=""Please see task type in https://g3doc.corp.google.com/experimental/robotics/grounded_dialog/data/dialog/synthetic_dialog/vqa/README.md?cl=head""),
    'timestamps': Sequence(
        Tensor(shape=(), dtype=float32, description=""Timestamps of the images""),
        length=-1
    ),
    'steps': Dataset({
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'observation': FeaturesDict({
            'raw_text_question': Tensor(shape=(), dtype=string, description=""Robot task question""),
            'raw_text_answer': Tensor(shape=(), dtype=string, description=""Robot task answer""),
            'images': Sequence(
                Image(shape=(288, 288, 3), dtype=uint8, encodingFormat=""jpeg""),
                length=-1
            ),
        }),
    }),
})",EEF Position,"image information is wrong , need to process in raw data",,,,,,,
Robonet,"2
（可以改出区别，但任务本身没什么质量）",FALSE,,,"NM (not mentioned)
fetch, pull, push, move .etc
mainly for moving",10,,Table Top,1,"82,775",30,1,✅,robo_net,Interact with the objects in the bin,3,0,0,3,"Sawyer (68k), Baxter (18k), WidowX (5k),  Franka (7.9k),  Kuka (1.8k),  Fetch (5k),  GoogleRobot (56k)",Single Arm,,household objects,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
        'robot': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(5,), dtype=float32),
        'discount': Scalar(shape=(), dtype=float32),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(240, 320, 3), dtype=uint8),
            'image1': Image(shape=(240, 320, 3), dtype=uint8),
            'image2': Image(shape=(240, 320, 3), dtype=uint8),
            'state': Tensor(shape=(5,), dtype=float32),
        }),
        'reward': Scalar(shape=(), dtype=float32),
    }),
})","EEF Position
delta pose 5(delta x, delta y, delta z, delta theta, gripper)",,,CoRL 2019,,scripts,"Sawyer , Baxter , WidowX , Franka , Kuka , Fetch , GoogleRobot","Gripper: Weiss Robotics WSG-50, Robotiq,  WidowX, Baxter,  Franka, Kuka",bad dataset
Berkeley MVP Data,5,FALSE,TRUE,,,6,,"Table Top, Kitchen (also toy kitchen)",2,480,94,5,✅,berkeley_mvp_converted_externally_to_rlds,"
push wooden cube
pick detergent from the sink
pick yellow cube
close fridge door
reach red block
pick fruit",1,0,1,0,xArm,Single Arm,,"cube, fridge","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(8,), dtype=float32, description=Robot action, consists of [7 delta joint pos,1x gripper binary state].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'gripper': Scalar(shape=(), dtype=bool, description=Binary gripper state (1 - closed, 0 - open)),
            'hand_image': Image(shape=(480, 640, 3), dtype=uint8, description=Hand camera RGB observation.),
            'joint_pos': Tensor(shape=(7,), dtype=float32, description=xArm joint positions (7 DoF).),
            'pose': Tensor(shape=(7,), dtype=float32, description=Gripper pose, robot frame, [3 position, 4 rotation]),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",Joint position (7 joint + 1 gripper),✅,,CoRL 2022,,human VR,,,
Berkeley RPT Data,4,FALSE,TRUE,,"pick an object from the bin
pick yellow cube 
destack cube
stack cube",4,,table top,1,908,459,30,✅,berkeley_rpt_converted_externally_to_rlds,"destack cube
stack cube
pick an object from the bin
pick yellow cube",3,0,1,2,franka,Single Arm,,K cubes,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(8,), dtype=float32, description=Robot action, consists of [7 delta joint pos,1x gripper binary state].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'gripper': Scalar(shape=(), dtype=bool, description=Binary gripper state (1 - closed, 0 - open)),
            'hand_image': Image(shape=(480, 640, 3), dtype=uint8, description=Hand camera RGB observation.),
            'joint_pos': Tensor(shape=(7,), dtype=float32, description=xArm joint positions (7 DoF).),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",Joint position (7 joint + 1 gripper),✅,,CoRL 2023,,scripts,,,
KAIST Nonprehensile Objects,3,FALSE,,,"rotate
push
topple
flip
make sth stand up
drag
lift
lie
pull",9,,Table Top,1,201,155,10,✅,kaist_nonprehensile_converted_externally_to_rlds,"rotate
push
topple
flip
make sth stand up
drag
lift
lie
pull",1,0,0,1,franka,Single Arm,,table top objects,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(20,), dtype=float32, description=Robot action, consists of [3x end-effector position residual, 3x end-effector axis-angle residual, 7x robot joint k_p gain coefficient, 7x robot joint damping ratio coefficient].The action residuals are global, i.e. multiplied on theleft-hand side of the current end-effector state.),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(480, 640, 3), dtype=uint8, description=Main camera RGB observation.),
            'partial_pointcloud': Tensor(shape=(512, 3), dtype=float32, description=Partial pointcloud observation),
            'state': Tensor(shape=(21,), dtype=float32, description=Robot state, consists of [joint_states, end_effector_pose].Joint states are 14-dimensional, formatted in the order of [q_0, w_0, q_1, w_0, ...].In other words,  joint positions and velocities are interleaved.The end-effector pose is 7-dimensional, formatted in the order of [position, quaternion].The quaternion is formatted in (x,y,z,w) order. The end-effector pose references the tool frame, in the center of the two fingers of the gripper.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})","20: 3 delta eef-pos, 3 delta eef-rot, 7 k_p, 7 damping ratio","resize image
take a subset of action
instruction is ✅",,,,,Franka,,
QUT Dynamic Grasping,,FALSE,,,,,,Table Top,,812,,30,✅,NA,,3,0,2,1,franka,Single Arm,,,NA,EEF Position,,,,,scripts,Franka,,cannot get it
Stanford MaskVIT Data,2,FALSE,,,"pick
push
reach
close the fridge door",4,,Table Top,1,"9,109",31,N/A,✅,stanford_mask_vit_converted_externally_to_rlds,"pick
push
reach
close the fridge door",1,0,0,1,Sawyer,Single Arm,,household objects (like bowls and cups),"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(5,), dtype=float32, description=Robot action, consists of [3x change in end effector position, 1x gripper yaw, 1x open/close gripper (-1 means to open the gripper, 1 means close)].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'end_effector_pose': Tensor(shape=(5,), dtype=float32, description=Robot end effector pose, consists of [3x Cartesian position, 1x gripper yaw, 1x gripper position]. This is the state used in the MaskViT paper.),
            'finger_sensors': Tensor(shape=(1,), dtype=float32, description=1x Sawyer gripper finger sensors.),
            'high_bound': Tensor(shape=(5,), dtype=float32, description=High bound for end effector pose normalization. Consists of [3x Cartesian position, 1x gripper yaw, 1x gripper position].),
            'image': Image(shape=(480, 480, 3), dtype=uint8, description=Main camera RGB observation.),
            'low_bound': Tensor(shape=(5,), dtype=float32, description=Low bound for end effector pose normalization. Consists of [3x Cartesian position, 1x gripper yaw, 1x gripper position].),
            'state': Tensor(shape=(15,), dtype=float32, description=Robot state, consists of [7x robot joint angles, 7x robot joint velocities,1x gripper position].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})","EEF Position
delta pose 5(delta x, delta y, delta z, delta theta, gripper)",,,,,scripts,sawyer,,
LSMO Dataset,"4
（可以改但并不diverse)",FALSE,FALSE,,"avoid obstacle and reach the blue pen
avoid obstacle and reach the scissors",2,,Table Top,1,50,239,10,✅,tokyo_u_lsmo_converted_externally_to_rlds,"avoid obstacle and reach the blue pen
avoid obstacle and reach the scissors",1,0,0,1,Cobotta,Single Arm,,"obstacle
pen
scissors","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [3x endeffector position, 3x euler angles,1x gripper action].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(120, 120, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(13,), dtype=float32, description=Robot state, consists of [3x endeffector position, 3x euler angles,6x robot joint angles, 1x gripper position].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})","EEF velocity
3 eef pose
3 euler angle
1 gripper action",modify action,,,,Expert Policy,,,
DLR Sara Pour Dataset,"3
（可以改并不diverse,感觉人已经改完了）",FALSE,,,Pour into the mug,1,,Table Top,1,100,130,10,✅,dlr_sara_pour_converted_externally_to_rlds,Pour into the mug,1,0,0,1,DLR SARA,Single Arm,,Household objects,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [3x robot EEF position, 3x robot EEF orientation yaw/pitch/roll calculated with scipy Rotation.as_euler(=""zxy"") Class].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(480, 640, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(6,), dtype=float32, description=Robot state, consists of [3x robot EEF position, 3x robot EEF orientation yaw/pitch/roll calculated with scipy Rotation.as_euler(=""zxy"") Class].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})","EEF velocity
3 eef pose
3 euler angle
1 gripper action",modify action,,,,Expert Policy,,,
DLR Sara Grid Clamp Dataset,2,FALSE,,,Place grid clamp,1,,"Table Top, Workshop environment",1,100,71,10,✅,dlr_sara_grid_clamp_converted_externally_to_rlds,Place grid clamp,1,0,0,1,DLR SARA,Single Arm,,,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [3x robot EEF position, 3x robot EEF orientation yaw/pitch/roll calculated with scipy Rotation.as_euler(=""zxy"") Class].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(480, 640, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(12,), dtype=float32, description=Robot state, consists of [3x robot EEF position, 3x robot EEF orientation yaw/pitch/roll calculated with scipy Rotation.as_euler(""zxy"") Class, 6x robot EEF wrench].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})","EEF velocity
3 eef pose
3 euler angle
1 gripper action",modify action,,,,Expert Policy,,,
DLR Wheelchair Shared Control,"2
（task很多，但结构上不是很相关-四爪）",FALSE,,,pick,9,,"Table Top, Shelf",2,104,86,5,✅,dlr_edan_shared_control_converted_externally_to_rlds,"pick the mug
pick the glass
pick the yellow ball
pick the orange
pick the tennis ball
pick the red apple
pick the pear
pick the green apple
pick the banana",1,0,0,1,DLR EDAN,Single Arm,,,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [3x robot EEF position, 3x robot EEF orientation yaw/pitch/roll calculated with scipy Rotation.as_euler(=""zxy"") Class].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(360, 640, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(7,), dtype=float32, description=Robot state, consists of [3x robot EEF position, 3x robot EEF orientation yaw/pitch/roll calculated with scipy Rotation.as_euler(=""zxy"") Class].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})","EEF Position
3 eef pose
3 eef euler zxy
1 gripper action",modify action,,,,Human teleoperating,,,
ASU TableTop Manipulation,3,FALSE,,,"put
place
pick
move
push",5,,Table Top,,110,236,"12,5",✅,asu_table_top_converted_externally_to_rlds,"put
place
pick
move
push",1,0,0,1,UR5,Single Arm,,"cube, bottle, carton","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [7x joint velocities, 2x gripper velocities, 1x terminate episode].),
        'action_delta': Tensor(shape=(7,), dtype=float32, description=Robot delta action, consists of [7x joint velocities, 2x gripper velocities, 1x terminate episode].),
        'action_inst': Text(shape=(), dtype=string),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'goal_object': Text(shape=(), dtype=string),
        'ground_truth_states': FeaturesDict({
            'EE': Tensor(shape=(6,), dtype=float32, description=xyzrpy),
            'bottle': Tensor(shape=(6,), dtype=float32, description=xyzrpy),
            'bread': Tensor(shape=(6,), dtype=float32, description=xyzrpy),
            'coke': Tensor(shape=(6,), dtype=float32, description=xyzrpy),
            'cube': Tensor(shape=(6,), dtype=float32, description=xyzrpy),
            'milk': Tensor(shape=(6,), dtype=float32, description=xyzrpy),
            'pepsi': Tensor(shape=(6,), dtype=float32, description=xyzrpy),
        }),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(224, 224, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(7,), dtype=float32, description=Robot state, consists of [6x robot joint angles, 1x gripper position].),
            'state_vel': Tensor(shape=(7,), dtype=float32, description=Robot joint velocity, consists of [6x robot joint angles, 1x gripper position].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})",EEF Position,,,,,scripts,,,action description is incorrect
Stanford Robocook,1,FALSE,,,"pinch with gripper
press with punch
press with press
roll with rolling pin",9,,"Table Top, Kitchen (also toy kitchen)",2,"2,460",45,5,✅,stanford_robocook_converted_externally_to_rlds,"
pinch the dough with an asymmetric gripper
press the dough with a circle punch
pinch the dough with a two-rod symmetric gripper
press the dough with a square press
roll the dough with a small rolling pin
press the dough with a square punch
pinch the dough with a two-plane symmetric gripper
press the dough with a circle press
roll the dough with a large rolling pin",4,4,0,4,Franka,Single Arm,,"kitchen tools
dough","FeaturesDict({
    'episode_metadata': FeaturesDict({
        'extrinsics_1': Tensor(shape=(4, 4), dtype=float32, description=Camera 1 Extrinsic Matrix.),
        'extrinsics_2': Tensor(shape=(4, 4), dtype=float32, description=Camera 2 Extrinsic Matrix.),
        'extrinsics_3': Tensor(shape=(4, 4), dtype=float32, description=Camera 3 Extrinsic Matrix.),
        'extrinsics_4': Tensor(shape=(4, 4), dtype=float32, description=Camera 4 Extrinsic Matrix.),
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(7,), dtype=float32, description=Robot action, consists of [3x robot end-effector velocities, 3x robot end-effector angular velocities, 1x gripper velocity].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'depth_1': Tensor(shape=(256, 256), dtype=float32, description=Camera 1 Depth observation.),
            'depth_2': Tensor(shape=(256, 256), dtype=float32, description=Camera 2 Depth observation.),
            'depth_3': Tensor(shape=(256, 256), dtype=float32, description=Camera 3 Depth observation.),
            'depth_4': Tensor(shape=(256, 256), dtype=float32, description=Camera 4 Depth observation.),
            'image_1': Image(shape=(256, 256, 3), dtype=uint8, description=Camera 1 RGB observation.),
            'image_2': Image(shape=(256, 256, 3), dtype=uint8, description=Camera 2 RGB observation.),
            'image_3': Image(shape=(256, 256, 3), dtype=uint8, description=Camera 3 RGB observation.),
            'image_4': Image(shape=(256, 256, 3), dtype=uint8, description=Camera 4 RGB observation.),
            'state': Tensor(shape=(7,), dtype=float32, description=Robot state, consists of [3x robot end-effector position, 3x robot end-effector euler angles, 1x gripper position].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})","EEF Position
3 eef velocity
3 eef angular velocity
1 gripper velocity",modify action,,CoRL 2023,,scripts,,,
ETH Agent Affordances,,FALSE,,,"close the oven
open the oven",2,,Kitchen(also toy kitchen),1,118,"1,265",66.6,✅,eth_agent_affordances,"close the oven
open the oven",1,1,0,1,Franka,Mobile manipulation,,,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
        'input_point_cloud': Tensor(shape=(10000, 3), dtype=float16, description=Point cloud (geometry only) of the object at the beginning of the episode (world frame) as a numpy array (10000,3).),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(6,), dtype=float32, description=Robot action, consists of [end-effector velocity (v_x,v_y,v_z,omega_x,omega_y,omega_z) in world frame),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(64, 64, 3), dtype=uint8, description=Main camera RGB observation. Not available for this dataset, will be set to np.zeros.),
            'input_point_cloud': Tensor(shape=(10000, 3), dtype=float16, description=Point cloud (geometry only) of the object at the beginning of the episode (world frame) as a numpy array (10000,3).),
            'state': Tensor(shape=(8,), dtype=float32, description=State, consists of [end-effector pose (x,y,z,yaw,pitch,roll) in world frame, 1x gripper open/close, 1x door opening angle].),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})","EEF velocity
v_x, v_y, v_z
omega_x, omega_y, omega_z",modify action,,,,Expert Policy,,,"input point-cloud
depth image"
Imperial Wrist Cam,3,FALSE,,,"put
open
pick up
hang
swipe
grasp
pour
insert
stack",17,,Table Top,1,170,43,10,✅,imperialcollege_sawyer_wrist_cam,"put apple in pot 
open lid 
pick up apple 
open bottle 
pick up kettle 
hang cup 
swipe 
grasp can 
pour in mug 
pick up mug 
insert cap in bottle 
pick up pan 
put cup in dishwasher 
insert toast 
stack bowls 
pick up bottle 
pick up shoe ",1,0,1,0,Sawyer,Single Arm,,kitchen household,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(8,), dtype=float32, description=Robot action, consists of 3x delta position in EEF frame, 3x delta ZYX euler angles, 1x gripper open/close, 1x terminate episode.),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(64, 64, 3), dtype=uint8, description=Main camera RGB observation (same as wrist in our case).),
            'state': Tensor(shape=(1,), dtype=float32, description=Gripper state (opened or closed)),
            'wrist_image': Image(shape=(64, 64, 3), dtype=uint8, description=Wrist camera RGB observation.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})","EEF Position
3 eef delta pose
3 zyx euler angles
1 gripper
1 terminate episode",modify action,,,,Human Kinesthetic,,,
CMU Franka Pick-Insert Data,"4
（场景比较diverse，但action没有很diverse但质量较好）",FALSE,FALSE,,"Insert
pick up",7,,Table Top,1,631,230,20,✅,iamlab_cmu_pickup_insert_converted_externally_to_rlds,"Insert greeen block.
Pick up pink block.
Pick up yellow block.
Pick up narrow blue stick.
Pick up green block.
Pick up blue block.
Pick up pink flower.",2,0,1,1,Franka,Single Arm,,,"FeaturesDict({
    'episode_metadata': FeaturesDict({
        'file_path': Text(shape=(), dtype=string),
    }),
    'steps': Dataset({
        'action': Tensor(shape=(8,), dtype=float32, description=Robot action, consists of [3x end-effector position, 4x end-effector quaternion, 1x gripper open/close].),
        'discount': Scalar(shape=(), dtype=float32, description=Discount if provided, default to 1.),
        'is_first': bool,
        'is_last': bool,
        'is_terminal': bool,
        'language_embedding': Tensor(shape=(512,), dtype=float32, description=Kona language embedding. See https://tfhub.dev/google/universal-sentence-encoder-large/5),
        'language_instruction': Text(shape=(), dtype=string),
        'observation': FeaturesDict({
            'image': Image(shape=(360, 640, 3), dtype=uint8, description=Main camera RGB observation.),
            'state': Tensor(shape=(20,), dtype=float32, description=Robot state, consists of [7x robot joint angles, 1x gripper status, 6x joint torques, 6x end-effector force].),
            'wrist_image': Image(shape=(240, 320, 3), dtype=uint8, description=Wrist camera RGB observation.),
        }),
        'reward': Scalar(shape=(), dtype=float32, description=Reward if provided, 1 on final step for demos.),
    }),
})","EEF Position
3 eef pose
4 eef quat
1 gripper
",modify action,,,,,,,
QUT Dexterous Manpulation,,FALSE,,,,,,Table Top,,,,30,✅,qut_dexterous_manipulation,,2,0,1,,Franka,Mobile manipulation,,,NA,EEF Position,,,,,,,,cannot get it
MPI Muscular Proprioception,,FALSE,,,,,,"The robot is alone in the environment, there are no other objects in the workspace.",,,,500,,N/A,,0,0,0,,PAMY2,Single Arm,,,NA,Desired pressures for the artificial muscles,,,,,,,,cannot get it
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
,,FALSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,