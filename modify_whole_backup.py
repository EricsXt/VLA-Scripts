import os
import sys
import json
import logging
from collections import defaultdict

import pandas as pd
import tensorflow as tf
import tensorflow_datasets as tfds
from tqdm import tqdm


# å¯¼å…¥ utils
try:
    from scripts.xintong.utils.log_utils import setup_logging as utils_setup_logging
    from scripts.xintong.utils.dataset_utils import (
        dataset2path as utils_dataset2path,
        debug_local_dataset_structure as utils_debug_local_dataset_structure,
    )
    from scripts.xintong.utils.annotations import TaskVidCaptionManager as UtilsTaskVidCaptionManager
    from scripts.xintong.utils.video_utils import visualize_task_episodes as utils_visualize_task_episodes
    from scripts.xintong.utils.config import DATASET_CONFIGS, Local_dataset, Google_dataset
    from scripts.xintong.utils.dataset_utils import extract_instruction_from_step0 as utils_extract_instruction_from_step0
except ModuleNotFoundError:
    # å°†é¡¹ç›®æ ¹è·¯å¾„åŠ å…¥sys.pathï¼Œæ”¯æŒç›´æ¥ç”¨ `python scripts/xintong/modify_whole.py` è¿è¡Œ
    PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    if PROJECT_ROOT not in sys.path:
        sys.path.append(PROJECT_ROOT)
    from scripts.xintong.utils.log_utils import setup_logging as utils_setup_logging
    from scripts.xintong.utils.dataset_utils import (
        dataset2path as utils_dataset2path,
        debug_local_dataset_structure as utils_debug_local_dataset_structure,
    )
    from scripts.xintong.utils.annotations import TaskVidCaptionManager as UtilsTaskVidCaptionManager
    from scripts.xintong.utils.video_utils import visualize_task_episodes as utils_visualize_task_episodes
    from scripts.xintong.utils.config import DATASET_CONFIGS, Local_dataset, Google_dataset
    from scripts.xintong.utils.dataset_utils import extract_instruction_from_step0 as utils_extract_instruction_from_step0







Whether_local_dataset = True # æ˜¯å¦æ˜¯æœ¬åœ°æ•°æ®é›† false æ˜¯googleæ•°æ®é›†

df = pd.read_csv('/home2/qrchen/embodied-datasets/scripts/xintong/Datasets/metadata.csv') # å¯ä»¥ä¿®æ”¹ä¸ºå…¶ä»–è·¯å¾„

# åˆ›å»ºlogæ–‡ä»¶å¤¹
log_dir = './log'
os.makedirs(log_dir, exist_ok=True)

#æ£€æŸ¥æ•°æ®ç»“æ„
if Whether_local_dataset:
    datasets_name = Local_dataset
    # è°ƒè¯•æœ¬åœ°æ•°æ®é›†ç»“æ„
    print("ğŸ” æ­£åœ¨æ£€æŸ¥æœ¬åœ°æ•°æ®é›†ç»“æ„...")
    utils_debug_local_dataset_structure()
    print("=" * 60)
else:
    datasets_name = Google_dataset


# å¼€å§‹é€ä¸ªå¤„ç†æ•°æ®é›†
for dataset_name in datasets_name:
    dataset = dataset_name
    logger = utils_setup_logging(log_dir, dataset_name)
    logger.info(f"Processing dataset: {dataset}\n")

    # ä»df ä¸­æ‰¾åˆ° DATASET_CONFIG ä¸­ dataset_name_in_csv å¯¹åº”çš„è¡Œï¼Œå¾—åˆ°å…¶ä¸­ nickname çš„å€¼
    dataset_name_in_csv = DATASET_CONFIGS[dataset_name]['dataset_name']
    dataset = df[df['Datasets'] == dataset_name_in_csv]
    dataset = dataset['NickName'].item()
    # è®¾ç½®ä¿å­˜è·¯å¾„
    save_root = os.path.join('/home2/qrchen/embodied-datasets/Modifications', dataset)
    os.makedirs(save_root, exist_ok=True)

    # é…ç½®å‚æ•°
    TRAIN_SPLIT = 'train' #åŸºæœ¬ä¸Šéƒ½æ˜¯train
    MAX_EPISODES = DATASET_CONFIGS[dataset_name]['max_episodes'] # æœ€å¤šæ”¶é›†å¤šå°‘ä¸ªepisode
    STRICT_MODE = True  # æ˜¯å¦ä¸¥æ ¼æ¨¡å¼
    # STRICT_MODE = True  -> ç›´åˆ°æ”¶é½ 10 ä¸ª task ä¸”æ¯ä¸ª task éƒ½ >= 10 ä¸ª episode æ‰åœ
    # STRICT_MODE = False -> ä¸€æ—¦é‡åˆ° 10 ä¸ªä¸åŒçš„ task å°±åœï¼ˆä¸ä¿è¯æ¯ä¸ª task éƒ½æœ‰ 10 ä¸ª episodeï¼‰
    # è§†é¢‘ç›¸å…³é…ç½®
    IMAGE = DATASET_CONFIGS[dataset_name]['image_field']
    OTHER_IMAGE_FIELDS = DATASET_CONFIGS[dataset_name].get('other_image_fields', None)
    START_IDX = 0  # ä»æ‰€é€‰ task ç¬¬å‡ ä¸ª episode å¼€å§‹
    ALL_TASKS = True # æ˜¯å¦å¯è§†åŒ–æ‰€æœ‰çš„task
    fps = DATASET_CONFIGS[dataset_name]['fps']
    skip_frame = DATASET_CONFIGS[dataset_name]['skip_frame'] # è·³è¿‡å¤šå°‘å¸§ï¼Œé»˜è®¤æ˜¯3å¸§


    #å®šä¹‰ä¸‰ä¸ªå±€éƒ¨å˜é‡
    task2count = defaultdict(int)
    instruction2episodes = defaultdict(list) # instruction -> [episode_id, ...]
    unique_tasks = set()  # ä¸åŒçš„task 

    # åŠ è½½æ•°æ®é›†å¹¶æŸ¥çœ‹å…¶ task å’Œ episode æ•°
    dataset_path = utils_dataset2path(dataset, is_local=Whether_local_dataset)
    if dataset_path is None:
        logger.error(f"æ— æ³•è·å–æ•°æ®é›†è·¯å¾„ï¼Œè·³è¿‡æ•°æ®é›†: {dataset}")
        continue
    
    if Whether_local_dataset:
        # æœ¬åœ°æ•°æ®é›†åŠ è½½
        logger.info(f"ä»æœ¬åœ°è·¯å¾„åŠ è½½æ•°æ®é›†: {dataset_path}")
        b = tfds.builder_from_directory(builder_dir=dataset_path)
    else:
        # Google Cloud æ•°æ®é›†åŠ è½½
        logger.info(f"ä»Google CloudåŠ è½½æ•°æ®é›†: {dataset_path}")
        b = tfds.builder_from_directory(builder_dir=dataset_path)
    
    # å¦‚æœMAX_EPISODESå¤§äºæ•°æ®é›†çš„episodeæ•°ï¼Œåˆ™ä½¿ç”¨æ•°æ®é›†çš„episodeæ•°
    ds = b.as_dataset(split=TRAIN_SPLIT)
    if MAX_EPISODES is None:
        max_episodes = len(ds)
    else:
        max_episodes = min(MAX_EPISODES, len(ds))
    logger.info(f"total episodes: {len(ds)}")
    ds = ds.take(max_episodes)

    episodes = []
    episode_id = 0
    pbar = tqdm(ds, desc="ç»Ÿè®¡ task æ„å»ºç´¢å¼•å¹¶ç¼“å­˜ episode")

    #ç¼“å­˜ episode
    for episode in pbar:
        try:
            episodes.append(episode)  # ç¼“å­˜ episod

            step_0 = next(iter(episode['steps']))  # ä»…å–ç¬¬ 0 æ­¥
            # å»é™¤æ¢è¡Œç¬¦ï¼Œé¿å…ä½œä¸º task åç§°æ—¶å‡ºç°æ¢è¡Œ
            instruction = utils_extract_instruction_from_step0(step_0).replace('\n', '')
            # å¦‚æœæ˜¯ç©ºçš„ ï¼Œåº”è¯¥è¡¥ä¸Šdummy
            if instruction == "":
                instruction = "Dummy instruction"
            # ç»Ÿè®¡ä¸ç´¢å¼•
            task2count[instruction] += 1
            instruction2episodes[instruction].append(episode_id)
            unique_tasks.add(instruction)

        except Exception as e:
            logger.error(f"[è·³è¿‡] episode {episode_id} å‡ºé”™ï¼š{e}")
        finally:
            episode_id += 1

    logger.info(f"å·²é‡‡é›†åˆ° {len(unique_tasks)} ä¸ªä¸åŒ task")
    logger.info(f"unique_tasks: {list(unique_tasks)}")

    # if STRICT_MODE:
    #     logger.info("å„ task æ”¶é›† episode æ•°é‡: ")
    #     for t in list(unique_tasks):
    #         logger.info(f"  {t[:60]}... -> {len(instruction2episodes[t])}")


    # ä¿å­˜ æ¯ä¸ªæŒ‡ä»¤å¯¹åº”çš„ episode_id åˆ—è¡¨
    # with open(os.path.join(save_root, "instruction2episodes.json"), "w", encoding="utf-8") as f:
    #     json.dump(instruction2episodes, f, indent=2, ensure_ascii=False)

    # å¾—åˆ°annotation ç±»

    annotation_file = os.path.join(save_root, 'annotations.json')
    # åˆ æ‰åŸæ¥çš„annotations.json
    if os.path.exists(annotation_file):
        os.remove(annotation_file)
    # åˆ›å»ºæ–°çš„annotations.json
    with open(annotation_file, "w", encoding="utf-8") as f:
        json.dump({}, f, indent=2, ensure_ascii=False)  
    manager = UtilsTaskVidCaptionManager(annotation_file) #å®šä¹‰äº†ä¸€ä¸ªç±»ï¼Œtaskä¸­ caption æ ‡æ³¨çš„å¯¹åº”å…³ç³»



    all_tasks = list(instruction2episodes.keys()) # å¯è§†åŒ–æ‰€æœ‰çš„task
    start_idx = START_IDX  # ä»ç¬¬å‡ ä¸ªepisodeå¼€å§‹ å¯ä»¥æŸ¥çœ‹ instruction2episodes æ–‡ä»¶æ¥ä¿®æ”¹
    #å¦‚æœå¯è§†åŒ–all taskï¼Œåˆ™å¯è§†åŒ–æ‰€æœ‰çš„taskï¼Œå¦åˆ™åªå¯è§†åŒ–æŒ‡å®šçš„task
    if ALL_TASKS:
        for task in all_tasks:
            save_dir = os.path.join(save_root, 'samples', task[:30].replace(' ','_').replace('/','_')) #é¿å…è¿‡é•¿çš„æ–‡ä»¶å
            utils_visualize_task_episodes(
                episodes, instruction2episodes, task, 
                start_idx=start_idx, count=-1, save_root=save_dir, 
                fps=fps, frame_stride=skip_frame,
                image_field=IMAGE,
                other_image_fields=DATASET_CONFIGS[dataset_name].get('other_image_fields'),
                dataset_nickname=dataset
            )
    else:
        save_dir = os.path.join(save_root, 'samples', task[:30].replace(' ','_').replace('/','_')) #é¿å…è¿‡é•¿çš„æ–‡ä»¶å
        utils_visualize_task_episodes(
            episodes, instruction2episodes, task, 
            start_idx=start_idx, count=-1, save_root=save_dir, 
            fps=fps, frame_stride=skip_frame,   
            image_field=IMAGE,
            other_image_fields=DATASET_CONFIGS[dataset_name].get('other_image_fields'),
            dataset_nickname=dataset
        )
    manager.save()